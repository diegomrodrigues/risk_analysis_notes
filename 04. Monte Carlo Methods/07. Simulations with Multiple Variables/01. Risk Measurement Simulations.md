### Simula√ß√µes com M√∫ltiplas Vari√°veis

### Introdu√ß√£o

Expandindo o conceito de simula√ß√µes de Monte Carlo introduzido no Cap√≠tulo 12 [^1], esta se√ß√£o explora a aplica√ß√£o dessas t√©cnicas em cen√°rios mais complexos que envolvem m√∫ltiplas vari√°veis de risco. Em continuidade ao que vimos anteriormente sobre simula√ß√µes com uma √∫nica vari√°vel aleat√≥ria, este cap√≠tulo se aprofunda nos m√©todos para simular sistemas financeiros que s√£o intrinsecamente multivariados. As aplica√ß√µes modernas de medi√ß√£o de risco frequentemente demandam a simula√ß√£o de um grande n√∫mero de vari√°veis de risco para refletir adequadamente a complexidade das institui√ß√µes financeiras [^321].

### Conceitos Fundamentais

#### Simula√ß√µes com M√∫ltiplas Vari√°veis de Risco

A aplica√ß√£o de simula√ß√µes de Monte Carlo em finan√ßas se estende naturalmente a problemas multivariados, onde o valor de um portf√≥lio ou derivativo depende de m√∫ltiplos fatores de risco. Em tais cen√°rios, √© crucial modelar n√£o apenas o comportamento individual de cada fator de risco, mas tamb√©m as interdepend√™ncias entre eles.

#### De Vari√°veis Independentes a Correlacionadas

O ponto de partida para simula√ß√µes com m√∫ltiplas vari√°veis √© frequentemente a gera√ß√£o de vari√°veis aleat√≥rias independentes, que ent√£o precisam ser transformadas para incorporar as correla√ß√µes observadas nos dados reais [^321]. Suponha que tenhamos $N$ fontes de risco. Se essas vari√°veis forem n√£o correlacionadas, a aleatoriza√ß√£o pode ser executada independentemente para cada vari√°vel, resultando na seguinte equa√ß√£o para a varia√ß√£o do pre√ßo do ativo *j* no instante *t*:

$$
\Delta S_{j,t} = S_{j,t-1} (\mu_j \Delta t + \sigma_j \epsilon_j \sqrt{\Delta t})
$$

onde:

*   $\Delta S_{j,t}$ √© a varia√ß√£o no pre√ßo do ativo *j* no instante *t*.
*   $S_{j,t-1}$ √© o pre√ßo do ativo *j* no instante *t-1*.
*   $\mu_j$ √© o drift instant√¢neo do ativo *j*.
*   $\Delta t$ √© o incremento de tempo.
*   $\sigma_j$ √© a volatilidade do ativo *j*.
*   $\epsilon_j$ √© uma vari√°vel aleat√≥ria normalmente distribu√≠da com m√©dia zero e vari√¢ncia unit√°ria.

Nesse contexto, os valores de $\epsilon$ s√£o independentes entre os per√≠odos de tempo e entre as s√©ries *j* = 1, ..., *N* [^321].

> üí° **Exemplo Num√©rico:**
>
> Considere dois ativos, A e B, com pre√ßos iniciais $S_{A,0} = 100$ e $S_{B,0} = 150$, drifts $\mu_A = 0.10$ e $\mu_B = 0.15$, e volatilidades $\sigma_A = 0.20$ e $\sigma_B = 0.25$, respectivamente. Assuma um incremento de tempo $\Delta t = 1/252$ (um dia √∫til em um ano). Gere duas vari√°veis aleat√≥rias independentes, $\epsilon_A = 0.5$ e $\epsilon_B = -0.3$.
>
> $\Delta S_{A,1} = 100 * (0.10 * (1/252) + 0.20 * 0.5 * \sqrt{1/252}) \approx 100 * (0.0004 + 0.0063) \approx 0.67$
>
> $S_{A,1} = S_{A,0} + \Delta S_{A,1} = 100 + 0.67 = 100.67$
>
> $\Delta S_{B,1} = 150 * (0.15 * (1/252) + 0.25 * (-0.3) * \sqrt{1/252}) \approx 150 * (0.0006 + (-0.00047)) \approx 0.02$
>
> $S_{B,1} = S_{B,0} + \Delta S_{B,1} = 150 + 0.02 = 150.02$
>
> Este exemplo demonstra como os pre√ßos dos ativos evoluem em um √∫nico passo de tempo, considerando seus drifts, volatilidades e vari√°veis aleat√≥rias independentes.

#### Incorporando Correla√ß√µes

Para levar em conta as correla√ß√µes entre as vari√°veis, inicia-se com um conjunto de vari√°veis independentes $\eta$, que s√£o ent√£o transformadas nas vari√°veis $\epsilon$ [^321]. Em um cen√°rio bivariado, essa transforma√ß√£o pode ser expressa como:

$$
\begin{aligned}
\epsilon_1 &= \eta_1 \\
\epsilon_2 &= \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2
\end{aligned}
$$

onde $\rho$ √© o coeficiente de correla√ß√£o entre as vari√°veis $\epsilon_1$ e $\epsilon_2$ [^321]. Pode-se verificar que a vari√¢ncia de $\epsilon_2$ √© unit√°ria:

$$V(\epsilon_2) = \rho^2 V(\eta_1) + (1-\rho^2)V(\eta_2) = \rho^2 + (1 - \rho^2) = 1$$

E a covari√¢ncia entre $\epsilon_1$ e $\epsilon_2$ √© $\rho$:

$$cov(\epsilon_1, \epsilon_2) = cov[\eta_1, \rho\eta_1 + (1-\rho^2)^{1/2}\eta_2] = \rho cov(\eta_1, \eta_1) = \rho$$

Este processo garante que as vari√°veis $\epsilon$ possuam a correla√ß√£o desejada $\rho$ [^322]. A quest√£o crucial √©, portanto, como escolher essa transforma√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Assuma que $\rho = 0.6$, $\eta_1 = 0.8$ e $\eta_2 = -0.5$. Ent√£o:
>
> $\epsilon_1 = \eta_1 = 0.8$
>
> $\epsilon_2 = 0.6 * 0.8 + \sqrt{1 - 0.6^2} * (-0.5) = 0.48 + 0.8 * (-0.5) = 0.48 - 0.4 = 0.08$
>
> Para verificar a correla√ß√£o, podemos gerar v√°rias amostras de $\eta_1$ e $\eta_2$ e calcular as correspondentes $\epsilon_1$ e $\epsilon_2$.
> ```python
> import numpy as np
>
> rho = 0.6
> num_samples = 1000
> eta1 = np.random.normal(0, 1, num_samples)
> eta2 = np.random.normal(0, 1, num_samples)
>
> epsilon1 = eta1
> epsilon2 = rho * eta1 + np.sqrt(1 - rho**2) * eta2
>
> correlation = np.corrcoef(epsilon1, epsilon2)[0, 1]
> print(f"Correla√ß√£o entre epsilon1 e epsilon2: {correlation}")
> ```
> Isso demonstrar√° que a correla√ß√£o entre $\epsilon_1$ e $\epsilon_2$ se aproxima de $\rho = 0.6$.

#### Fatora√ß√£o de Cholesky

Em um contexto mais geral, considere um vetor de $N$ valores de $\epsilon$, para os quais desejamos impor uma estrutura de correla√ß√£o definida por $V(\epsilon) = E[\epsilon \epsilon'] = R$, onde $R$ √© a matriz de correla√ß√£o [^322]. Para gerar vari√°veis correlacionadas, utiliza-se a **fatora√ß√£o de Cholesky**. Dado que $R$ √© uma matriz sim√©trica real, ela pode ser decomposta em fatores de Cholesky da seguinte forma:

$$
R = T T'
$$

onde $T$ √© uma matriz triangular inferior com zeros nos cantos superiores direitos [^322]. Inicialmente, temos um vetor $N$, $\eta$, composto por vari√°veis independentes com vari√¢ncia unit√°ria, isto √©, $V(\eta) = I$, em que $I$ √© a matriz identidade. Em seguida, constr√≥i-se a vari√°vel $\epsilon = T \eta$. A matriz de covari√¢ncia de $\epsilon$ √© dada por $V(\epsilon) = E[\epsilon \epsilon'] = E[T \eta \eta' T'] = T E[\eta \eta'] T' = T I T' = T T' = R$ [^322]. Assim, √© confirmado que os valores de $\epsilon$ possuem as correla√ß√µes desejadas.

![Fatora√ß√£o de Cholesky](./../images/figure1.png)

#### Exemplo Bivariado

Para ilustrar, considere o caso de duas vari√°veis. A matriz de correla√ß√£o pode ser decomposta da seguinte forma [^322]:

$$
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & 0 \\
a_{12} & a_{22}
\end{bmatrix}
\begin{bmatrix}
a_{11} & a_{12} \\
0 & a_{22}
\end{bmatrix}
=
\begin{bmatrix}
a_{11}^2 & a_{11}a_{12} \\
a_{11}a_{12} & a_{12}^2 + a_{22}^2
\end{bmatrix}
$$

Os elementos do lado direito da equa√ß√£o devem corresponder exatamente a cada elemento da matriz de correla√ß√£o. Uma vez que a matriz de Cholesky √© triangular, os fatores podem ser obtidos por substitui√ß√£o sucessiva, definindo:

$$
\begin{aligned}
a_{11}^2 &= 1 \\
a_{11} a_{12} &= \rho \\
a_{12}^2 + a_{22}^2 &= 1
\end{aligned}
$$

O que resulta em:

$$
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
\rho & \sqrt{1-\rho^2}
\end{bmatrix}
\begin{bmatrix}
1 & \rho \\
0 & \sqrt{1-\rho^2}
\end{bmatrix}
$$

Dessa forma:

$$
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
\rho & \sqrt{1-\rho^2}
\end{bmatrix}
\begin{bmatrix}
\eta_1 \\
\eta_2
\end{bmatrix}
$$

> üí° **Exemplo Num√©rico:**
>
> Seja $\rho = 0.75$. Ent√£o a matriz de Cholesky $T$ √©:
>
> $T = \begin{bmatrix} 1 & 0 \\ 0.75 & \sqrt{1-0.75^2} \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0.75 & 0.6614 \end{bmatrix}$
>
> Agora, gere duas vari√°veis independentes $\eta_1 = 0.4$ e $\eta_2 = -0.2$. Ent√£o:
>
> $\epsilon = T \eta = \begin{bmatrix} 1 & 0 \\ 0.75 & 0.6614 \end{bmatrix} \begin{bmatrix} 0.4 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.4 \\ 0.75 * 0.4 + 0.6614 * (-0.2) \end{bmatrix} = \begin{bmatrix} 0.4 \\ 0.1677 \end{bmatrix}$
>
> Assim, $\epsilon_1 = 0.4$ e $\epsilon_2 = 0.1677$ s√£o as vari√°veis correlacionadas geradas.

#### N√∫mero de Fatores de Risco

Para que a decomposi√ß√£o funcione, a matriz $R$ deve ser **positiva definida**. Caso contr√°rio, n√£o √© poss√≠vel transformar $N$ fontes independentes de risco em $N$ vari√°veis correlacionadas $\epsilon$ [^323]. Conforme discutido no Cap√≠tulo 8, essa condi√ß√£o pode ser verificada utilizando a **decomposi√ß√£o de valor singular (SVD)** [^323]. Essa decomposi√ß√£o da matriz de covari√¢ncia oferece uma verifica√ß√£o de que a matriz est√° bem comportada. Se algum dos autovalores for zero ou menor que zero, a decomposi√ß√£o de Cholesky falhar√° [^323].

Quando a matriz $R$ n√£o √© positiva definida, seu **determinante** √© zero. Intuitivamente, o determinante $d$ √© uma medida do "volume" de uma matriz. Se $d$ √© zero, a dimens√£o da matriz √© menor que $N$ [^323]. O determinante pode ser calculado facilmente a partir da decomposi√ß√£o de Cholesky. Como a matriz $T$ tem zeros acima de sua diagonal, seu determinante se reduz ao produto de todos os coeficientes diagonais $d_T = \prod_{i=1}^{N} a_{ii}$ [^323]. O determinante da matriz de covari√¢ncia $R$ √© ent√£o $d = d_T^2$ [^323].

> üí° **Exemplo Num√©rico:**
>
> Considere a seguinte matriz de correla√ß√£o que *n√£o* √© positiva definida:
> $R = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$
>
> O determinante desta matriz √© $(1*1) - (1*1) = 0$. Isso indica que a matriz n√£o √© positiva definida. Se tentarmos realizar a decomposi√ß√£o de Cholesky, encontraremos um erro, pois $\sqrt{1 - \rho^2}$ envolver√° a raiz quadrada de um n√∫mero negativo quando $\rho = 1$.
>
> Por outro lado, se considerarmos uma matriz positiva definida como:
> $R = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
>
> O determinante √© $(1*1) - (0.5*0.5) = 0.75 > 0$, indicando que a matriz √© positiva definida. A decomposi√ß√£o de Cholesky pode ser realizada sem problemas.

**Teorema 1** Uma matriz de correla√ß√£o $R$ √© positiva semi-definida se e somente se todos os seus autovalores s√£o n√£o negativos.

**Proof:** Seja $R$ uma matriz de correla√ß√£o sim√©trica. Ent√£o, existe uma matriz ortogonal $Q$ tal que $R = QDQ^T$, onde $D$ √© uma matriz diagonal contendo os autovalores de $R$. Para qualquer vetor $x$, temos $x^TRx = x^TQDQ^Tx = (Q^Tx)^TD(Q^Tx)$. Seja $y = Q^Tx$. Ent√£o, $x^TRx = y^TDy = \sum_{i=1}^n \lambda_i y_i^2$, onde $\lambda_i$ s√£o os autovalores de $R$. Se todos os $\lambda_i$ s√£o n√£o negativos, ent√£o $x^TRx \geq 0$ para todo $x$, o que significa que $R$ √© positiva semi-definida. Reciprocamente, se $R$ √© positiva semi-definida, ent√£o $x^TRx \geq 0$ para todo $x$. Escolhendo $x$ como um autovetor de $R$, temos $Rx = \lambda x$, ent√£o $x^TRx = \lambda x^Tx \geq 0$. Como $x^Tx > 0$, devemos ter $\lambda \geq 0$. Portanto, todos os autovalores de $R$ devem ser n√£o negativos. $\blacksquare$

**Teorema 1.1** Se a matriz de correla√ß√£o $R$ n√£o √© positiva definida, ent√£o existe uma matriz de correla√ß√£o $\hat{R}$ que √© positiva definida e pr√≥xima de $R$ no sentido da norma de Frobenius.

**Proof:** Seja $R = QDQ^T$ a decomposi√ß√£o espectral de $R$, onde $Q$ √© uma matriz ortogonal e $D$ √© uma matriz diagonal com os autovalores de $R$ na diagonal. Se $R$ n√£o √© positiva definida, ent√£o alguns autovalores s√£o negativos. Seja $D^+$ a matriz diagonal obtida substituindo os autovalores negativos de $D$ por zero. Ent√£o $\hat{R} = QD^+Q^T$ √© uma matriz positiva semi-definida. Para obter uma matriz positiva definida, podemos adicionar uma pequena perturba√ß√£o √† diagonal de $D^+$. Seja $\epsilon > 0$ um pequeno n√∫mero positivo e seja $D_\epsilon = D^+ + \epsilon I$, onde $I$ √© a matriz identidade. Ent√£o $\hat{R}_\epsilon = QD_\epsilon Q^T$ √© uma matriz positiva definida, pois todos os seus autovalores s√£o maiores ou iguais a $\epsilon$. Al√©m disso, $\hat{R}_\epsilon$ converge para $R$ quando $\epsilon$ tende a zero se os autovalores negativos de $R$ forem substitu√≠dos por zero antes de adicionar $\epsilon$. $\blacksquare$

**Lema 1** Dada uma matriz de covari√¢ncia $R$, a matriz de covari√¢ncia amostral $\hat{R}$ calculada a partir de dados simulados pode n√£o ser positiva definida devido a erros de amostragem, mesmo que a verdadeira matriz de covari√¢ncia $R$ seja positiva definida.

*Proof:* A matriz de covari√¢ncia amostral $\hat{R}$ √© uma estimativa de $R$ baseada em um n√∫mero finito de amostras. Devido √† aleatoriedade do processo de amostragem, $\hat{R}$ pode ter autovalores negativos, mesmo que a verdadeira matriz $R$ seja positiva definida. Isso ocorre especialmente quando o n√∫mero de amostras √© pequeno em rela√ß√£o ao n√∫mero de vari√°veis. $\blacksquare$

Para contornar o problema de matrizes de correla√ß√£o n√£o positivas definidas, uma abordagem comum √© realizar uma transforma√ß√£o na matriz original para torn√°-la positiva definida. Uma dessas transforma√ß√µes √© a introdu√ß√£o de um fator de amortecimento, que consiste em adicionar uma pequena constante √† diagonal da matriz de correla√ß√£o. Formalmente, dada uma matriz de correla√ß√£o $R$, a matriz regularizada $R_{\lambda}$ √© definida como:

$$R_{\lambda} = (1 - \lambda)R + \lambda I$$

onde $I$ √© a matriz identidade e $\lambda$ √© um par√¢metro de regulariza√ß√£o entre 0 e 1.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos a seguinte matriz de correla√ß√£o n√£o positiva definida:
>
> $R = \begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & 0.9 \\ 0.9 & 0.9 & 1 \end{bmatrix}$
>
> Podemos usar Python para verificar se ela √© positiva definida e aplicar a regulariza√ß√£o:
> ```python
> import numpy as np
> from scipy.linalg import eigh
>
> R = np.array([[1, 0.9, 0.9],
>               [0.9, 1, 0.9],
>               [0.9, 0.9, 1]])
>
> # Verificando se a matriz √© positiva definida
> eigenvalues = eigh(R, eigvals_only=True)
> print("Autovalores de R:", eigenvalues)
>
> # Adicionando regulariza√ß√£o
> lambda_value = 0.1
> R_lambda = (1 - lambda_value) * R + lambda_value * np.eye(3)
>
> # Verificando se a matriz regularizada √© positiva definida
> eigenvalues_regularized = eigh(R_lambda, eigvals_only=True)
> print("Autovalores de R_lambda:", eigenvalues_regularized)
> ```
> A sa√≠da mostrar√° que alguns autovalores de `R` s√£o negativos (ou muito pr√≥ximos de zero devido a erros num√©ricos), indicando que n√£o √© positiva definida. Ap√≥s a regulariza√ß√£o, todos os autovalores de `R_lambda` ser√£o positivos.

**Proposi√ß√£o 1** A matriz regularizada $R_{\lambda}$ √© sempre positiva definida para $\lambda > 0$, mesmo que a matriz original $R$ n√£o seja positiva definida.

*Proof:* Seja $\lambda_{min}$ o menor autovalor de $R$. Os autovalores de $R_{\lambda}$ s√£o dados por $(1 - \lambda)\lambda_i + \lambda$, onde $\lambda_i$ s√£o os autovalores de $R$. Portanto, o menor autovalor de $R_{\lambda}$ √© $(1 - \lambda)\lambda_{min} + \lambda$. Se $\lambda_{min} \geq 0$, ent√£o $R$ j√° √© positiva semi-definida, e $R_{\lambda}$ √© positiva definida para $\lambda > 0$. Se $\lambda_{min} < 0$, ent√£o o menor autovalor de $R_{\lambda}$ √© $(1 - \lambda)\lambda_{min} + \lambda$. Para que $R_{\lambda}$ seja positiva definida, precisamos ter $(1 - \lambda)\lambda_{min} + \lambda > 0$, o que implica $\lambda > \frac{\lambda_{min}}{\lambda_{min} - 1}$. Como $\lambda_{min} < 0$, ent√£o $\frac{\lambda_{min}}{\lambda_{min} - 1}$ est√° entre 0 e 1. Portanto, para qualquer $\lambda > 0$, $R_{\lambda}$ √© positiva definida. $\blacksquare$

Para solidificar a compreens√£o da Proposi√ß√£o 1, podemos fornecer uma prova passo a passo mais detalhada:

**Prova Detalhada da Proposi√ß√£o 1:**

Provaremos que a matriz regularizada $R_{\lambda} = (1 - \lambda)R + \lambda I$ √© sempre positiva definida para $\lambda > 0$, mesmo quando a matriz original $R$ n√£o √© positiva definida.

I. **Defini√ß√£o de Matriz Positiva Definida:** Uma matriz $A$ √© positiva definida se $x^T A x > 0$ para todo vetor n√£o nulo $x$.

II. **Autovalores e Matrizes Positivas Definidas:** Uma matriz sim√©trica √© positiva definida se e somente se todos os seus autovalores s√£o positivos.

III. **Autovalores de $R_{\lambda}$:** Seja $\lambda_i$ um autovalor de $R$. Ent√£o, os autovalores de $R_{\lambda}$ s√£o dados por $(1 - \lambda) \lambda_i + \lambda$.

IV. **Caso 1: $R$ √© Positiva Semi-Definida:** Se $R$ √© positiva semi-definida, ent√£o todos os seus autovalores $\lambda_i$ s√£o n√£o negativos ($\lambda_i \geq 0$). Portanto, $(1 - \lambda) \lambda_i + \lambda > 0$ para $\lambda > 0$, pois ambos os termos s√£o n√£o negativos e $\lambda$ √© estritamente positivo.

V. **Caso 2: $R$ n√£o √© Positiva Semi-Definida:** Se $R$ n√£o √© positiva semi-definida, ent√£o ela tem pelo menos um autovalor negativo. Seja $\lambda_{min}$ o menor autovalor de $R$. Para que $R_{\lambda}$ seja positiva definida, precisamos garantir que todos os seus autovalores sejam positivos, ou seja, $(1 - \lambda) \lambda_i + \lambda > 0$ para todos os $i$. Em particular, precisamos garantir que $(1 - \lambda) \lambda_{min} + \lambda > 0$.

VI. **Condi√ß√£o para $\lambda$:** Resolvendo a desigualdade $(1 - \lambda) \lambda_{min} + \lambda > 0$, obtemos:
    $$
    \lambda - \lambda \lambda_{min} + \lambda_{min} > 0 \\
    \lambda (1 - \lambda_{min}) > - \lambda_{min} \\
    \lambda > \frac{-\lambda_{min}}{1 - \lambda_{min}}
    $$
    Como $\lambda_{min} < 0$, ent√£o $-\lambda_{min} > 0$ e $1 - \lambda_{min} > 1$. Portanto, $0 < \frac{-\lambda_{min}}{1 - \lambda_{min}} < 1$.
    Para qualquer $\lambda > 0$, a condi√ß√£o $(1 - \lambda)\lambda_{min} + \lambda > 0$ √© satisfeita.

VII. **Conclus√£o:** Em ambos os casos, $R_{\lambda}$ √© positiva definida para $\lambda > 0$. Portanto, a matriz regularizada $R_{\lambda}$ √© sempre positiva definida para $\lambda > 0$, mesmo que a matriz original $R$ n√£o seja positiva definida. ‚ñ†

Este m√©todo garante que a matriz resultante seja positiva definida, permitindo a aplica√ß√£o da decomposi√ß√£o de Cholesky.

### Conclus√£o

Este cap√≠tulo explorou as nuances da aplica√ß√£o das simula√ß√µes de Monte Carlo em contextos multivariados, destacando a import√¢ncia de capturar corretamente as correla√ß√µes entre os fatores de risco. A fatora√ß√£o de Cholesky surge como uma ferramenta poderosa para gerar vari√°veis correlacionadas a partir de vari√°veis independentes, enquanto a verifica√ß√£o da positividade definida da matriz de correla√ß√£o garante a validade da decomposi√ß√£o. A correta implementa√ß√£o desses m√©todos √© crucial para a obten√ß√£o de medidas de risco precisas e confi√°veis em aplica√ß√µes financeiras complexas.

### Refer√™ncias

[^1]: Cap√≠tulo 12 do livro texto.
[^321]: P√°gina 321 do livro texto.
[^322]: P√°gina 322 do livro texto.
[^323]: P√°gina 323 do livro texto.
<!-- END -->