### Simula√ß√µes com M√∫ltiplas Vari√°veis

#### Introdu√ß√£o

Expandindo o conceito de simula√ß√µes de Monte Carlo introduzido no Cap√≠tulo 12 [^1], esta se√ß√£o explora a aplica√ß√£o dessas t√©cnicas em cen√°rios mais complexos que envolvem m√∫ltiplas vari√°veis de risco. Em continuidade ao que vimos anteriormente sobre simula√ß√µes com uma √∫nica vari√°vel aleat√≥ria, este cap√≠tulo se aprofunda nos m√©todos para simular sistemas financeiros que s√£o intrinsecamente multivariados. As aplica√ß√µes modernas de medi√ß√£o de risco frequentemente demandam a simula√ß√£o de um grande n√∫mero de vari√°veis de risco para refletir adequadamente a complexidade das institui√ß√µes financeiras [^321].

#### Conceitos Fundamentais

##### Simula√ß√µes com M√∫ltiplas Vari√°veis de Risco

A aplica√ß√£o de simula√ß√µes de Monte Carlo em finan√ßas se estende naturalmente a problemas multivariados, onde o valor de um portf√≥lio ou derivativo depende de m√∫ltiplos fatores de risco. Em tais cen√°rios, √© crucial modelar n√£o apenas o comportamento individual de cada fator de risco, mas tamb√©m as interdepend√™ncias entre eles.

##### De Vari√°veis Independentes a Correlacionadas

O ponto de partida para simula√ß√µes com m√∫ltiplas vari√°veis √© frequentemente a gera√ß√£o de vari√°veis aleat√≥rias independentes, que ent√£o precisam ser transformadas para incorporar as correla√ß√µes observadas nos dados reais [^321]. Suponha que tenhamos $N$ fontes de risco. Se essas vari√°veis forem n√£o correlacionadas, a aleatoriza√ß√£o pode ser executada independentemente para cada vari√°vel, resultando na seguinte equa√ß√£o para a varia√ß√£o do pre√ßo do ativo *j* no instante *t*:

$$
\Delta S_{j,t} = S_{j,t-1} (\mu_j \Delta t + \sigma_j \epsilon_j \sqrt{\Delta t})
$$

onde:

*   $\Delta S_{j,t}$ √© a varia√ß√£o no pre√ßo do ativo *j* no instante *t*.
*   $S_{j,t-1}$ √© o pre√ßo do ativo *j* no instante *t-1*.
*   $\mu_j$ √© o drift instant√¢neo do ativo *j*.
*   $\Delta t$ √© o incremento de tempo.
*   $\sigma_j$ √© a volatilidade do ativo *j*.
*   $\epsilon_j$ √© uma vari√°vel aleat√≥ria normalmente distribu√≠da com m√©dia zero e vari√¢ncia unit√°ria.

Nesse contexto, os valores de $\epsilon$ s√£o independentes entre os per√≠odos de tempo e entre as s√©ries *j* = 1, ..., *N* [^321].

> üí° **Exemplo Num√©rico:**
>
> Considere dois ativos, A e B, com pre√ßos iniciais $S_{A,0} = 100$ e $S_{B,0} = 150$, drifts $\mu_A = 0.10$ e $\mu_B = 0.15$, e volatilidades $\sigma_A = 0.20$ e $\sigma_B = 0.25$, respectivamente. Assuma um incremento de tempo $\Delta t = 1/252$ (um dia √∫til em um ano). Gere duas vari√°veis aleat√≥rias independentes, $\epsilon_A = 0.5$ e $\epsilon_B = -0.3$.
>
> $\Delta S_{A,1} = 100 * (0.10 * (1/252) + 0.20 * 0.5 * \sqrt{1/252}) \approx 100 * (0.0004 + 0.0063) \approx 0.67$
>
> $S_{A,1} = S_{A,0} + \Delta S_{A,1} = 100 + 0.67 = 100.67$
>
> $\Delta S_{B,1} = 150 * (0.15 * (1/252) + 0.25 * (-0.3) * \sqrt{1/252}) \approx 150 * (0.0006 + (-0.00047)) \approx 0.02$
>
> $S_{B,1} = S_{B,0} + \Delta S_{B,1} = 150 + 0.02 = 150.02$
>
> Este exemplo demonstra como os pre√ßos dos ativos evoluem em um √∫nico passo de tempo, considerando seus drifts, volatilidades e vari√°veis aleat√≥rias independentes.

**Proposi√ß√£o 2** A Equa√ß√£o para $\Delta S_{j,t}$ pode ser reescrita em termos de retornos logar√≠tmicos.

*Proof:* Dividindo ambos os lados da equa√ß√£o por $S_{j,t-1}$, temos:
$\frac{\Delta S_{j,t}}{S_{j,t-1}} = \mu_j \Delta t + \sigma_j \epsilon_j \sqrt{\Delta t}$
Essa equa√ß√£o representa a varia√ß√£o relativa do pre√ßo do ativo. Para expressar isso em termos de retornos logar√≠tmicos, podemos usar a aproxima√ß√£o $\ln(1+x) \approx x$ para pequenos valores de $x$. Assim, o retorno logar√≠tmico √© aproximadamente:
$\ln(\frac{S_{j,t}}{S_{j,t-1}}) = \ln(1 + \frac{\Delta S_{j,t}}{S_{j,t-1}}) \approx \frac{\Delta S_{j,t}}{S_{j,t-1}} = \mu_j \Delta t + \sigma_j \epsilon_j \sqrt{\Delta t}$.
Portanto, o retorno logar√≠tmico do ativo *j* no instante *t* √© aproximadamente $\mu_j \Delta t + \sigma_j \epsilon_j \sqrt{\Delta t}$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, podemos calcular os retornos logar√≠tmicos para os ativos A e B:
>
> Retorno logar√≠tmico de A: $\ln(\frac{S_{A,1}}{S_{A,0}}) = \ln(\frac{100.67}{100}) \approx \ln(1.0067) \approx 0.006677$
>
> Retorno logar√≠tmico de B: $\ln(\frac{S_{B,1}}{S_{B,0}}) = \ln(\frac{150.02}{150}) \approx \ln(1.000133) \approx 0.000133$
>
> Comparando com a aproxima√ß√£o:
>
> Retorno logar√≠tmico de A (aproximado): $0.10 * (1/252) + 0.20 * 0.5 * \sqrt{1/252} \approx 0.0004 + 0.0063 \approx 0.0067$
>
> Retorno logar√≠tmico de B (aproximado): $0.15 * (1/252) + 0.25 * (-0.3) * \sqrt{1/252} \approx 0.0006 + (-0.00047) \approx 0.00013$
>
> Os retornos logar√≠tmicos calculados diretamente e os aproximados s√£o muito pr√≥ximos, validando a aproxima√ß√£o para pequenos incrementos de tempo.

##### Incorporando Correla√ß√µes

Para levar em conta as correla√ß√µes entre as vari√°veis, inicia-se com um conjunto de vari√°veis independentes $\eta$, que s√£o ent√£o transformadas nas vari√°veis $\epsilon$ [^321]. Em um cen√°rio bivariado, essa transforma√ß√£o pode ser expressa como:

$$
\begin{aligned}
\epsilon_1 &= \eta_1 \\
\epsilon_2 &= \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2
\end{aligned}
$$

onde $\rho$ √© o coeficiente de correla√ß√£o entre as vari√°veis $\epsilon_1$ e $\epsilon_2$ [^321]. Pode-se verificar que a vari√¢ncia de $\epsilon_2$ √© unit√°ria:

$$V(\epsilon_2) = \rho^2 V(\eta_1) + (1-\rho^2)V(\eta_2) = \rho^2 + (1 - \rho^2) = 1$$

**Prova da Vari√¢ncia de $\epsilon_2$:**

Provaremos que a vari√¢ncia de $\epsilon_2$ √© igual a 1, dado que $\epsilon_2 = \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2$ e que $\eta_1$ e $\eta_2$ s√£o vari√°veis aleat√≥rias independentes com m√©dia 0 e vari√¢ncia 1.

I.  **Defini√ß√£o da Vari√¢ncia:** A vari√¢ncia de uma vari√°vel aleat√≥ria $X$ √© definida como $V(X) = E[X^2] - E[X]^2$. Como $E[\eta_1] = E[\eta_2] = 0$, temos $V(\eta_1) = E[\eta_1^2] = 1$ e $V(\eta_2) = E[\eta_2^2] = 1$.

II. **C√°lculo da Vari√¢ncia de $\epsilon_2$:**
    $$
    V(\epsilon_2) = V(\rho \eta_1 + \sqrt{1 - \rho^2} \eta_2)
    $$

III. **Propriedades da Vari√¢ncia:** Usando a propriedade $V(aX + bY) = a^2V(X) + b^2V(Y) + 2abCov(X, Y)$ e o fato de que $\eta_1$ e $\eta_2$ s√£o independentes (portanto, $Cov(\eta_1, \eta_2) = 0$):
     $$
    V(\epsilon_2) = \rho^2 V(\eta_1) + (1 - \rho^2) V(\eta_2) + 2 \rho \sqrt{1 - \rho^2} Cov(\eta_1, \eta_2)
    $$

IV. **Substitui√ß√£o dos Valores:** Como $V(\eta_1) = 1$, $V(\eta_2) = 1$ e $Cov(\eta_1, \eta_2) = 0$:
    $$
    V(\epsilon_2) = \rho^2 (1) + (1 - \rho^2) (1) + 2 \rho \sqrt{1 - \rho^2} (0)
    $$

V.  **Simplifica√ß√£o:**
    $$
    V(\epsilon_2) = \rho^2 + 1 - \rho^2 = 1
    $$

Portanto, a vari√¢ncia de $\epsilon_2$ √© 1. ‚ñ†

E a covari√¢ncia entre $\epsilon_1$ e $\epsilon_2$ √© $\rho$:

$$cov(\epsilon_1, \epsilon_2) = cov[\eta_1, \rho\eta_1 + (1-\rho^2)^{1/2}\eta_2] = \rho cov(\eta_1, \eta_1) = \rho$$

**Prova da Covari√¢ncia entre $\epsilon_1$ e $\epsilon_2$:**

Provaremos que a covari√¢ncia entre $\epsilon_1$ e $\epsilon_2$ √© igual a $\rho$, dado que $\epsilon_1 = \eta_1$, $\epsilon_2 = \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2$, e que $\eta_1$ e $\eta_2$ s√£o vari√°veis aleat√≥rias independentes com m√©dia 0 e vari√¢ncia 1.

I. **Defini√ß√£o da Covari√¢ncia:** A covari√¢ncia entre duas vari√°veis aleat√≥rias $X$ e $Y$ √© definida como $Cov(X, Y) = E[XY] - E[X]E[Y]$.

II. **C√°lculo da Covari√¢ncia:**
    $$
    Cov(\epsilon_1, \epsilon_2) = Cov(\eta_1, \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2)
    $$

III. **Propriedades da Covari√¢ncia:** Usando a propriedade $Cov(X, aY + bZ) = aCov(X, Y) + bCov(X, Z)$:
    $$
    Cov(\epsilon_1, \epsilon_2) = \rho Cov(\eta_1, \eta_1) + \sqrt{1 - \rho^2} Cov(\eta_1, \eta_2)
    $$

IV. **Substitui√ß√£o dos Valores:** Como $Cov(\eta_1, \eta_1) = V(\eta_1) = 1$ e $\eta_1$ e $\eta_2$ s√£o independentes (portanto, $Cov(\eta_1, \eta_2) = 0$):
    $$
    Cov(\epsilon_1, \epsilon_2) = \rho (1) + \sqrt{1 - \rho^2} (0)
    $$

V. **Simplifica√ß√£o:**
    $$
    Cov(\epsilon_1, \epsilon_2) = \rho
    $$

Portanto, a covari√¢ncia entre $\epsilon_1$ e $\epsilon_2$ √© $\rho$. ‚ñ†

Este processo garante que as vari√°veis $\epsilon$ possuam a correla√ß√£o desejada $\rho$ [^322]. A quest√£o crucial √©, portanto, como escolher essa transforma√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Assuma que $\rho = 0.6$, $\eta_1 = 0.8$ e $\eta_2 = -0.5$. Ent√£o:
>
> $\epsilon_1 = \eta_1 = 0.8$
>
> $\epsilon_2 = 0.6 * 0.8 + \sqrt{1 - 0.6^2} * (-0.5) = 0.48 + 0.8 * (-0.5) = 0.48 - 0.4 = 0.08$
>
> Para verificar a correla√ß√£o, podemos gerar v√°rias amostras de $\eta_1$ e $\eta_2$ e calcular as correspondentes $\epsilon_1$ e $\epsilon_2$.
> ```python
> import numpy as np
>
> rho = 0.6
> num_samples = 1000
> eta1 = np.random.normal(0, 1, num_samples)
> eta2 = np.random.normal(0, 1, num_samples)
>
> epsilon1 = eta1
> epsilon2 = rho * eta1 + np.sqrt(1 - rho**2) * eta2
>
> correlation = np.corrcoef(epsilon1, epsilon2)[0, 1]
> print(f"Correla√ß√£o entre epsilon1 e epsilon2: {correlation}")
> ```
> Isso demonstrar√° que a correla√ß√£o entre $\epsilon_1$ e $\epsilon_2$ se aproxima de $\rho = 0.6$.

**Lema 2** A transforma√ß√£o linear descrita pelas equa√ß√µes $\epsilon_1 = \eta_1$ e $\epsilon_2 = \rho \eta_1 + \sqrt{1 - \rho^2} \eta_2$ preserva a normalidade das vari√°veis, ou seja, se $\eta_1$ e $\eta_2$ s√£o normalmente distribu√≠das, ent√£o $\epsilon_1$ e $\epsilon_2$ tamb√©m s√£o normalmente distribu√≠das.

*Proof:* Como $\epsilon_1 = \eta_1$, e $\eta_1$ √© normalmente distribu√≠da por hip√≥tese, ent√£o $\epsilon_1$ √© normalmente distribu√≠da. $\epsilon_2$ √© uma combina√ß√£o linear de duas vari√°veis normais independentes, $\eta_1$ e $\eta_2$. Sabe-se que uma combina√ß√£o linear de vari√°veis normais independentes tamb√©m √© normalmente distribu√≠da. Portanto, $\epsilon_2$ √© normalmente distribu√≠da. $\blacksquare$

##### Fatora√ß√£o de Cholesky

Em um contexto mais geral, considere um vetor de $N$ valores de $\epsilon$, para os quais desejamos impor uma estrutura de correla√ß√£o definida por $V(\epsilon) = E[\epsilon \epsilon'] = R$, onde $R$ √© a matriz de correla√ß√£o [^322]. Para gerar vari√°veis correlacionadas, utiliza-se a **fatora√ß√£o de Cholesky**. Dado que $R$ √© uma matriz sim√©trica real, ela pode ser decomposta em fatores de Cholesky da seguinte forma:

$$
R = T T'
$$

onde $T$ √© uma matriz triangular inferior com zeros nos cantos superiores direitos [^322]. Inicialmente, temos um vetor $N$, $\eta$, composto por vari√°veis independentes com vari√¢ncia unit√°ria, isto √©, $V(\eta) = I$, em que $I$ √© a matriz identidade. Em seguida, constr√≥i-se a vari√°vel $\epsilon = T \eta$. A matriz de covari√¢ncia de $\epsilon$ √© dada por $V(\epsilon) = E[\epsilon \epsilon'] = E[T \eta \eta' T'] = T E[\eta \eta'] T' = T I T' = T T' = R$ [^322]. Assim, √© confirmado que os valores de $\epsilon$ possuem as correla√ß√µes desejadas.

**Prova da Matriz de Covari√¢ncia de $\epsilon$:**

Provaremos que a matriz de covari√¢ncia de $\epsilon$ √© igual a $R$, dado que $\epsilon = T \eta$, onde $T$ √© a matriz triangular inferior da decomposi√ß√£o de Cholesky de $R$ e $\eta$ √© um vetor de vari√°veis independentes com vari√¢ncia unit√°ria.

I. **Defini√ß√£o da Matriz de Covari√¢ncia:** A matriz de covari√¢ncia de um vetor aleat√≥rio $X$ √© definida como $V(X) = E[(X - E[X])(X - E[X])']$. Se $E[X] = 0$, ent√£o $V(X) = E[XX']$.

II. **C√°lculo da Matriz de Covari√¢ncia de $\epsilon$:**
    $$
    V(\epsilon) = E[\epsilon \epsilon']
    $$

III. **Substitui√ß√£o de $\epsilon$:** Como $\epsilon = T \eta$:
    $$
    V(\epsilon) = E[(T \eta) (T \eta)'] = E[T \eta \eta' T']
    $$

IV. **Propriedades da Expectativa:** Usando a propriedade de que $E[AXB] = A E[X] B$ se $A$ e $B$ s√£o constantes:
    $$
    V(\epsilon) = T E[\eta \eta'] T'
    $$

V.  **Matriz de Covari√¢ncia de $\eta$:** Dado que $\eta$ √© um vetor de vari√°veis independentes com vari√¢ncia unit√°ria, a matriz de covari√¢ncia de $\eta$ √© a matriz identidade $I$, ou seja, $E[\eta \eta'] = I$:
    $$
    V(\epsilon) = T I T'
    $$

VI. **Simplifica√ß√£o:**
    $$
    V(\epsilon) = T T'
    $$

VII. **Decomposi√ß√£o de Cholesky:** Pela defini√ß√£o da decomposi√ß√£o de Cholesky, $R = T T'$:
    $$
    V(\epsilon) = R
    $$

Portanto, a matriz de covari√¢ncia de $\epsilon$ √© $R$. ‚ñ†

![Matriz de covari√¢ncia](./../images/figure1.png)

##### Exemplo Bivariado

Para ilustrar, considere o caso de duas vari√°veis. A matriz de correla√ß√£o pode ser decomposta da seguinte forma [^322]:

$$
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & 0 \\
a_{12} & a_{22}
\end{bmatrix}
\begin{bmatrix}
a_{11} & a_{12} \\
0 & a_{22}
\end{bmatrix}
=
\begin{bmatrix}
a_{11}^2 & a_{11}a_{12} \\
a_{11}a_{12} & a_{12}^2 + a_{22}^2
\end{bmatrix}
$$

Os elementos do lado direito da equa√ß√£o devem corresponder exatamente a cada elemento da matriz de correla√ß√£o. Uma vez que a matriz de Cholesky √© triangular, os fatores podem ser obtidos por substitui√ß√£o sucessiva, definindo:

$$
\begin{aligned}
a_{11}^2 &= 1 \\
a_{11} a_{12} &= \rho \\
a_{12}^2 + a_{22}^2 &= 1
\end{aligned}
$$

**Prova da Deriva√ß√£o dos Elementos da Matriz de Cholesky:**

Provaremos que os elementos da matriz de Cholesky podem ser obtidos por substitui√ß√£o sucessiva, dadas as equa√ß√µes $a_{11}^2 = 1$, $a_{11} a_{12} = \rho$ e $a_{12}^2 + a_{22}^2 = 1$.

I.  **Primeira Equa√ß√£o:** $a_{11}^2 = 1$. Resolvendo para $a_{11}$, obtemos $a_{11} = \pm 1$. Por conven√ß√£o, escolhemos $a_{11} = 1$.

II. **Segunda Equa√ß√£o:** $a_{11} a_{12} = \rho$. Substituindo $a_{11} = 1$, obtemos $a_{12} = \rho$.

III. **Terceira Equa√ß√£o:** $a_{12}^2 + a_{22}^2 = 1$. Substituindo $a_{12} = \rho$, obtemos $\rho^2 + a_{22}^2 = 1$. Resolvendo para $a_{22}$, obtemos $a_{22}^2 = 1 - \rho^2$, e assim $a_{22} = \pm \sqrt{1 - \rho^2}$. Por conven√ß√£o, escolhemos $a_{22} = \sqrt{1 - \rho^2}$.

Portanto, os elementos da matriz de Cholesky s√£o $a_{11} = 1$, $a_{12} = \rho$ e $a_{22} = \sqrt{1 - \rho^2}$. ‚ñ†

O que resulta em:

$$
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
\rho & \sqrt{1-\rho^2}
\end{bmatrix}
\begin{bmatrix}
1 & \rho \\
0 & \sqrt{1-\rho^2}
\end{bmatrix}
$$

Dessa forma:

$$
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
\rho & \sqrt{1-\rho^2}
\end{bmatrix}
\begin{bmatrix}
\eta_1 \\
\eta_2
\end{bmatrix}
$$

> üí° **Exemplo Num√©rico:**
>
> Seja $\rho = 0.75$. Ent√£o a matriz de Cholesky $T$ √©:
>
> $T = \begin{bmatrix} 1 & 0 \\ 0.75 & \sqrt{1-0.75^2} \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0.75 & 0.6614 \end{bmatrix}$
>
> Agora, gere duas vari√°veis independentes $\eta_1 = 0.4$ e $\eta_2 = -0.2$. Ent√£o:
>
> $\epsilon = T \eta = \begin{bmatrix} 1 & 0 \\ 0.75 & 0.6614 \end{bmatrix} \begin{bmatrix} 0.4 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.4 \\ 0.75 * 0.4 + 0.6614 * (-0.2) \end{bmatrix} = \begin{bmatrix} 0.4 \\ 0.1677 \end{bmatrix}$
>
> Assim, $\epsilon_1 = 0.4$ e $\epsilon_2 = 0.1677$ s√£o as vari√°veis correlacionadas geradas.

> üí° **Exemplo Num√©rico:**
>
> Para visualizar o efeito da correla√ß√£o, podemos simular um grande n√∫mero de amostras e plotar os resultados.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> rho = 0.75
> num_samples = 1000
>
> # Matriz de Cholesky
> T = np.array([[1, 0],
>               [rho, np.sqrt(1 - rho**2)]])
>
> # Vari√°veis independentes
> eta = np.random.normal(0, 1, size=(2, num_samples))
>
> # Vari√°veis correlacionadas
> epsilon = T @ eta
>
> # Plotagem
> plt.figure(figsize=(8, 6))
> plt.scatter(epsilon[0], epsilon[1], alpha=0.5)
> plt.xlabel('Epsilon 1')
> plt.ylabel('Epsilon 2')
> plt.title(f'Simula√ß√£o de Vari√°veis Correlacionadas (rho={rho})')
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo gera um gr√°fico de dispers√£o mostrando a rela√ß√£o entre $\epsilon_1$ e $\epsilon_2$. A forma el√≠ptica dos pontos demonstra visualmente a correla√ß√£o entre as vari√°veis.

##### N√∫mero de Fatores de Risco

Para que a decomposi√ß√£o funcione, a matriz $R$ deve ser **positiva definida**. Caso contr√°rio, n√£o √© poss√≠vel transformar $N$ fontes independentes de risco em $N$ vari√°veis correlacionadas $\epsilon$ [^323]. Conforme discutido no Cap√≠tulo 8, essa condi√ß√£o pode ser verificada utilizando a **decomposi√ß√£o de valor singular (SVD)** [^323]. Essa decomposi√ß√£o da matriz de covari√¢ncia oferece uma verifica√ß√£o de que a matriz est√° bem comportada. Se algum dos autovalores for zero ou menor que zero, a decomposi√ß√£o de Cholesky falhar√° [^323].

Quando a matriz $R$ n√£o √© positiva definida, seu **determinante** √© zero. Intuitivamente, o determinante $d$ √© uma medida do "volume" de uma matriz. Se $d$ √© zero, a dimens√£o da matriz √© menor que $N$ [^323]. O determinante pode ser calculado facilmente a partir da decomposi√ß√£o de Cholesky. Como a matriz $T$ tem zeros acima de sua diagonal, seu determinante se reduz ao produto de todos os coeficientes diagonais $d_T = \prod_{i=1}^{N} a_{ii}$ [^323]. O determinante da matriz de covari√¢ncia $R$ √© ent√£o $d = d_T^2$ [^323].

**Prova do Determinante da Matriz de Covari√¢ncia $R$:**

Provaremos que o determinante da matriz de covari√¢ncia $R$ √© igual a $d_T^2$, onde $d_T$ √© o determinante da matriz triangular inferior $T$ da decomposi√ß√£o de Cholesky de $R$.

I. **Decomposi√ß√£o de Cholesky:** Pela decomposi√ß√£o de Cholesky, $R = T T'$.

II. **Determinante do Produto de Matrizes:** O determinante do produto de duas matrizes √© o produto dos determinantes, ou seja, $det(AB) = det(A) det(B)$. Portanto, $det(R) = det(T T') = det(T) det(T')$.

III. **Determinante da Transposta:** O determinante de uma matriz transposta √© igual ao determinante da matriz original, ou seja, $det(T') = det(T)$. Portanto, $det(R) = det(T) det(T) = det(T)^2$.

IV. **Determinante de uma Matriz Triangular:** O determinante de uma matriz triangular (superior ou inferior) √© o produto dos elementos diagonais. Portanto, $det(T) = \prod_{i=1}^{N} a_{ii} = d_T$, onde $a_{ii}$ s√£o os elementos diagonais de $T$.

V. **Substitui√ß√£o:** Substituindo $det(T) = d_T$ na equa√ß√£o $det(R) = det(T)^2$, obtemos $det(R) = d_T^2$.

Portanto, o determinante da matriz de covari√¢ncia $R$ √© $d = d_T^2$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere a seguinte matriz de correla√ß√£o que *n√£o* √© positiva definida:
> $R = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$
>
> O determinante desta matriz √© $(1*1) - (1*1) = 0$. Isso indica que a matriz n√£o √© positiva definida. Se tentarmos realizar a decomposi√ß√£o de Cholesky, encontraremos um erro, pois $\sqrt{1 - \rho^2}$ envolver√° a raiz quadrada de um n√∫mero negativo quando $\rho = 1$.
>
> Por outro lado, se considerarmos uma matriz positiva definida como:
> $R = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
>
> O determinante √© $(1*1) - (0.5*0.5) = 0.75 > 0$, indicando que a matriz √© positiva definida. A decomposi√ß√£o de Cholesky pode ser realizada sem problemas.

**Teorema 1** Uma matriz de correla√ß√£o $R$ √© positiva semi-definida se e somente se todos os seus autovalores s√£o n√£o negativos.

**Proof:** Seja $R$ uma matriz de correla√ß√£o sim√©trica. Ent√£o, existe uma matriz ortogonal $Q$ tal que $R = QDQ^T$, onde $D$ √© uma matriz diagonal contendo os autovalores de $R$. Para qualquer vetor $x$, temos $x^TRx = x^TQDQ^Tx = (Q^Tx)^TD(Q^Tx)$. Seja $y = Q^Tx$. Ent√£o, $x^TRx = y^TDy = \sum_{i=1}^n \lambda_i y_i^2$, onde $\lambda_i$ s√£o os autovalores de $R$. Se todos os $\lambda_i$ s√£o n√£o negativos, ent√£o $x^TRx \geq 0$ para todo $x$, o que significa que $R$ √© positiva semi-definida. Reciprocamente, se $R$ √© positiva semi-definida, ent√£o $x^TRx \geq 0$ para todo $x$. Escolhendo $x$ como um autovetor de $R$, temos $Rx = \lambda x$, ent√£o $x^TRx = \lambda x^Tx \geq 0$. Como $x^Tx > 0$, devemos ter $\lambda \geq 0$. Portanto, todos os autovalores de $R$ devem ser n√£o negativos. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere a matriz $R = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$. Podemos calcular os autovalores usando NumPy:
> ```python
> import numpy as np
>
> R = np.array([[1, 2], [2, 1]])
> eigenvalues = np.linalg.eigvals(R)
> print("Autovalores de R:", eigenvalues)
> ```
> Os autovalores resultantes ser√£o aproximadamente `[ 3., -1.]`. Como um dos autovalores √© negativo, a matriz n√£o √© positiva definida.

**Teorema 1.1** Se a matriz de correla√ß√£o $R$ n√£o √© positiva definida, ent√£o existe uma matriz de correla√ß√£o $\hat{R}$ que √© positiva definida e pr√≥xima de $R$ no sentido da norma de Frobenius.

**Proof:** Seja $R = QDQ^T$ a decomposi√ß√£o espectral de $R$, onde $Q$ √© uma matriz ortogonal e $D$ √© uma matriz diagonal com os autovalores de $R$ na diagonal. Se $R$ n√£o √© positiva definida, ent√£o alguns autovalores s√£o negativos. Seja $D^+$ a matriz diagonal obtida substituindo os autovalores negativos de $D$ por zero. Ent√£o $\hat{R} = QD^+Q^T$ √© uma matriz positiva semi-definida. Para obter uma matriz positiva definida, podemos adicionar uma pequena perturba√ß√£o √† diagonal de $D^+$. Seja $\epsilon > 0$ um pequeno n√∫mero positivo e seja $D_\epsilon = D^+ + \epsilon I$, onde $I$ √© a matriz identidade. Ent√£o $\hat{R}_\epsilon = QD_\epsilon Q^T$ √© uma matriz positiva definida, pois todos os seus autovalores s√£o maiores ou iguais a $\epsilon$. Al√©m disso, $\hat{R}_\epsilon$ converge para $R$ quando $\epsilon$ tende a zero se os autovalores negativos de $R$ forem substitu√≠dos por zero antes de adicionar $\epsilon$. $\blacksquare$

**Lema 1** Dada uma matriz de covari√¢ncia $R$, a matriz de covari√¢ncia amostral $\hat{R}$ calculada a partir de dados simulados pode n√£o ser positiva definida devido a erros de amostragem, mesmo que a verdadeira matriz de covari√¢ncia $R$ seja positiva definida.

*Proof:* A matriz de covari√¢ncia amostral $\hat{R}$ √© uma estimativa de $R$ baseada em um n√∫mero finito de amostras. Devido √† aleatoriedade do processo de amostragem, $\hat{R}$ pode ter autovalores negativos, mesmo que a verdadeira matriz $R$ seja positiva definida. Isso ocorre especialmente quando o n√∫mero de amostras √© pequeno em rela√ß√£o ao n√∫mero de vari√°veis. $\blacksquare$

Para contornar o problema de matrizes de correla√ß√£o n√£o positivas definidas, uma abordagem comum √© realizar uma transforma√ß√£o na matriz original para torn√°-la positiva definida. Uma dessas transforma√ß√µes √© a introdu√ß√£o de um fator de amortecimento, que consiste em adicionar uma pequena constante √† diagonal da matriz de correla√ß√£o. Formalmente, dada uma matriz de correla√ß√£o $R$, a matriz regularizada $R_{\lambda}$ √© definida como:

$$R_{\lambda} = (1 - \lambda)R + \lambda I$$

onde $I$ √© a matriz identidade e $\lambda$ √© um par√¢metro de regulariza√ß√£o entre 0 e 1.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos a seguinte matriz de correla√ß√£o n√£o positiva definida:
>
> $R = \begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & 0.9 \\ 0.9 & 0.9 & 1 \end{bmatrix}$
>
> Podemos usar Python para verificar se ela √© positiva definida e aplicar a regulariza√ß√£o:
> ```python
> import numpy as np
> from scipy.linalg import eigh
>
> R = np.array([[1, 0.9, 0.9],
>               [0.9, 1, 0.9],
>               [0.9, 0.9, 1]])
>
> # Verificando se a matriz √© positiva definida
> eigenvalues = eigh(R, eigvals_only=True)
> print("Autovalores de R:", eigenvalues)
>
> # Adicionando regulariza√ß√£o
> lambda_value = 0.1
> R_lambda = (1 - lambda_value) * R + lambda_value * np.eye```python
(R.shape[0])

# Autovalores da matriz regularizada
eigenvalues_lambda = eigh(R_lambda, eigvals_only=True)
print("Autovalores de R com regulariza√ß√£o:", eigenvalues_lambda)
```

**Interpreta√ß√£o:**

A regulariza√ß√£o linear (ridge regression) ajuda a condicionar a matriz de covari√¢ncia `R`, adicionando um m√∫ltiplo da matriz identidade. Isso desloca os autovalores para cima, garantindo que a matriz permane√ßa bem condicionada e invert√≠vel, mesmo que `R` seja originalmente singular ou mal condicionada.  Isto √© especialmente √∫til em cen√°rios de aprendizado de m√°quina onde a multicolinearidade pode inflar os coeficientes de regress√£o.

### Otimiza√ß√£o de Portf√≥lio com CAPM

O Modelo de Precifica√ß√£o de Ativos de Capital (CAPM) fornece uma estrutura para determinar o retorno esperado de um ativo ou portf√≥lio em rela√ß√£o ao risco de mercado.

**F√≥rmula CAPM:**

$E(R_i) = R_f + \beta_i (E(R_m) - R_f)$

Onde:
- $E(R_i)$ √© o retorno esperado do ativo $i$
- $R_f$ √© a taxa de retorno livre de risco
- $\beta_i$ √© o beta do ativo $i$, medindo sua sensibilidade aos movimentos do mercado
- $E(R_m)$ √© o retorno esperado do mercado

#### C√°lculo do Beta ($\beta$)

O beta de um ativo √© calculado como:

$\beta_i = \frac{Cov(R_i, R_m)}{Var(R_m)}$

Onde:
- $Cov(R_i, R_m)$ √© a covari√¢ncia entre os retornos do ativo $i$ e os retornos do mercado
- $Var(R_m)$ √© a vari√¢ncia dos retornos do mercado

#### Implementa√ß√£o

1.  **Calcular Retornos:** Calcule os retornos simples dos ativos e do √≠ndice de mercado.
2.  **Calcular Covari√¢ncia e Vari√¢ncia:** Calcule a covari√¢ncia entre os retornos dos ativos e os retornos do mercado, bem como a vari√¢ncia dos retornos do mercado.
3.  **Calcular Betas:** Use a f√≥rmula acima para calcular o beta de cada ativo.
4.  **Calcular Retornos Esperados:** Use a f√≥rmula CAPM para calcular o retorno esperado de cada ativo.

```python
import numpy as np
import pandas as pd

# Dados de exemplo (substitua pelos seus dados reais)
# Retornos di√°rios de dois ativos e do √≠ndice de mercado
np.random.seed(42)
retornos_ativos = pd.DataFrame({
    'Ativo1': np.random.normal(0.0005, 0.01, 100),
    'Ativo2': np.random.normal(0.0002, 0.015, 100)
})
retornos_mercado = pd.Series(np.random.normal(0.0003, 0.009, 100))
taxa_livre_de_risco = 0.0001  # Exemplo: 0.01%

# 1. Calcular Covari√¢ncias
covariancias = retornos_ativos.cov(retornos_mercado)

# 2. Calcular Vari√¢ncia do Mercado
variancia_mercado = retornos_mercado.var()

# 3. Calcular Betas
betas = covariancias / variancia_mercado
print("Betas dos ativos:\n", betas)

# 4. Calcular Retornos Esperados (CAPM)
retorno_esperado_mercado = retornos_mercado.mean() * 252  # Anualizando
retornos_esperados = taxa_livre_de_risco + betas * (retorno_esperado_mercado - taxa_livre_de_risco)
print("\nRetornos esperados (CAPM):\n", retornos_esperados)
```

**Considera√ß√µes Adicionais:**

*   **Retornos Anualizados:**  Multiplicar os retornos di√°rios m√©dios por 252 (o n√∫mero aproximado de dias √∫teis em um ano) para anualizar os retornos.
*   **Taxa Livre de Risco:** A taxa livre de risco √© geralmente obtida a partir de t√≠tulos do governo com vencimento semelhante ao horizonte de investimento.
*   **Dados Hist√≥ricos:** O CAPM depende de dados hist√≥ricos para calcular betas e retornos esperados. A precis√£o das previs√µes depende da estabilidade das rela√ß√µes hist√≥ricas.
*   **Simplifica√ß√µes:** O CAPM √© um modelo simplificado e tem limita√ß√µes.  Ele assume que os investidores s√£o racionais, avessos ao risco, e t√™m expectativas homog√™neas. Na pr√°tica, essas suposi√ß√µes podem n√£o ser v√°lidas.

<!-- END -->