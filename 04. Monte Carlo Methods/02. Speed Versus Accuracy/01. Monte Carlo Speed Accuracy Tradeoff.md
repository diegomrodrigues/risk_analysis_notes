## 12.3 Speed Versus Accuracy in Monte Carlo Simulations

### Introdução
O uso de métodos de Monte Carlo (MC) em finanças é amplo, especialmente na avaliação de derivativos complexos e na medição de riscos [^1, ^307]. No entanto, uma limitação fundamental desses métodos reside em suas demandas computacionais [^316]. A necessidade de um grande número de replicações para alcançar uma precisão aceitável torna-se ainda mais pronunciada em portfólios com muitos ativos e instrumentos complexos, exigindo *“simulação dentro de uma simulação”* [^316]. Este capítulo explora o *trade-off* entre velocidade e precisão em simulações de Monte Carlo, focando em como o número de replicações afeta a precisão dos resultados.

### Conceitos Fundamentais

A principal desvantagem dos métodos de Monte Carlo (MC) reside em seus requisitos de tempo computacional [^316]. Para ilustrar, considere um portfólio exposto a apenas um fator de risco. Suponha que sejam necessárias 10.000 replicações desse fator de risco para uma precisão aceitável. Se o portfólio contiver 1.000 ativos a serem precificados usando avaliações completas, serão necessárias 10 milhões de avaliações [^316].

Além disso, se o portfólio contiver instrumentos complexos, como hipotecas ou opções exóticas, cuja avaliação em si requer uma simulação, a medição do risco em uma data-alvo exigirá uma *“simulação dentro de uma simulação”* [^316]. Isso significa que para a avaliação (ou seja, do horizonte VAR até o vencimento do instrumento) e para o gerenciamento de risco (ou seja, do presente até o horizonte VAR), o número de simulações necessárias pode atingir valores astronômicos [^316].

#### 12.3.1 Precisão
As simulações inevitavelmente geram variabilidade de amostragem, ou variações nos valores dos estimadores, devido ao número limitado de replicações [^316]. Mais replicações levam a estimativas mais precisas, mas levam mais tempo para serem estimadas [^316]. Definindo $K$ como o número de replicações ou tentativas pseudoaleatórias, é útil avaliar o *trade-off* entre precisão e o número de replicações [^316].

A Figura 12-3 [^317] ilustra a convergência da distribuição empírica na Figura 12-1 [^311] em direção à verdadeira. Com $K = 100$, o histograma que representa a distribuição do preço final é bastante irregular. O histograma torna-se mais suave com 1000 replicações, ainda mais com 10.000 replicações, e eventualmente deve convergir para a distribuição contínua no painel direito [^317]. Uma vantagem do método de Monte Carlo é que o usuário pode avaliar o aumento na precisão diretamente conforme o número de replicações aumenta [^317].

Se o processo subjacente for normal, a distribuição empírica deve convergir para uma distribuição normal [^317]. Nesta situação, a análise de Monte Carlo deve produzir exatamente o mesmo resultado que o método delta-normal: o VAR estimado a partir do quantil da amostra deve convergir para o valor de $\alpha\sigma$ [^317]. Qualquer desvio deve ser devido à variação da amostragem [^317]. Assumindo que não haja outra fonte de erro, este efeito pode ser medido pelo erro padrão assintótico para o quantil da amostra relatado no Capítulo 5, usando $K$ como o tamanho da amostra [^317]. Um método simples para avaliar a precisão é repetir as simulações várias vezes, digamos, $M = 1000$, e tomar o erro padrão dos quantis estimados nos experimentos $M$ [^317].

A Tabela 12-2 [^318] descreve os resultados de 1000 execuções de simulação em uma distribuição normal padrão com um número crescente de replicações. A tabela mostra que para um VAR de 99% com 100 replicações, o erro padrão da estimativa em torno de -2,326 é 0,409, bastante alto [^318]. Em nossa amostra de 1000 execuções, a estimativa de VAR variou de -4,17 a -1,53 [^318]. Essa dispersão é bastante perturbadora. Para aumentar a precisão do VAR em um fator de 10, precisamos aumentar o número de replicações em um fator de 100, para um total de 10.000 [^318]. De fato, a primeira linha mostra que isso diminui o erro padrão para 0,037, que é aproximadamente 0,409 dividido por 10 [^318]. Observe que, em contraste, o desvio padrão é estimado com muito mais precisão porque usa dados de toda a distribuição [^318].

Também poderíamos relatar o erro padrão em termos relativos, definido como a razão entre o erro padrão e o valor esperado da medida de risco [^318]. Por exemplo, os bancos normalmente relatam seu VAR de 99% usando cerca de 500 dias [^318]. Da Tabela 12-2 [^318], isso leva a um erro relativo no VAR de cerca de 0,170/2,326 = 7,3 por cento. O erro relativo depende do número de replicações, bem como da forma da distribuição, como mostrado na Tabela 12-3 [^318, ^319]. A tabela mostra que o erro é maior para distribuições com assimetria à esquerda e, inversamente, menor para distribuições com assimetria à direita [^319]. Isso ocorre porque quanto mais longa a cauda esquerda, menos precisa é a estimativa de VAR [^319].

Alternativamente, poderíamos procurar o número de replicações necessárias para medir o VAR com um erro relativo de 1 por cento [^319]. Para a distribuição normal, precisamos de mais de 20.000 replicações para garantir que o erro relativo na primeira linha esteja abaixo de 1 por cento [^319].

#### 12.3.2 Métodos de Aceleração
Isso levou a uma busca por métodos para acelerar os cálculos [^319]. Um dos primeiros e mais fáceis é a *técnica da variável antitética*, que consiste em mudar o sinal de todas as amostras aleatórias $\epsilon$ [^319]. Este método, que é apropriado quando a distribuição original é simétrica, cria o dobro do número de replicações para os fatores de risco com pouco custo adicional [^319]. Ainda precisamos, no entanto, do dobro do número original de avaliações completas na data-alvo [^319].

Esta abordagem pode ser aplicada ao método de simulação histórica, onde podemos adicionar um vetor de mudanças de preços históricos com o sinal invertido [^319]. Isso também é útil para eliminar o efeito de tendências nos dados históricos recentes [^319].

Outra ferramenta útil é a *técnica das variáveis de controle* [^319]. Estamos tentando estimar o VAR, uma função da amostra de dados. Chame isso de $V(X)$ [^319]. Suponha agora que a função possa ser aproximada por outra função, como uma aproximação quadrática $V^Q(X)$, para a qual temos uma solução de forma fechada $v^Q$ [^319]. Para qualquer amostra, o erro é então conhecido como $V^Q(X) - v^Q$ para a aproximação quadrática [^320]. Se este erro estiver altamente correlacionado com o erro de amostragem em $V(X)$, o estimador de variável de controle pode ser tomado como

$$\nV_{CV} = V(X) - [V^Q(X) - v^Q] \qquad (12.5)\n$$

Este estimador tem uma variância muito menor do que o original quando a função quadrática fornece uma boa aproximação da verdadeira função [^320]. O método de aceleração mais eficaz é a *técnica de amostragem de importância*, que tenta amostrar ao longo dos caminhos que são mais importantes para o problema em questão [^320]. A ideia é que, se nosso objetivo é medir um quantil de cauda com precisão, não há sentido em fazer simulações que gerarão observações no centro da distribuição [^320]. O método envolve mudanças na distribuição de variáveis aleatórias [^320]. Glasserman et al. (2000) mostram que, em relação ao método de Monte Carlo usual, a variância dos estimadores VAR pode ser reduzida por um fator de pelo menos 10 [^320].

Uma aplicação relacionada é a *técnica de amostragem estratificada*, que pode ser explicada intuitivamente da seguinte forma: suponha que precisamos de VAR para uma posição longa em uma opção de compra [^320]. Estamos tentando manter o número de replicações em $K = 1000$ [^320]. Para aumentar a precisão do estimador VAR, podemos particionar a região de simulação em duas zonas [^320]. Como antes, começamos com uma distribuição uniforme, que então é transformada em uma distribuição normal para o preço do ativo subjacente usando o *método de transformação inversa* [^320].

Defina essas duas zonas, ou estratos, para a distribuição uniforme como $[0.0, 0.1]$ e $[0.1, 1.0]$ [^320]. Assim, a *estratificação* é o processo de agrupar os dados em regiões mutuamente exclusivas e coletivamente exaustivas [^320]. Normalmente, as probabilidades do número aleatório cair em ambas as zonas são selecionadas como $p_1 = 10$ por cento e $p_2 = 90$ por cento, respectivamente [^320]. Agora mudamos essas probabilidades para 50 por cento para ambas as regiões [^320]. O número de observações agora é $K_1 = 500$ para a primeira região e $K_2 = 500$ para a segunda [^320]. Isso aumenta o número de amostras para o fator de risco na primeira região, cauda esquerda [^320].

Os estimadores para a média precisam ser ajustados para a estratificação [^320]. Ponderamos o estimador para cada região por sua probabilidade, isto é,

$$\nE(F_T) = p_1 \frac{\sum_{i=1}^{K_1} F_i}{K_1} + p_2 \frac{\sum_{i=1}^{K_2} F_i}{K_2} \qquad (12.6)\n$$

Para calcular o VAR, simplesmente examinamos a primeira região [^321]. O quantil de 50 por cento para a primeira região, por exemplo, fornece um estimador de um VAR de cauda esquerda de 10 × 0,5 = 5 por cento [^321]. Como o VAR usa apenas o número de observações na região certa, nem sequer precisamos calcular seu valor, o que economiza no tempo necessário para a avaliação completa [^321].

Isso reflete o princípio geral de que usar mais informações sobre a distribuição do portfólio resulta em simulações mais eficientes [^321]. Em geral, infelizmente, a função de *payoff* não é conhecida [^321]. Nem tudo está perdido, no entanto [^321]. Em vez disso, a simulação pode prosseguir em duas passagens [^321]. A primeira passagem executa um Monte Carlo tradicional [^321]. O gerente de risco então examina a região dos fatores de risco que causam perdas em torno do VAR [^321]. Uma segunda passagem é então realizada com muito mais amostras desta região [^321].

### Conclusão

Em resumo, a escolha do número de replicações em simulações de Monte Carlo é um equilíbrio entre a precisão desejada e os recursos computacionais disponíveis. Técnicas de aceleração, como variáveis antitéticas, variáveis de controle e amostragem de importância, podem ajudar a reduzir a carga computacional sem sacrificar a precisão. A compreensão dessas técnicas e suas limitações é crucial para a aplicação eficaz de métodos de Monte Carlo na avaliação de riscos financeiros [^316].

### Referências
[^1]: Capítulo 12 do livro.
[^307]: Página 307 do livro.
[^311]: Página 311 do livro.
[^316]: Página 316 do livro.
[^317]: Página 317 do livro.
[^318]: Página 318 do livro.
[^319]: Página 319 do livro.
[^320]: Página 320 do livro.
[^321]: Página 321 do livro.

<!-- END -->