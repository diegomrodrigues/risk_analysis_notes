## Avaliação de Desempenho em Previsão de Probabilidade de Excedência

### Introdução

A avaliação rigorosa de modelos preditivos é um pilar fundamental em qualquer domínio de análise de dados, e a previsão de probabilidade de excedência (Exceedance Probability Forecasting) não é exceção. Como explorado no contexto da previsão de Significant Wave Height (SWH) [^5], o objetivo frequentemente transcende a simples previsão de valores pontuais, focando na estimativa da probabilidade de uma variável ultrapassar um limiar crítico pré-definido [^6]. Esta tarefa pode ser abordada através de modelos de classificação binária, ensembles de previsões [^7], ou métodos que transformam previsões pontuais em probabilidades, como a abordagem baseada na Cumulative Distribution Function (CDF) [^8]. Dada a diversidade de abordagens e a natureza específica da tarefa, torna-se crucial empregar métricas de avaliação adequadas e métodos estatísticos robustos para comparar o desempenho dos modelos de forma significativa. Este capítulo detalha as metodologias de avaliação pertinentes, focando nas métricas específicas para tarefas de probabilidade e regressão, bem como na análise da significância estatística das diferenças de desempenho observadas, baseando-se exclusivamente nas práticas descritas no contexto experimental fornecido.

### Conceitos Fundamentais

#### Métricas para Previsão de Probabilidade de Excedência

A avaliação do desempenho na previsão da probabilidade de excedência, que é inerentemente uma tarefa probabilística, requer métricas que capturem a qualidade das probabilidades estimadas. Duas métricas centrais são utilizadas para este fim [^1]:

1.  **Area Under the ROC Curve (AUC):** A AUC é uma métrica amplamente utilizada para avaliar o desempenho de modelos de classificação binária, que é a formulação tradicional para a previsão de excedência [^7]. A curva ROC (Receiver Operating Characteristic) plota a taxa de verdadeiros positivos contra a taxa de falsos positivos para diferentes limiares de decisão. A AUC representa a área sob esta curva, variando de 0 a 1. Um valor de AUC de 1 indica um classificador perfeito, enquanto um valor de 0.5 sugere um desempenho não superior ao aleatório. No contexto da probabilidade de excedência, a AUC mede a capacidade do modelo de discriminar corretamente entre instâncias que excederão o limiar e aquelas que não o farão, independentemente da calibração das probabilidades.

2.  **Log Loss:** A log loss (ou perda logarítmica) é uma métrica de avaliação que penaliza previsões probabilísticas incorretas, atribuindo uma penalidade maior quanto mais confiante e errada for a previsão. Para um evento binário (excedência ou não), a log loss é calculada como $- \frac{1}{N} \sum_{i=1}^{N} [b_i \log(p_i) + (1 - b_i) \log(1 - p_i)]$, onde $N$ é o número de observações, $b_i$ é o valor binário real (1 se excedeu, 0 caso contrário) [^7], e $p_i$ é a probabilidade estimada de excedência. Valores menores de log loss indicam melhor desempenho, refletindo tanto a capacidade discriminativa quanto a calibração das probabilidades previstas.

Estas métricas são essenciais para quantificar quão bem um modelo estima a *probabilidade* do evento de excedência ocorrer [^1].

#### Métricas para Avaliação de Regressão (Desempenho da Previsão Numérica)

Embora o objetivo final seja a probabilidade de excedência, algumas abordagens, como a baseada na CDF [^8], dependem fundamentalmente de previsões pontuais (numeric forecasts) [^9]. Avaliar o desempenho destas previsões numéricas subjacentes é, portanto, igualmente importante, pois a sua qualidade impacta diretamente a estimativa de probabilidade derivada. As métricas típicas de regressão são usadas para *measure performance in predicting numeric values* [^2]:

1.  **Mean Absolute Error (MAE):** Calcula a média das diferenças absolutas entre os valores previstos ($\hat{y}_i$) e os valores reais ($y_i$). $MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$. Fornece uma medida do erro médio na mesma unidade da variável original.
2.  **Mean Absolute Percentage Error (MAPE):** Calcula a média percentual das diferenças absolutas. $MAPE = \frac{100\\%}{N} \sum_{i=1}^{N} |\frac{y_i - \hat{y}_i}{y_i}|$. É útil para comparar erros entre séries com escalas diferentes, mas pode ser problemática quando os valores reais ($y_i$) são próximos de zero.
3.  **Coefficient of Determination ($R^2$):** Representa a proporção da variância na variável dependente que é previsível a partir das variáveis independentes. Varia de $-\infty$ a 1, onde 1 indica que o modelo explica toda a variabilidade dos dados de resposta em torno da sua média. $R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}$, onde $\bar{y}$ é a média dos valores reais.
4.  **Root Mean Squared Error (RMSE):** É a raiz quadrada do erro quadrático médio ($MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$). $RMSE = \sqrt{MSE}$. Penaliza erros maiores de forma mais significativa que o MAE e está na mesma unidade da variável original.

Estas métricas [^2] fornecem uma avaliação complementar, focando na precisão das previsões numéricas que podem servir de base para a estimativa da probabilidade de excedência.

#### Análise Bayesiana para Comparação de Modelos

Para além de calcular métricas de desempenho, é fundamental avaliar se as diferenças observadas entre modelos são estatisticamente significativas ou se podem ser atribuídas ao acaso, especialmente quando se comparam abordagens distintas (e.g., classificação vs. regressão + CDF) ou diferentes algoritmos dentro da mesma abordagem. A **análise Bayesiana** oferece uma estrutura robusta para esta avaliação [^3].

Especificamente, o **Bayesian correlated t-test** é empregado para *assess the significance of the results by comparing pairs of predictive models across forecasting horizons* [^3]. Este teste é particularmente adequado para dados emparelhados, como os resultados de diferentes modelos avaliados nos mesmos folds de validação cruzada e para múltiplos horizontes de previsão, como é comum em time series forecasting [^10].

Um conceito chave na interpretação da análise Bayesiana é a **region of practical equivalence (ROPE)**. A ROPE é definida como um intervalo em torno da diferença zero (e.g., [-1%, 1%] para a diferença percentual em AUC) que representa uma zona onde as diferenças de desempenho são consideradas praticamente insignificantes [^4].

> A definição de uma **region of practical equivalence (ROPE)** permite determinar se as diferenças de desempenho entre os modelos são *meaningful* [^4]. Se a distribuição posterior da diferença de desempenho entre dois modelos cair maioritariamente dentro da ROPE, os modelos são considerados praticamente equivalentes. Se cair maioritariamente fora da ROPE (a favor de um dos modelos), conclui-se que um modelo é significativamente melhor que o outro em termos práticos.

Esta abordagem [^3, ^4] fornece uma avaliação mais matizada do que os testes de hipóteses nulas tradicionais, quantificando a probabilidade de um modelo ser melhor, pior ou praticamente equivalente a outro. No contexto apresentado, o Bayesian correlated t-test foi usado para comparar o modelo NN+CDF com todos os restantes, utilizando uma ROPE de [-1%, 1%] sobre a métrica AUC [^11].

### Conclusão

A avaliação abrangente de modelos de previsão de probabilidade de excedência exige um conjunto multifacetado de ferramentas. Métricas como **AUC** e **log loss** são cruciais para avaliar a qualidade das previsões probabilísticas em si [^1]. Quando as probabilidades são derivadas de previsões numéricas, métricas de regressão como **MAE**, **MAPE**, **$R^2$** e **RMSE** tornam-se relevantes para avaliar a precisão dessas previsões subjacentes [^2]. Finalmente, para determinar se as diferenças de desempenho observadas são substanciais, a **análise Bayesiana**, especificamente o **Bayesian correlated t-test** juntamente com a definição de uma **ROPE**, oferece um método poderoso para comparar pares de modelos e inferir a significância prática dos resultados [^3, ^4]. A aplicação conjunta destas métricas e técnicas de análise estatística permite uma seleção de modelos informada e robusta no domínio desafiador da previsão de probabilidade de excedência.

### Referências

[^1]: We evaluate the exceedance probability performance of each approach using the area under the ROC curve (AUC) and log loss as evaluation metrics. (Page 12)
[^2]: Besides, we also use typical regression metrics to measure forecasting performance, i.e., the performance for predicting the numeric value of future observations. Specifically, we use the mean absolute error (MAE), mean absolute percentage error (MAPE), coefficient of determination ($R^2$), and root mean squared error (RMSE). (Page 12)
[^3]: We carried out a Bayesian analysis to assess the significance of the results using the Bayesian correlated t-test [3]. This test is used to compare pairs of predictive models across the forecasting horizon. (Page 16)
[^4]: We define the region of practical equivalence (ROPE) for the Bayes correlated t-test to be the interval [-1%, 1%]. This means that the performance of the two methods under comparison is considered equivalent if their percentage difference is within this interval. ... ROPE ... determine if performance differences are meaningful. (Page 16)
[^5]: Abstract: Significant wave height forecasting is a key problem in ocean data analytics... we focus on the prediction of extreme values of significant wave height... framed as an exceedance probability forecasting problem. (Page 1)
[^6]: Exceedance probability forecasting denotes the process of estimating the probability that a time series will exceed a predefined threshold in a predefined future period. (Page 2)
[^7]: Exceedance forecasting is typically formalized as a binary classification problem... Ensemble-based forecasting (regression) approaches can also be applied, in which the exceedance probability is estimated according to the ratio of individual predictions that exceed the threshold. ... Logistic regression is commonly used to this effect [42]. (Page 2, 6)
[^8]: Instead, we propose a novel approach based on point forecasting. ... we use the cumulative distribution function (CDF) of this distribution to estimate the exceedance probability. (Page 1, 3)
[^9]: The proposed method leverages the numeric predictions produced by the forecasting model regarding the value of future observations. Here, we denote the point forecast for the i-th observation as $\hat{y}_i$. (Page 7)
[^10]: We carry out a Monte Carlo cross-validation procedure to evaluate the performance of models [33]. ... Monte Carlo cross-validation is applied with 10 folds. (Page 12) ... Specifically, we experiment with a horizon from 1 to 24 hours in advance. (Page 11)
[^11]: In this case, we compare NN+CDF with all remaining methods. We define the region of practical equivalence (ROPE) for the Bayes correlated t-test to be the interval [-1%, 1%]. ... The results are presented in Figure 10, which shows the probability of NN+CDF winning in blue, drawing in green (results within the ROPE), or losing in red against each remaining method. (Page 16)

<!-- END -->