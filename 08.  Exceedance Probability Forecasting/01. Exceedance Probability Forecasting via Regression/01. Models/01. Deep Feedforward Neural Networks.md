## Deep Feedforward Neural Networks for Time Series Forecasting

### Introdução
Este capítulo explora o uso de redes neurais *feedforward* profundas (Deep Feedforward Neural Networks - DFFNNs) para modelagem e previsão de séries temporais, especificamente no contexto do problema de previsão de altura significativa de ondas (Significant Wave Height - SWH) [^1]. Como mencionado anteriormente, a previsão de SWH é crucial para diversas operações marítimas e prevenção de desastres costeiros [^1]. As redes *feedforward* profundas são uma ferramenta poderosa na modelagem não linear e, portanto, adequadas para capturar a complexidade inerente às séries temporais [^4]. Este capítulo se baseia nos conceitos de modelagem de séries temporais e probabilidade de excedência introduzidos anteriormente, focando na aplicação e configuração detalhada de DFFNNs [^1].

### Conceitos Fundamentais
As DFFNNs são uma classe de redes neurais artificiais nas quais as conexões entre os nós não formam um ciclo. Em outras palavras, o fluxo de informação é unidirecional, da camada de entrada para a camada de saída, passando por uma ou mais camadas ocultas [^4].

**Arquitetura da Rede:** [^1]
No contexto fornecido, as DFFNNs são utilizadas com arquiteturas específicas, como:
*   **Número de Camadas Ocultas:** 2
*   **Número de Unidades por Camada:** 32 unidades na primeira camada oculta e 16 unidades na segunda camada oculta.

A escolha do número de camadas e unidades é um hiperparâmetro crucial que afeta a capacidade da rede de aprender padrões complexos nos dados. Arquiteturas menores podem não conseguir capturar a complexidade dos dados, enquanto arquiteturas maiores podem levar ao *overfitting* [^4].

**Regularização Dropout:** [^1]
Para mitigar o problema de *overfitting*, a regularização *dropout* é aplicada. O *dropout* desativa aleatoriamente uma proporção de neurônios durante o treinamento, forçando a rede a aprender representações mais robustas e generalizáveis [^1]. No caso em questão, uma taxa de *dropout* de 0.2 é utilizada após cada camada oculta. Isso significa que 20% dos neurônios em cada camada são desativados aleatoriamente durante cada iteração de treinamento [^1].

**Funções de Ativação ReLU:** [^1]
A função de ativação *rectified linear unit* (ReLU) é empregada nas camadas ocultas. A ReLU é definida como:

$$
ReLU(x) = max(0, x)
$$

A ReLU é uma função de ativação popular devido à sua simplicidade e eficiência computacional. Ela ajuda a evitar o problema do desaparecimento do gradiente, comum em redes profundas treinadas com funções de ativação sigmoide ou tangente hiperbólica [^1].

**Otimizador Adam:** [^1]
O otimizador Adam (Adaptive Moment Estimation) é utilizado para ajustar os pesos da rede durante o treinamento. Adam é um algoritmo de otimização adaptativo que combina as vantagens do *momentum* e do RMSProp, tornando-o eficiente para treinar redes neurais profundas [^1].

**Função de Perda MSE:** [^1]
A função de perda *mean squared error* (MSE) é utilizada para quantificar a diferença entre as previsões da rede e os valores reais. A MSE é definida como:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

onde $y_i$ é o valor real, $\hat{y}_i$ é a previsão da rede, e $n$ é o número de amostras [^1].

**Treinamento e Validação:** [^1]
A rede é treinada utilizando dados de treinamento e validada utilizando dados de validação. O processo de treinamento é realizado por 30 *epochs*. Um *epoch* representa uma iteração completa sobre todo o conjunto de treinamento. A validação é usada para monitorar o desempenho da rede em dados não vistos durante o treinamento, permitindo ajustar os hiperparâmetros e evitar o *overfitting* [^1].

**Auto-regressão:** [^1]
No contexto do estudo [^1], a modelagem de séries temporais é feita usando uma estratégia auto-regressiva, como formalizado na Seção 3.1 [^1]. Isso significa que os valores futuros da série temporal são previstos com base em seus valores passados. Em outras palavras, a rede neural aprende a relação entre os valores passados da série temporal e seus valores futuros [^1].

**Embedding Time Delay:** [^1]
Para implementar a estratégia auto-regressiva, é utilizado o *time delay embedding* (Teorema de Takens [^4]). O *embedding* transforma a série temporal em um conjunto de observações da forma (X, y), onde y é o valor a ser previsto e X é um vetor contendo os valores passados da série temporal (lags) [^1].

### Conclusão
As redes neurais *feedforward* profundas, com a arquitetura, regularização e otimização apropriadas, são uma ferramenta eficaz para a previsão de séries temporais, como demonstrado no contexto da previsão de SWH [^1]. A combinação de DFFNNs com técnicas de regularização, como o *dropout*, e otimizadores adaptativos, como o Adam, permite construir modelos robustos e generalizáveis [^1]. Além disso, a utilização de uma estratégia auto-regressiva com *time delay embedding* permite capturar a dependência temporal inerente às séries temporais [^1]. Os resultados do estudo [^1] mostram que as DFFNNs, juntamente com a metodologia baseada na CDF, fornecem estimativas de probabilidade de excedência melhores do que as alternativas de última geração [^1].

### Referências
[^1]: Texto fornecido.
[^4]: Cerqueira, V., Torgo, L., Mozetič, I.: Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning 109(11), 1997-2028 (2020)
<!-- END -->