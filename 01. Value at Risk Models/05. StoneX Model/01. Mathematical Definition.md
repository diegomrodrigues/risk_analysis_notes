## 1. Visão geral do problema

Queremos calcular, para cada *carteira* (identificada por `GroupAccountNumber`) em uma *data de referência*  $AsOfDate$, uma medida de *Value at Risk* (VaR) baseada em *simulação histórica*. Em linhas gerais, o modelo:

1.  **Identifica** as posições (exposições) de cada carteira em diversos *instrumentos* (por exemplo, contratos de boi, milho, dólar etc.), associadas a uma *quantidade de dias até expirar* (o *tenor*).
2.  **Projeta** variações históricas nos preços (*returns*) para cada instrumento e tenor, ao longo de um determinado *período de lookback* (por exemplo, 252 dias).
3.  **Calcula** o *lucro e prejuízo (PnL)* diário hipotético de cada posição, aplicando os *retornos históricos* às exposições $\Delta$ (delta) e $\Gamma$ (gama).
4.  **Forma** uma distribuição empírica de PnLs históricos.
5.  **Extrai** o VaR (em geral, o quantil da perda, como por exemplo o quantil de 1% ou 5%) dessa distribuição.

O resultado final é um valor que indica quanto, segundo o histórico analisado, a carteira pode *perder* (ou ganhar, dependendo da convenção) em condições adversas com certa probabilidade (por exemplo, 1%).

------

## 2. Notação e conjuntos básicos

-   Seja $\Omega$ o conjunto de *carteiras* (por exemplo, distintos `GroupAccountNumber`).
-   Seja $\mathcal{D} = \{t_1, t_2, \dots, t_n\}$ o conjunto de *datas de referência* ou $AsOfDate$.
-   Seja $\mathcal{I}$ o conjunto de *instrumentos* (por exemplo, "CORN BMF", "USD/BRL", "CATTLE LIVE BMF" etc.).
-   Seja $\mathcal{T}$ o conjunto de possíveis *tenores* (ou número de dias restantes até expirar) que surgem nos dados.
-   Define-se ainda um *horizonte de lookback* $L$ (em dias) e um *horizonte de holding forward* $H$ (número de dias para frente que se supõe reter a posição).

Em Python/Spark, isso se manifesta como DataFrames distintos:

1.  `exposures`: Contém as exposições $\Delta$ e $\Gamma$, segmentadas por carteira, data de referência, instrumento e tenor.
2.  `returns`: Contém retornos (variações) históricas para cada instrumento e tenor ao longo das datas passadas.

------

## 3. Exposições (Delta e Gama)

Para cada carteira $\omega \in \Omega$ e data $AsOfDate = t_k \in \mathcal{D}$, temos uma tabela de exposições:

$\Delta_{k,\omega,i,\tau}, \quad \Gamma_{k,\omega,i,\tau}$

onde:

-   $i \in \mathcal{I}$ é o instrumento.
-   $\tau \in \mathcal{T}$ é o tenor reportado (número de dias até a expiração do contrato ou alguma forma de bucket de tempo).
-   $\Delta$ mede a *exposição linear* em relação às variações de preço do instrumento.
-   $\Gamma$ mede a *exposição de segunda ordem* (aproximando o efeito de convexidade da posição).

Além disso, cada instrumento $i$ pode ter um *tamanho de contrato* (denominado no código de `contract_size`), que denotaremos por:

$CS_i \quad (\text{por instrumento } i)$.

**Observação**: na implementação, às vezes a lógica define o `contract_size` como 1 (quando não há ajuste) ou algum fator numérico (por exemplo, se o contrato na bolsa tem um tamanho específico).

------

## 4. Retornos históricos

Para cada instrumento $i$ e para cada *tenor* $\tau$, calculamos retornos históricos ao longo de um *período de lookback*. Denotemos o *conjunto de datas históricas* relevantes por $\{d_1, d_2, \ldots, d_L\}$ (os últimos $L$ dias úteis, por exemplo 252 dias).

No DataFrame `returns`, cada linha contém algo como:

$R_{d,i,\tau} = \text{retorno no dia } d \text{ do instrumento } i \text{ para o tenor } \tau$.

-   Para alguns instrumentos (por exemplo, futuros de commodities negociados em bolsas locais), o retorno pode ser definido como *variação absoluta* de preço:

    $R_{d,i,\tau} = P_{d,i,\tau} - P_{d-\tau,i,\tau}$,

    se estivermos usando a diferença entre o preço do dia $d$ e o preço do dia $d-\tau$.

-   Para outros casos (como "USD/BRL"), pode-se utilizar *variação percentual*:

    $R_{d,\text{USD/BRL},\tau}  = \frac{P_{d,\text{USD/BRL},\tau} - P_{d-\tau,\text{USD/BRL},\tau}}{P_{d,\text{USD/BRL},\tau}}$,

    dependendo do modo de cálculo interno (o código faz distinção se é "usd_adjust = True", etc.).

Na prática, a implementação faz esse cálculo em `MitraReturns`, armazenando uma coluna `RETURN` e uma coluna `key` = *instrumento\_tenor*, para permitir o *join* posterior com as exposições.

------

## 5. Holding period e ajuste de tenores

O código também considera um *lookforward period* $H$. Ele gera, para cada exposição, valores de “*holding period*” de 1 até $H$.  Para cada $\tau$ original, pode haver um “tenor ajustado” $\tau'$ (denominado `AdjustedTenor` no código), em que:

-   Se $\tau > \text{HoldingPeriod}$, então $\tau' = \text{HoldingPeriod}$.
-   Caso contrário, faz-se um cálculo de resto ou modulo, pois, ao expirar um contrato, assume-se que se inicia uma nova posição semelhante.

Formalmente, se chamarmos a coluna `HoldingPeriod` de $h$ ($1 \leq h \leq H$), temos:

$\tau' =  \begin{cases} h, & \text{se } \tau > h,\\ \tau, & \text{se } \tau \le h \text{ e } (h \bmod \tau) = 0,\\ h \bmod \tau, & \text{caso contrário}. \end{cases}$

Isso gera uma nova “chave” de tenor ajustado $\tau'$ por posição. O objetivo é modelar que, se o contrato vence durante o período de retenção, ele é “rolado” (renovado) como uma nova posição do mesmo instrumento.

------

## 6. Cálculo de PnL projetado

Depois de criadas todas as combinações $(\omega, t_k, i, \tau')$ com cada $h$ no intervalo $\{1,\dots,H\}$, e de ter à disposição os retornos históricos $R_{d,i,\tau'}$, o modelo calcula o *lucro e prejuízo* (PnL) projetado. Geralmente, a formulação de PnL usando aproximação *delta-gama* é:

$\text{pnl}_{d,k,\omega,i,\tau'} \;=\;  \Delta_{k,\omega,i,\tau'} \cdot R_{d,i,\tau'} \cdot CS_i \;+\; \frac{\Gamma_{k,\omega,i,\tau'}}{2} \cdot \bigl(R_{d,i,\tau'}\bigr)^2 \cdot CS_i$,

onde:

-   $\Delta_{k,\omega,i,\tau'}$ e $\Gamma_{k,\omega,i,\tau'}$ são as exposições delta e gama da carteira $\omega$, na data de referência $t_k$, para o instrumento $i$ e tenor ajustado $\tau'$.
-   $R_{d,i,\tau'}$ é o retorno observado historicamente no dia $d$.
-   $CS_i$ é o *tamanho do contrato* do instrumento $i$.

O código funde (“join”) os DataFrames de exposições e retornos pela coluna `key` = `Instrument_Tenor`, obtendo uma série histórica de PnLs por dia $d$ no *lookback*.

Então, **agrega-se** o PnL por carteira $\omega$, data de referência $t_k$ e holding period $h$ (se necessário), somando sobre todos os instrumentos:

$PnL_{d,k,\omega}(h)  \;=\; \sum_{i,\tau'}  \Bigl[ \Delta_{k,\omega,i,\tau'} \, R_{d,i,\tau'} \, CS_i  \;+\; \tfrac{1}{2}\,\Gamma_{k,\omega,i,\tau'}\,\bigl(R_{d,i,\tau'}\bigr)^2\,CS_i \Bigr]$.

Em termos de banco de dados, isso aparece como um `groupBy("GroupAccountNumber", "AsOfDate", "HoldingPeriod")` e depois a soma dos PnLs.

------

## 7. Distribuição empírica e cálculo do VaR

Para cada $\omega$, data de referência $t_k$ e holding period $h$, gera-se então uma **distribuição histórica** de PnLs:

$\bigl\{\, PnL_{d,k,\omega}(h) \;:\; d=1,\dots,L \bigr\}$.

Em seguida, para cada nível de confiança $\alpha$ (por exemplo, $\alpha=0{,}01$ para 99% de confiança), o **VaR** no sentido de *perda esperada no pior $\alpha$%* ou *quantil* é computado via estatística empírica:

$VaR_{\alpha}(k,\omega,h) \;=\; \text{percentile}_{\alpha}  \Bigl(\, P_n \;=\; PnL_{d,k,\omega}(h) \Bigr)$,

onde $\text{percentile}_{\alpha}$ denota o quantil de ordem $\alpha$. Dependendo da convenção de sinal, podemos ter:

-   Se interpretamos $PnL > 0$ como *ganho*, então um quantil baixo (por exemplo, $\alpha=0{,}01$) retorna um valor negativo (pior perda). Nesse caso, às vezes o VaR é relatado como $-\text{quantile}_{\alpha}(\cdot)$ para expressar o valor positivo de perda.
-   Se interpretamos $PnL < 0$ diretamente como *perda*, o quantil $\alpha$ em si já dá o valor do VaR (geralmente negativo).

No código PySpark, esse passo aparece como:

```python
var_spark = (
    pnl_df
    .groupBy("GroupAccountNumber", "AsOfDate", "HoldingPeriod")
    .agg(F.expr(f"percentile(pnl, {alpha})").alias("VaR"))
    .withColumn("Quantile", F.lit(alpha))
)
```

Portanto, a saída final é um DataFrame com colunas:

$(\text{GroupAccountNumber}, AsOfDate, \text{HoldingPeriod}, \text{Quantile}, \text{VaR})$.

------

## 8. Pontos essenciais e considerações adicionais

1.  **Estrutura de datas**:
    -   O modelo distingue *datas de referência* ($AsOfDate$) das *datas históricas* para cálculo de retorno ($d \in \{1,\dots,L\}$).
    -   Há ainda o conceito de *HoldingPeriod* $h$, que vai de 1 a $H$, indicando o número de dias que a posição será mantida.
2.  **Rolagem de contrato** (Ajuste de *tenor*):
    -   Quando um contrato *expira* antes do $h$-ésimo dia, o código assume que se inicia nova posição igual, fazendo o “casamento” no `AdjustedTenor`.
    -   Formalmente, $\tau'$ funciona como um *tenor efetivo* naquele período.
3.  **Exposição agregada**:
    -   Se houver a *supergroup* (por exemplo, "Brazil Supergroup"), o modelo soma as exposições de todos os clientes, gerando um PnL agregado.
4.  **Retornos específicos**:
    -   Alguns instrumentos têm retornos *percentuais* (por exemplo, *FX*), outros *absolutos* (por exemplo, futuros de commodities), controlados internamente pela lógica de `MitraReturns`.
5.  **Percentil empírico**:
    -   O VaR em cada $\alpha$ é calculado via `percentile(pnl, alpha)`, *sem* supor distribuição paramétrica (Gaussiana, etc.).
    -   É, portanto, uma abordagem de *simulação histórica pura*.

------

## 9. Conclusão

De forma resumida, o **fluxo algébrico** do modelo é:

1.  **Exposições**: $(\Delta_{k,\omega,i,\tau}, \Gamma_{k,\omega,i,\tau})$.
2.  **Retornos históricos**: $R_{d,i,\tau}$.
3.  **Criação de holding periods** $\{1,\dots,H\}$ e ajuste de tenor $\tau \mapsto \tau'$.
4.  **Cálculo de PnL**: $PnL_{d,k,\omega}(h) \;=\; \sum_{i,\tau'}  \left[  \Delta_{k,\omega,i,\tau'} \; R_{d,i,\tau'} \; CS_i  \;+\;  \tfrac{1}{2}\,\Gamma_{k,\omega,i,\tau'}\,(R_{d,i,\tau'})^2 \; CS_i \right]$.
5.  **Estimativa empírica de VaR** para cada $\alpha$, tomando o quantil $\alpha$ da amostra $\{PnL_{1,k,\omega}(h), \ldots, PnL_{L,k,\omega}(h)\}$.

Essa descrição matemática possibilita pensar sobre cada passo de forma sistemática, permitindo análises adicionais como:

-   Comparar hipóteses sobre *rolagem* de contratos.
-   Investigar métodos de interpolação ou *smoothing* de $\Delta$ e $\Gamma$.
-   Avaliar mudanças na forma de calcular o *return* (absoluto vs. percentual).
-   Expandir a análise para *component VaR* (identificar a contribuição de cada instrumento ao risco total).

Em suma, a implementação em PySpark reflete diretamente esse encadeamento de relações algébricas, onde cada `join` e `groupBy` corresponde a uma parte específica da construção teórica.