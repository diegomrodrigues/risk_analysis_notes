## Verifica√ß√£o do Modelo VAR: Erros Tipo I e Tipo II e Pot√™ncia do Teste

### Introdu√ß√£o
Este cap√≠tulo aborda a crucial considera√ß√£o dos erros tipo I e tipo II no contexto da verifica√ß√£o de modelos VAR, bem como a influ√™ncia do n√∫mero de observa√ß√µes ($T$) na pot√™ncia do teste. Como discutido nos cap√≠tulos anteriores, a valida√ß√£o de modelos VAR envolve testes de hip√≥teses para avaliar se a taxa de falha observada √© consistente com a probabilidade de exce√ß√£o especificada pelo modelo [^1]. Este cap√≠tulo explora detalhadamente a natureza dos erros tipo I e tipo II, o *trade-off* inerente na escolha do n√≠vel de signific√¢ncia e o impacto do tamanho da amostra ($T$) na capacidade de um teste estat√≠stico para detetar um modelo VAR mal calibrado, assim como o risco de rejeitar um modelo VAR bem calibrado. O foco ser√° na intera√ß√£o entre estes conceitos, buscando o melhor entendimento sobre a avalia√ß√£o de modelos VAR.

### Erros Tipo I e Tipo II em Backtesting de Modelos VAR

Na verifica√ß√£o de modelos VAR, como em qualquer teste de hip√≥tese, enfrentamos a possibilidade de tomar decis√µes incorretas. Existem dois tipos de erros que podem ocorrer:

*   **Erro Tipo I (Falso Positivo):** Rejeitar a hip√≥tese nula quando ela √© verdadeira. No contexto de backtesting de modelos VAR, isso significa rejeitar a hip√≥tese de que o modelo est√° corretamente calibrado, quando na verdade ele est√°.
*   **Erro Tipo II (Falso Negativo):** N√£o rejeitar a hip√≥tese nula quando ela √© falsa. Isso significa aceitar a hip√≥tese de que o modelo VAR est√° corretamente calibrado, quando na verdade ele n√£o est√°, ou seja, subestima ou sobrestima os riscos.

**Lema 15** A probabilidade de cometer um erro tipo I √© igual ao n√≠vel de signific√¢ncia $\alpha$ do teste.
*Prova:*
I. O n√≠vel de signific√¢ncia $\alpha$ √© definido como a probabilidade m√°xima de rejeitar a hip√≥tese nula quando ela √© verdadeira.
II. No contexto de um teste de hip√≥tese, a probabilidade de cometer um erro tipo I √© precisamente a probabilidade de rejeitar a hip√≥tese nula, quando ela √© verdadeira.
III. A probabilidade de cometer um erro tipo I corresponde √† probabilidade de observar um valor da estat√≠stica de teste que se encontra na regi√£o de rejei√ß√£o sob a hip√≥tese nula, e, por defini√ß√£o, √© igual ao n√≠vel de signific√¢ncia $\alpha$.
IV. Portanto, a probabilidade de cometer um erro tipo I √© igual ao n√≠vel de signific√¢ncia $\alpha$ do teste. ‚ñ†

> üí° **Exemplo Num√©rico:** Se escolhermos um n√≠vel de signific√¢ncia $\alpha = 0.05$ (5%), ent√£o h√° uma probabilidade de 5% de rejeitar um modelo VAR que est√° realmente bem calibrado (cometer um erro tipo I). Se escolhermos um n√≠vel de signific√¢ncia $\alpha=0.01$ (1%), o risco de cometer um erro tipo I diminui para 1%, mas o risco de cometer um erro tipo II aumenta.
```mermaid
graph LR
    A[N√≠vel de Signific√¢ncia (Œ±)] --> B{Probabilidade de Erro Tipo I};
    B --> C[Rejeitar Modelo Correto];
```

**Lema 16** A probabilidade de cometer um erro tipo II √© denotada por $\beta$, e o poder do teste √© igual a $1 - \beta$.
*Prova:*
I. O erro tipo II ocorre quando n√£o rejeitamos a hip√≥tese nula quando ela √© falsa.
II. A probabilidade de cometer um erro tipo II √© denotada por $\beta$.
III. O poder de um teste de hip√≥tese √© definido como a probabilidade de rejeitar corretamente a hip√≥tese nula quando ela √© falsa.
IV.  Logo, o poder do teste √© igual a $1 - \beta$.
V.  Um teste com maior poder significa que ele tem uma maior probabilidade de detectar corretamente um modelo VAR mal calibrado.
VI.  Portanto, a probabilidade de cometer um erro tipo II √© dada por $\beta$, e o poder do teste √© dado por $1-\beta$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se a probabilidade de cometer um erro tipo II (n√£o rejeitar um modelo mal calibrado) √© $\beta = 0.2$ (20%), ent√£o o poder do teste, que √© a probabilidade de detetar um modelo mal calibrado, √© $1 - 0.2 = 0.8$ (80%). Em geral, pretende-se um teste com maior poder, e que minimize a probabilidade de cometer um erro tipo II.
```mermaid
graph LR
    A{Probabilidade de Erro Tipo II (Œ≤)} --> B{1-Œ≤};
    B --> C[Poder do Teste];
    A --> D[Aceitar Modelo Incorreto];
```
**Lema 16.1**  A probabilidade $\beta$ de cometer um erro tipo II √© influenciada por v√°rios fatores, incluindo o tamanho da amostra ($T$), o n√≠vel de signific√¢ncia ($\alpha$), e a magnitude do desvio da hip√≥tese nula.
*Prova:*
I.  Conforme estabelecido, o tamanho da amostra $T$ afeta a variabilidade da estat√≠stica de teste, influenciando o poder do teste.
II. O n√≠vel de signific√¢ncia $\alpha$ determina a regi√£o de rejei√ß√£o da hip√≥tese nula, e um valor menor de $\alpha$ aumenta $\beta$.
III.  A magnitude do desvio da hip√≥tese nula, ou seja, qu√£o mal calibrado est√° o modelo, tamb√©m afeta $\beta$. Desvios maiores tornam mais f√°cil a dete√ß√£o de uma m√° calibra√ß√£o, reduzindo $\beta$.
IV. Portanto, $\beta$ √© influenciada pelo tamanho da amostra, o n√≠vel de signific√¢ncia e a magnitude do desvio da hip√≥tese nula. ‚ñ†

**Proposi√ß√£o 21** O n√≠vel de signific√¢ncia $\alpha$ e o poder do teste ($1-\beta$) est√£o inversamente relacionados. Reduzir a probabilidade de cometer um erro tipo I aumenta a probabilidade de cometer um erro tipo II, e vice-versa.

*Prova:*
I. O n√≠vel de signific√¢ncia $\alpha$ define a regi√£o de rejei√ß√£o do teste de hip√≥tese.
II.  Se reduzirmos $\alpha$, o valor de corte que delimita a regi√£o de rejei√ß√£o se torna mais extremo, tornando o teste mais dif√≠cil de rejeitar a hip√≥tese nula. Isto diminui a probabilidade de cometer um erro tipo I (rejeitar um modelo verdadeiro).
III.  Contudo, a redu√ß√£o da regi√£o de rejei√ß√£o tamb√©m aumenta a probabilidade de n√£o detetar um modelo mal calibrado (erro tipo II).
IV. Inversamente, aumentar $\alpha$ torna o teste mais f√°cil de rejeitar a hip√≥tese nula, aumentando a probabilidade de rejeitar um modelo verdadeiro (erro tipo I) e reduzindo a probabilidade de n√£o detectar um modelo mal calibrado (erro tipo II).
V. Portanto, o n√≠vel de signific√¢ncia $\alpha$ e o poder do teste ($1-\beta$) est√£o inversamente relacionados, criando um *trade-off* na sele√ß√£o do n√≠vel de signific√¢ncia. ‚ñ†

> üí° **Exemplo Num√©rico:** Se utilizarmos um n√≠vel de signific√¢ncia $\alpha = 0.01$ (1%), ent√£o o risco de rejeitar um modelo VAR que est√° bem calibrado √© baixo (1%). Contudo, a probabilidade de n√£o detetar um modelo que est√° mal calibrado √© maior. Se aumentarmos o n√≠vel de signific√¢ncia para $\alpha = 0.10$ (10%), aumenta a probabilidade de rejeitar um modelo que est√° bem calibrado, mas diminui a probabilidade de n√£o detetar um modelo que est√° mal calibrado. Este balan√ßo entre erros do tipo I e do tipo II depende das consequ√™ncias de cada tipo de erro, e deve ser considerado na escolha do valor de $\alpha$.

A escolha do n√≠vel de signific√¢ncia $\alpha$ e, consequentemente, o valor de corte para o z-score, depende do contexto do problema e do custo relativo dos erros tipo I e tipo II.

> üí° **Exemplo Num√©rico:** Se a consequ√™ncia de rejeitar um modelo bem calibrado (erro tipo I) for muito pequena (por exemplo, o custo de recalibrar o modelo √© baixo), podemos usar um n√≠vel de signific√¢ncia maior (por exemplo, 10%), para aumentar o poder do teste e diminuir o risco de aceitar um modelo mal calibrado (erro tipo II). No entanto, se as consequ√™ncias de n√£o detetar um modelo mal calibrado forem elevadas (por exemplo, grandes perdas financeiras), o n√≠vel de signific√¢ncia deve ser mais baixo (por exemplo, 1%), para reduzir a probabilidade de aceitar um modelo que possa subestimar o risco.

**Proposi√ß√£o 21.1** A escolha do n√≠vel de signific√¢ncia $\alpha$ pode ser baseada em um *framework* de tomada de decis√£o Bayesiana, onde as probabilidades *a priori* de o modelo ser bem ou mal calibrado e os custos associados aos erros tipo I e II s√£o considerados.
*Prova:*
I. Em uma abordagem Bayesiana, atribu√≠mos probabilidades *a priori* √† hip√≥tese nula (o modelo est√° bem calibrado) e √† hip√≥tese alternativa (o modelo est√° mal calibrado).
II. Tamb√©m definimos as fun√ß√µes de custo para os erros tipo I ($C_{I}$) e tipo II ($C_{II}$).
III. O objetivo √© minimizar o risco esperado, que √© dado por: $R = \alpha C_{I} P(H_0) + \beta C_{II} P(H_1)$, onde $P(H_0)$ √© a probabilidade *a priori* de a hip√≥tese nula ser verdadeira e $P(H_1)$ √© a probabilidade *a priori* de a hip√≥tese alternativa ser verdadeira ($P(H_1)=1-P(H_0)$).
IV.  O n√≠vel de signific√¢ncia $\alpha$ pode ser escolhido de forma a minimizar este risco esperado, considerando as probabilidades *a priori* e os custos associados a cada tipo de erro.
V.  Portanto, um *framework* Bayesiano permite uma escolha mais informada do n√≠vel de signific√¢ncia, com base nas probabilidades e nos custos dos erros. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo VAR e, com base em dados hist√≥ricos e informa√ß√µes de mercado, acreditamos que existe uma probabilidade de 80% do modelo estar bem calibrado ($P(H_0) = 0.8$) e uma probabilidade de 20% de estar mal calibrado ($P(H_1) = 0.2$). Adicionalmente, o custo de um erro tipo I (rejeitar um modelo bom) √© de 100 unidades ($C_I = 100$) (custo de recalibrar o modelo) e o custo de um erro tipo II (aceitar um modelo mau) √© de 1000 unidades ($C_{II} = 1000$) (custo de perdas devido ao risco subestimado). Vamos comparar dois n√≠veis de signific√¢ncia, $\alpha = 0.05$ e $\alpha = 0.01$. Para $\alpha = 0.05$,  temos uma probabilidade de 5% de erro tipo I e, supondo um poder do teste de 90% (ou seja, $\beta = 0.1$), o risco esperado √©: $R = 0.05 \times 100 \times 0.8 + 0.1 \times 1000 \times 0.2 = 4 + 20 = 24$ unidades. Para $\alpha = 0.01$, temos 1% de chance de erro tipo I. Supondo que o poder do teste seja 70% (ou seja, $\beta = 0.3$), o risco esperado √© $R = 0.01 \times 100 \times 0.8 + 0.3 \times 1000 \times 0.2 = 0.8 + 60 = 60.8$ unidades. Nesta an√°lise simples, um n√≠vel de signific√¢ncia $\alpha = 0.05$ parece mais apropriado. Contudo, estes c√°lculos s√£o simplifica√ß√µes, e na pr√°tica a escolha do valor de alfa requer uma an√°lise mais profunda, com modelos e dados mais adequados.

### Impacto do N√∫mero de Observa√ß√µes (T) na Pot√™ncia do Teste

O n√∫mero de observa√ß√µes $T$ desempenha um papel crucial na capacidade de um teste estat√≠stico para detetar desvios do comportamento esperado. Em backtesting de modelos VAR, aumentar o per√≠odo de tempo de teste aumenta a quantidade de dados dispon√≠veis para a avalia√ß√£o do modelo. Isso afeta diretamente a pot√™ncia do teste.

**Lema 17** A pot√™ncia de um teste de hip√≥tese, definida como a probabilidade de rejeitar corretamente a hip√≥tese nula quando ela √© falsa, aumenta com o n√∫mero de observa√ß√µes $T$.
*Prova:*
I. A pot√™ncia do teste √© igual a $1 - \beta$, onde $\beta$ √© a probabilidade de cometer um erro tipo II.
II.  √Ä medida que o n√∫mero de observa√ß√µes $T$ aumenta, a variabilidade da estimativa do par√¢metro diminui.
III. Com menor variabilidade, a distribui√ß√£o amostral do estimador torna-se mais concentrada em torno do seu valor verdadeiro, o que aumenta a probabilidade de detetar uma diferen√ßa entre o valor observado e o valor esperado, dado que esta diferen√ßa exista.
IV.  Portanto, um teste com maior n√∫mero de observa√ß√µes possui maior poder estat√≠stico.
V.  Consequentemente, aumentar o n√∫mero de observa√ß√µes $T$ aumenta a pot√™ncia do teste, tornando-o mais eficiente para detetar desvios da calibra√ß√£o correta do modelo VAR. ‚ñ†

> üí° **Exemplo Num√©rico:** Um backtesting de um modelo VAR com 250 dias pode n√£o ser suficiente para detectar um modelo mal calibrado com certeza, devido √† falta de dados. Um backtesting mais longo, com 1000 dias, teria maior poder para rejeitar corretamente a hip√≥tese nula, caso o modelo fosse mal calibrado.
```mermaid
graph LR
    A[Aumentar T (N√∫mero de Observa√ß√µes)] --> B{Reduzir Variabilidade};
    B --> C[Maior Poder do Teste];
```

**Lema 17.1** Para um valor de $p$ fixo, a regi√£o de n√£o rejei√ß√£o da hip√≥tese nula diminui √† medida que $T$ aumenta.
*Prova:*
I.  A regi√£o de n√£o rejei√ß√£o da hip√≥tese nula √© definida pelos valores da estat√≠stica de teste que n√£o levam √† rejei√ß√£o da hip√≥tese nula.
II.  Esta regi√£o √© centrada no valor esperado da estat√≠stica de teste sob a hip√≥tese nula, e sua largura depende da variabilidade da estat√≠stica de teste.
III. Ao aumentar $T$, o n√∫mero de observa√ß√µes, a vari√¢ncia da estat√≠stica de teste, por exemplo o z-score, diminui, e a distribui√ß√£o da estat√≠stica torna-se mais concentrada em torno do valor esperado.
IV.  A diminui√ß√£o na vari√¢ncia leva a uma redu√ß√£o da largura da regi√£o de n√£o rejei√ß√£o, tornando o teste mais preciso na sua capacidade de detetar desvios da hip√≥tese nula.
V. Portanto, para um valor de $p$ fixo, a regi√£o de n√£o rejei√ß√£o da hip√≥tese nula diminui √† medida que $T$ aumenta. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere dois testes de modelos VAR com diferentes per√≠odos de backtesting: $T_1=250$ e $T_2=1000$. O modelo VAR tem um n√≠vel de confian√ßa de 99% ($p=0.01$). Num teste com 250 observa√ß√µes, o n√∫mero esperado de exce√ß√µes √© $np = 250 \times 0.01 = 2.5$. A regi√£o de n√£o rejei√ß√£o da hip√≥tese nula ser√° maior, o que significa que s√£o necess√°rios desvios maiores do valor esperado para que a hip√≥tese nula seja rejeitada. Usando a distribui√ß√£o binomial, a regi√£o de n√£o rejei√ß√£o, com $\alpha=0.05$, √© aproximadamente $0 \leq N \leq 7$, onde $N$ √© o n√∫mero de exce√ß√µes. No caso de um teste com 1000 observa√ß√µes, o n√∫mero esperado de exce√ß√µes √© $np = 1000 \times 0.01 = 10$. A regi√£o de n√£o rejei√ß√£o ser√° menor, e o teste poder√° rejeitar a hip√≥tese nula mesmo com desvios mais pequenos do valor esperado. Neste caso, usando a distribui√ß√£o binomial, a regi√£o de n√£o rejei√ß√£o, com $\alpha=0.05$, √© aproximadamente $4 \leq N \leq 16$. Veja que com o aumento de $T$, o n√∫mero de exce√ß√µes esperado, aumenta de 2.5 para 10, mas o intervalo de n√£o rejei√ß√£o torna-se proporcionalmente menor.

**Proposi√ß√£o 22** Aumentar o n√∫mero de observa√ß√µes $T$ reduz a probabilidade de cometer um erro tipo II (n√£o rejeitar um modelo mal calibrado) e o poder do teste aumenta.
*Prova:*
I. A probabilidade de cometer um erro tipo II, $\beta$, √© a probabilidade de n√£o rejeitar a hip√≥tese nula quando ela √© falsa.
II. O poder do teste √© definido como a probabilidade de rejeitar corretamente a hip√≥tese nula quando ela √© falsa, sendo igual a $1-\beta$.
III. Conforme estabelecido, um aumento do n√∫mero de observa√ß√µes $T$ leva a uma diminui√ß√£o da variabilidade da estat√≠stica de teste.
IV. Como resultado, a distribui√ß√£o da estat√≠stica de teste torna-se mais concentrada em torno do valor esperado, o que aumenta a probabilidade de o valor da estat√≠stica de teste se encontrar na regi√£o de rejei√ß√£o, dado que a hip√≥tese nula √© falsa.
V.  Portanto, aumentar o n√∫mero de observa√ß√µes $T$ reduz a probabilidade de cometer um erro tipo II, aumentando o poder do teste. ‚ñ†

> üí° **Exemplo Num√©rico:** Se um modelo VAR apresentar uma pequena diferen√ßa entre a taxa de falha observada e a esperada, com um tamanho de amostra pequeno, essa diferen√ßa pode n√£o ser estatisticamente significativa, e a hip√≥tese nula ser√° aceita, cometendo-se um erro do tipo II. Se o tamanho da amostra for aumentado, o mesmo desvio entre a taxa de falha observada e a esperada poder√° ser estatisticamente significativo, e a hip√≥tese nula ser√° rejeitada, corretamente, e o poder do teste aumenta. Por exemplo, suponha que um modelo VAR com $p=0.01$ √© backtested com $T=250$ observa√ß√µes, e apresenta 4 exce√ß√µes. O n√∫mero esperado de exce√ß√µes √© $250 \times 0.01 = 2.5$. Usando a distribui√ß√£o binomial, o p-value deste resultado √© $0.19$, n√£o sendo estatisticamente significativo para um n√≠vel de signific√¢ncia de 5%. Contudo, se o mesmo modelo for backtested com $T=1000$ observa√ß√µes e apresentar 15 exce√ß√µes. O n√∫mero esperado de exce√ß√µes √© $1000 \times 0.01 = 10$. O p-value deste resultado √© $0.02$, sendo estatisticamente significativo para um n√≠vel de signific√¢ncia de 5%.

A tabela 6-2 no contexto mostra que as regi√µes de n√£o rejei√ß√£o s√£o dadas em termos do n√∫mero de exce√ß√µes $N$ para diferentes valores de $T$ e de $p$. Os intervalos das regi√µes de n√£o rejei√ß√£o s√£o expressos como um n√∫mero de exce√ß√µes. Por exemplo, para $p=0.01$ e $T=252$, a regi√£o de n√£o rejei√ß√£o √© $N < 7$. Para o mesmo $p$, e $T=1000$, a regi√£o de n√£o rejei√ß√£o √© $4 < N < 17$. Se considerarmos a taxa de falha $N/T$ (e n√£o o n√∫mero de exce√ß√µes), vemos que a regi√£o de n√£o rejei√ß√£o em termos da taxa de falha diminui com o aumento de $T$.
```mermaid
graph LR
    A[Aumentar T] --> B{Reduzir Erro Tipo II};
    B --> C[Aumentar Poder do Teste];
```
**Proposi√ß√£o 23** Embora aumentar o n√∫mero de observa√ß√µes $T$ aumente a pot√™ncia do teste, tamb√©m aumenta o risco de rejeitar um modelo VAR bem calibrado, devido √† maior sensibilidade a pequenos desvios da taxa de falha esperada.
*Prova:*
I. Aumento do tamanho da amostra $T$ aumenta a probabilidade de detectar um desvio, seja ele grande ou pequeno.
II. Um modelo que poderia ter sido aceito para um pequeno $T$, pode ser rejeitado para um grande $T$, mesmo que o modelo seja correto, pois ele nunca ser√° perfeito e pequenas flutua√ß√µes ser√£o sempre observadas.
III. Desta forma, com um grande $T$ aumenta a probabilidade de rejeitar uma hip√≥tese nula, mesmo quando ela √© verdadeira, o que significa rejeitar um modelo que √© razoavelmente bem calibrado.
IV. Isto √© uma consequ√™ncia de que, para um n√≠vel de signific√¢ncia $\alpha$ fixo, a regi√£o de aceita√ß√£o diminui √† medida que o tamanho da amostra aumenta, o que aumenta o risco de cometer um erro tipo I.
V. Portanto, √© importante notar que, embora aumentar o n√∫mero de observa√ß√µes $T$ aumente a pot√™ncia do teste, tamb√©m aumenta a sensibilidade a pequenas varia√ß√µes na taxa de falha, o que pode levar a rejeitar modelos que, de outra forma, seriam considerados bem calibrados. ‚ñ†

> üí° **Exemplo Num√©rico:** Um modelo VAR com um n√≠vel de confian√ßa de 99% (p=0.01), poder√° ter um desempenho considerado bom com 250 dias de backtesting, e uma taxa de exce√ß√£o de 2/250 = 0.8%, e passar o teste de hip√≥tese. Ao aumentar o per√≠odo de backtesting para 1000 dias, a taxa de exce√ß√£o observada poderia ser, por exemplo, 15/1000 = 1.5%, e o mesmo modelo seria rejeitado, devido ao maior n√∫mero de observa√ß√µes e √† consequente diminui√ß√£o da variabilidade da estimativa. Apesar do modelo ter um comportamento aceit√°vel, para um teste com 250 observa√ß√µes, a maior variabilidade na estimativa da taxa de exce√ß√£o (a propor√ß√£o entre o n√∫mero de exce√ß√µes e o n√∫mero total de observa√ß√µes) faz com que o resultado obtido n√£o seja estatisticamente diferente do esperado. Contudo, ao aumentar o n√∫mero de observa√ß√µes para 1000, a variabilidade da estimativa diminui, e o desvio da taxa de exce√ß√£o observada relativamente √† taxa de exce√ß√£o definida pelo modelo torna-se estatisticamente significativo.

**Proposi√ß√£o 24** Em cen√°rios de baixa probabilidade de exce√ß√£o (baixo valor de $p$), torna-se cada vez mais dif√≠cil detectar modelos mal calibrados, j√° que o n√∫mero de exce√ß√µes √© reduzido e os testes perdem poder.
*Prova:*
I. Modelos VAR com altas taxas de confian√ßa, tem taxas de exce√ß√£o $p$ baixas, de acordo com a defini√ß√£o de Value at Risk.
II. O n√∫mero esperado de exce√ß√µes √© dado por $pT$, e quando $p$ √© baixo e/ou $T$ √© pequeno, o n√∫mero esperado de exce√ß√µes tamb√©m √© baixo.
III.  Com poucas exce√ß√µes, a vari√¢ncia do n√∫mero de exce√ß√µes diminui, reduzindo a pot√™ncia do teste e aumentando a probabilidade de aceitar modelos VAR mal calibrados, incorrendo em erros tipo II.
IV.  Portanto, em cen√°rios de baixas probabilidades de exce√ß√£o, a dete√ß√£o de modelos VAR mal calibrados torna-se cada vez mais dif√≠cil.
V.  Para valores de $p$ muito baixos, amostras muito grandes s√£o necess√°rias para se conseguir uma pot√™ncia adequada do teste. ‚ñ†

> üí° **Exemplo Num√©rico:** Para um modelo VAR com $p=0.001$ (n√≠vel de confian√ßa de 99.9%), e um per√≠odo de backtesting de $T=250$, o n√∫mero esperado de exce√ß√µes seria $0.25$, o que significa que em m√©dia se espera menos de uma exce√ß√£o a cada ano. Para detetar uma pequena diferen√ßa entre a probabilidade de exce√ß√£o real e a definida pelo modelo, seria necess√°rio um tamanho de amostra $T$ muito grande. Por exemplo, um teste com 10000 observa√ß√µes teria em m√©dia 10 exce√ß√µes, e seria mais capaz de detetar desvios da hip√≥tese nula.

**Proposi√ß√£o 25** O poder do teste pode ser afetado pela autocorrela√ß√£o das exce√ß√µes.
*Prova:*
I. A autocorrela√ß√£o nas exce√ß√µes implica que uma exce√ß√£o √© mais prov√°vel de ser seguida por outra exce√ß√£o do que o esperado sob a hip√≥tese de independ√™ncia.
II. Isso viola a suposi√ß√£o de que as exce√ß√µes s√£o eventos independentes, uma premissa comum nos testes de backtesting baseados em estat√≠sticas como o z-score.
III. A autocorrela√ß√£o positiva pode levar a um agrupamento de exce√ß√µes, que n√£o √© capturado adequadamente por testes que assumem a independ√™ncia das exce√ß√µes, levando a uma subestima√ß√£o do risco, e reduzindo o poder do teste.
IV. Se a autocorrela√ß√£o n√£o for considerada, podemos aceitar um modelo VAR que apresente um comportamento inadequado, o que significa que existe um aumento na probabilidade de cometer erros tipo II.
V. Portanto, a autocorrela√ß√£o das exce√ß√µes pode afetar negativamente a pot√™ncia do teste, e o n√£o reconhecimento da autocorrela√ß√£o pode reduzir o poder do teste de forma significativa. ‚ñ†

> üí° **Exemplo Num√©rico:** Se um modelo VAR apresentar autocorrela√ß√£o positiva nas exce√ß√µes, √© poss√≠vel que uma exce√ß√£o seja seguida por mais exce√ß√µes, criando "clusters" de perdas. Por exemplo, suponha que um modelo VAR com $p=0.01$ e $T=1000$ apresenta 20 exce√ß√µes. Se estas exce√ß√µes estivessem distribu√≠das de forma independente, um resultado de 20 exce√ß√µes n√£o seria estatisticamente significativo (p-value=0.12), para um n√≠vel de signific√¢ncia de 5%, e o modelo n√£o seria rejeitado. Contudo, se as 20 exce√ß√µes ocorressem em 5 dias consecutivos, este resultado seria extremamente improv√°vel (p-value muito baixo) e indicaria que o modelo est√° subestimar o risco, pois o modelo n√£o est√° a prever a possibilidade de agrupamento de perdas. Um backtest que n√£o considera a autocorrela√ß√£o pode classificar este modelo como adequado, quando este est√° a subestimar o risco. √â necess√°rio usar testes de hip√≥tese mais adequados, como testes que avaliem a independ√™ncia das exce√ß√µes, nomeadamente atrav√©s da an√°lise do seu autocorrelograma.

**Lema 17.2**  O aumento do per√≠odo de backtesting $T$  pode levar a mudan√ßas nas condi√ß√µes de mercado, o que pode afetar a validade do modelo VAR.
*Prova:*
I. Os modelos VAR s√£o geralmente calibrados com base em dados hist√≥ricos.
II.  Ao aumentar o per√≠odo de backtesting, pode-se incluir dados que reflitam diferentes condi√ß√µes de mercado, que podem j√° n√£o ser as condi√ß√µes atuais.
III. Se as condi√ß√µes de mercado mudarem significativamente durante o per√≠odo de backtesting, a suposi√ß√£o de que o modelo VAR, treinado com dados de uma determinada condi√ß√£o, ainda √© v√°lido, pode n√£o ser verdadeira.
IV. Esta mudan√ßa de condi√ß√µes pode invalidar o modelo VAR, e diminuir o poder do teste ao introduzir um fator de vi√©s nos resultados do backtest.
V. Portanto, o aumento do per√≠odo de backtesting $T$ deve ser acompanhado da an√°lise e adapta√ß√£o do modelo VAR √†s mudan√ßas nas condi√ß√µes de mercado. ‚ñ†

> üí° **Exemplo Num√©rico:** Se um modelo VAR foi calibrado durante um per√≠odo de baixa volatilidade, e o backtesting √© feito incluindo um per√≠odo de alta volatilidade, o modelo pode apresentar um n√∫mero excessivo de exce√ß√µes, n√£o devido a um problema de calibra√ß√£o, mas devido a mudan√ßas nas condi√ß√µes de mercado. Este cen√°rio dificulta a avalia√ß√£o correta do modelo, e pode levar a uma m√° avalia√ß√£o do poder do teste. Por exemplo, um modelo VAR calibrado com dados de baixa volatilidade, e um n√≠vel de confian√ßa de 99% (p=0.01), num per√≠odo de backtesting com alta volatilidade poder√° apresentar uma taxa de exce√ß√£o de 2%, quando o esperado era 1%. O backtest poder√° rejeitar o modelo, quando na realidade o modelo apenas n√£o se adapta √†s novas condi√ß√µes de mercado, que deveriam ser consideradas na calibra√ß√£o do modelo.

**Proposi√ß√£o 26** O poder do teste pode ser influenciado pela metodologia de c√°lculo do VaR e pela forma como as perdas s√£o agregadas.
*Prova:*
I. O c√°lculo do VaR pode envolver diferentes metodologias, como simula√ß√£o hist√≥rica, m√©todos param√©tricos ou simula√ß√µes Monte Carlo.
II. Cada metodologia tem diferentes pressupostos e pode gerar estimativas de VaR diferentes para o mesmo conjunto de dados, afetando o n√∫mero de exce√ß√µes observadas.
III. A forma como as perdas s√£o agregadas, por exemplo, diariamente, semanalmente, ou por outros intervalos de tempo, tamb√©m afeta o n√∫mero de exce√ß√µes e, consequentemente, a pot√™ncia do teste.
IV.  Uma escolha inadequada da metodologia ou do per√≠odo de agrega√ß√£o pode levar a erros tipo I ou tipo II, mesmo que o modelo VAR esteja corretamente calibrado.
V. Portanto, o poder do teste pode ser influenciado pela metodologia de c√°lculo do VaR e pela forma como as perdas s√£o agregadas, o que deve ser considerado durante o backtesting. ‚ñ†

> üí° **Exemplo Num√©rico:** Se usarmos uma simula√ß√£o hist√≥rica com uma janela temporal muito curta, e as perdas n√£o inclu√≠rem as piores perdas observadas no passado, ent√£o o backtesting n√£o ter√° poder para detetar modelos que subestimam o risco. Por exemplo, se a janela de dados usada na simula√ß√£o hist√≥rica n√£o incluir per√≠odos de crise, o modelo VAR ser√° incapaz de captar o verdadeiro risco do ativo.

**Proposi√ß√£o 26.1** O *p-value* do teste tamb√©m pode ser afetado pela escolha do tamanho da amostra $T$.
*Prova:*
I. O *p-value* √© a probabilidade de observar um resultado da estat√≠stica de teste t√£o ou mais extremo que o observado, assumindo que a hip√≥tese nula √© verdadeira.
II.  O c√°lculo do *p-value* depende da distribui√ß√£o da estat√≠stica de teste sob a hip√≥tese nula.
III. O tamanho da amostra $T$ afeta a distribui√ß√£o da estat√≠stica de teste, uma vez que com um tamanho de amostra maior, a vari√¢ncia da estat√≠stica de teste diminui.
IV. A diminui√ß√£o na vari√¢ncia leva a uma distribui√ß√£o mais concentrada em torno do valor esperado sob a hip√≥tese nula, o que pode levar a um *p-value* menor, para o mesmo valor observado da estat√≠stica de teste.
V.  Portanto, o *p-value* do teste pode ser afetado pela escolha do tamanho da amostra $T$. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que um modelo VAR com $p=0.01$ apresenta 5 exce√ß√µes em 250 dias de backtesting. Usando a distribui√ß√£o binomial, o p-value deste resultado √© 0.10, e o teste falha em rejeitar a hip√≥tese nula para um n√≠vel de signific√¢ncia de 5%. Contudo, se o mesmo modelo apresentar 20 exce√ß√µes em 1000 dias de backtesting, o p-value deste resultado √© 0.02, sendo estatisticamente significativo e levando √† rejei√ß√£o do modelo, para o mesmo n√≠vel de signific√¢ncia de 5%. Isto mostra que o p-value diminui com o aumento do tamanho da amostra, tornando o teste mais sens√≠vel ao desvio entre o n√∫mero de exce√ß√µes observado e o esperado.

### Conclus√£o

Este cap√≠tulo detalhou a import√¢ncia de considerar os erros tipo I e tipo II no contexto da verifica√ß√£o de modelos VAR, e como o n√∫mero de observa√ß√µes $T$ afeta o poder do teste e a probabilidade de se cometer tais erros.  A escolha do n√≠vel de signific√¢ncia $\alpha$, que controla a probabilidade de rejeitar um modelo correto, implica um balan√ßo com a probabilidade de n√£o detectar um modelo mal calibrado. Aumentar o n√∫mero de observa√ß√µes $T$ aumenta o poder do teste e permite uma avalia√ß√£o mais precisa da calibra√ß√£o do modelo, mas aumenta tamb√©m o risco de rejeitar um modelo bom. Para modelos com uma baixa probabilidade de exce√ß√£o $p$, o n√∫mero de observa√ß√µes necess√°rio para se atingir um bom n√≠vel de poder pode ser muito elevado. Portanto, a verifica√ß√£o de modelos VAR requer um equil√≠brio entre o risco de rejeitar modelos bons e o risco de aceitar modelos ruins, e a escolha do n√≠vel de confian√ßa, do n√∫mero de observa√ß√µes e de outras caracter√≠sticas do modelo devem ser consideradas com aten√ß√£o. A considera√ß√£o destes aspetos √© crucial para garantir que os modelos VAR forne√ßam medidas de risco fi√°veis e precisas. Al√©m disso, √© fundamental considerar a possibilidade de autocorrela√ß√£o das exce√ß√µes e as mudan√ßas nas condi√ß√µes de mercado durante o backtesting, para se obter uma avalia√ß√£o precisa e confi√°vel do modelo VAR.

### Refer√™ncias

[^1]: *‚ÄúThis chapter turns to backtesting techniques for verifying the accuracy of VAR models.‚Äù*
[^10]: *‚ÄúThe issue is how to make this decision. This accept or reject decision is a classic statistical decision problem. At the outset, it should be noted that this decision must be made at some confidence level.‚Äù*
[^11]: *‚ÄúThe distribution could be normal, or skewed, or with heavy tails, or time-varying. We simply count the number of exceptions. As a result, this approach is fully nonparametric.‚Äù*
<!-- END -->
