{
  "topics": [
    {
      "topic": "Backtesting Fundamentals",
      "sub_topics": [
        "Backtesting involves systematically comparing historical Value-at-Risk (VAR) forecasts with associated portfolio returns to validate model accuracy. It serves as a 'reality check' for VAR model calibration, identifying issues like faulty assumptions or incorrect parameters. This process is central to regulatory decisions and risk management.",
        "The core idea of backtesting is to compare predicted loss levels from a VAR model with actual observed losses to validate the model's accuracy. The goal is for the number of exceedances (actual losses exceeding predicted VAR) to align with the model's confidence level.",
        "A key aspect of backtesting is differentiating between actual returns (P&L), which are affected by intraday trades, and hypothetical returns, representing the performance of a 'frozen' portfolio based on previous day's close values. Hypothetical returns are crucial for evaluating the VAR model, while actual returns are essential for assessing real financial outcomes and regulatory compliance.",
        "The concept of 'cleaned return' approximates a hypothetical return by excluding non-market-to-market items like fees. Using hypothetical returns is crucial to evaluate the VAR model. The comparison between actual and predicted losses is often represented by plotting the absolute daily P&L against the VAR.",
        "The number of exceedances should be in line with the model's confidence level. Too many exceptions mean the model underestimates risk, while too few means it is overly conservative."
      ]
    },
    {
      "topic": "Statistical Framework for Backtesting",
      "sub_topics": [
        "Backtesting is a statistical hypothesis testing problem that uses failure rates (proportion of times VAR is exceeded) to assess model accuracy. The number of exceptions follows a binomial distribution.",
        "The central task of backtesting is to assess if the observed number of exceptions significantly deviates from what is expected under the null hypothesis that the model is correct, with the key concept being whether the number of exceptions is 'too small' or 'too large'. This assessment is based on a test that makes no assumptions about return distribution.",
        "The failure rate is defined as the number of exceptions (N) divided by the total number of observations (T). A model is considered calibrated if the failure rate converges to the specified probability level, which is closely aligned with the confidence level chosen.",
        "The number of exceptions (x) follows a binomial distribution: f(x) = (T choose x) * p^x * (1-p)^(T-x), with a mean of E(x) = pT and variance of V(x) = p(1-p)T. When T is large, a normal distribution can approximate this distribution using the central limit theorem.",
        "Model verification must address both type 1 errors (rejecting a correct model) and type 2 errors (not rejecting an incorrect model). The objective is to minimize both, where type 1 error is also known as the probability of rejecting the null hypothesis when it is true, and type 2 error is the probability of not rejecting the null hypothesis when it is false.",
        "For large sample sizes, the binomial distribution can be approximated using a normal distribution with the z-statistic formula: z = (x-pT) / sqrt(p(1-p)T). This simplifies hypothesis testing and helps evaluate if the model should be rejected."
      ]
    },
    {
      "topic": "Model Verification Based on Failure Rates",
      "sub_topics": [
        "The failure rate (N/T) should converge to the expected exception probability (p) as sample size increases. It's crucial to assess whether the number of exceptions (N) is 'too small' or 'too large' under the null hypothesis that p matches the chosen significance level.",
        "The z-statistic (z = (x - pT) / sqrt(p(1-p)T)) follows a standard normal distribution N(0,1), facilitating hypothesis testing. This is used to determine if the number of exceptions is statistically different from the expected value, with no specific assumption made regarding the return distribution.",
         "For a large enough sample size, the binomial distribution can be approximated by a normal distribution, making it straightforward to calculate the standardized z-score and set a threshold to identify poorly calibrated models.",
         "The expected value of exceptions, x, is E(x) = pT and the variance is V(x) = p(1-p)T. For large T, the binomial distribution can be approximated by the normal distribution using the central limit theorem.",
        "The probability mass function (PMF) of a binomial distribution is f(x) = (T choose x) * p^x * (1-p)^(T-x), where T is the number of trials, x is the number of successes (exceptions), and p is the probability of success. The expected value (mean) of x is E(x) = pT, and the variance is V(x) = p(1-p)T. When T is large, the distribution can be approximated using the normal distribution, allowing the computation of the z-statistic.",
        "Under the null hypothesis that the model is correctly calibrated, the number of exceptions x follows a binomial probability distribution. The simplest method to verify model accuracy is to record the failure rate, given by N/T.",
         "The test decision rule defines a cut-off value of |z| at a given confidence level, such as 1.96 for a 95% confidence level in a two-tailed test. This is used to determine if the number of exceptions is acceptably small.",
        "The test statistic z, which asymptotically follows a normal distribution with mean 0 and variance 1, is used to test if the number of exceptions is significantly different from the expected value. The null hypothesis is rejected if the absolute value of z is greater than a cutoff value, such as 1.96 for a 95% test.",
        "The z-statistic is computed as z = (x - pT) / sqrt(p(1-p)T), and for a two-tailed test at a 95% confidence level, the null hypothesis is rejected if |z| is greater than 1.96. This is equivalent to a chi-square test.",
        "This method is nonparametric, meaning that it does not assume a specific return distribution, and is based on the concept of Bernoulli trials, modeling a sequence of successes or failures. Under the null hypothesis, the number of exceptions (x) follows a binomial distribution. The test seeks deviations from the expected value.",
          "Financial institutions must consider type 1 errors (rejecting a correct model) and type 2 errors (not rejecting an incorrect model). For a fixed value of p, the non-rejection region shrinks as the number of observations (T) increases, meaning greater test power but also higher risk of rejecting a true null hypothesis.",
        "This process tests model calibration by determining if the probability of rejecting a correct model (Type 1 error) is low (e.g., 10.8%), and also the probability of not rejecting an incorrect model (Type 2 error)."
      ]
    },
    {
      "topic": "Likelihood Ratio Test for Unconditional Coverage",
      "sub_topics": [
         "The likelihood ratio test for unconditional coverage (LRuc) determines if the observed failure rate is consistent with the expected failure rate, thus testing the unconditional coverage of the VAR model.",
         "The test statistic is given by LRuc = -2 * ln[(1-p)^(T-N) * p^N] + 2 * ln[(1 - (N/T))^(T-N) * (N/T)^N], where N is the number of exceptions, T is the total number of observations, and p is the expected failure rate.",
        "This statistic asymptotically follows a chi-square distribution with one degree of freedom. The null hypothesis is rejected if LRuc is greater than 3.841, which corresponds to a 95% confidence level.",
        "LRuc is asymptotically distributed as a chi-square distribution with one degree of freedom. Under the null hypothesis that the model is correctly calibrated, we would reject it if LRuc > 3.841, which is the critical value for a 95% confidence level.",
        "This test is equivalent to the z-statistic because a chi-square variable with one degree of freedom is the square of a standard normal variable."
      ]
    },
    {
      "topic": "Conditional Coverage Models",
      "sub_topics": [
        "Conditional coverage assesses whether exceptions are serially independent and do not cluster together over time, addressing the limitation of unconditional coverage tests that ignore time variation or dependence in exception occurrences. This tests if exceptions are serially independent, defining Tij as the number of days state j occurred, given state i the previous day, where states i and j can be '0' for no exception or '1' for an exception.",
        "The framework based on unconditional coverage ignores time variation. Observed exceptions may cluster, which would invalidate the model. A system for backtesting should measure conditional coverage, or determine if exceptions are dependent on current conditions.",
        "A deviation indicator is set to 0 if VAR is not exceeded and to 1 if it is exceeded. T_ij represents the number of days when state j occurred today given state i yesterday. An example of conditional exceptions includes identifying how many exceptions occur following other exceptions.",
        "To test conditional coverage, a deviation indicator is set to 0 if VAR is not exceeded and to 1 if it is. Tij is the number of days in which state j occurred one day while it was at i the previous day. We define πi as the probability of observing an exception conditional on state i the previous day. If today's exception is independent of yesterday's, then π0 = π1.",
        "The likelihood ratio test for independence is defined as LRind = -2 * ln[(1 – π)^(T00 + T10) * π^(T01 + T11)] + 2 * ln[(1 – π0)^T00 * π0^T01 * (1 - π1)^T10 * π1^T11], where π is the unconditional exception probability, π0 = T01 / (T00 + T01), and π1 = T11 / (T10 + T11).",
        "LRind is asymptotically distributed as a chi-square distribution with one degree of freedom. Independence is rejected if LRind is greater than a critical value of 3.841 (95% confidence level).",
        "Christoffersen's test extends the LRuc test to determine if deviations are serially independent. This test sets a deviation indicator equal to 1 when VAR is exceeded and equal to 0 when VAR is not exceeded.",
        "The combined test statistic for conditional coverage includes both the LRuc and LRind results. The resulting distribution follows a chi-squared (x²) distribution, and when it's greater than a given threshold the hypothesis is rejected.",
        "The combined test statistic for conditional coverage is LRcc = LRuc + LRind, and each component is independently distributed as chi-square with 1 degree of freedom. The sum is distributed as chi-square with 2 degrees of freedom. A 95% confidence level has a critical value of 5.991. Independence is rejected if LRind > 3.841.",
        "The combined test statistic for conditional coverage is defined as LRcc = LRuc + LRind, which is the sum of the likelihood ratio tests for unconditional coverage and independence. This is distributed as a chi-square distribution with two degrees of freedom. We reject at the 95% confidence level if LRcc > 5.991.",
        "The formulas use the following notation: π = (T01 + T11)/T, π0 = T01 / (T00 + T01), and π1 = T11 / (T10 + T11). T00 is the number of days without exception preceded by a day without exception, T01 is the number of days with exceptions preceded by a day without exception, T10 is the number of days without exceptions preceded by a day with exception, and T11 is the number of days with exceptions preceded by a day with exception.",
         "The probability of observing an exception conditional on state i of the previous day is denoted by pi (either pi_0 or pi_1), where pi_0 = T_01/T_0, and pi_1 = T_11/T_1. where T_0 and T_1 represent the total number of days when the previous state was 0 or 1 respectively.",
        "The test identifies a transition matrix that relates today's exception to yesterday's exception (or non-exception) and calculates the probabilities of exceptions conditional on previous events. The test statistic for serial independence is LR_ind = -2 * ln[(1 - pi_hat)^(T00+T10) * (pi_hat)^(T01+T11)] + 2 * ln[(1 - pi_0)^T00 * pi_0^T01 * (1 - pi_1)^T10 * pi_1^T11 ], where pi_hat = (T01+T11)/T. This follows a chi-square distribution with one degree of freedom, rejecting the null if LRind > 3.841.",
        "The test statistic indicates that if today's exception is independent of yesterday's, then the transition probabilities will be identical. A conditional exception table can signal issues with the risk model and highlight a clustering effect.",
         "The model should measure conditional coverage based on current market conditions."
      ]
    },
    {
      "topic": "Extensions and Alternative Tests",
       "sub_topics": [
        "Standard exception tests often lack power, especially with high VAR confidence levels and low observations. This motivates the need for improved tests, where more observations improve test power. Duration-based tests analyze the time period between exceptions.",
         "Parametric backtests can use information from the underlying probability distribution used to calculate VAR, particularly when VAR is a multiple of the standard deviation. These tests analyze if realized and forecast volatility fit properly.",
         "The Kuiper statistic focuses on the entire probability distribution rather than just exceptions, providing a more comprehensive assessment of the model's fit. This test is non-parametric but more powerful.",
         "Statistical decision theory shows exception tests are the most powerful in their class. More powerful tests use more information, such as the full probability distribution or the duration between exceptions."
      ]
    },
    {
      "topic": "The Basel Rules",
      "sub_topics": [
       "The Basel Committee rules for backtesting are derived from failure rate testing. They determine if a model is good or bad, define the error types, and set consequences for financial institutions.",
         "The Basel framework uses a “traffic light” approach, categorizing banks into ‘green,’ ‘yellow,’ or ‘red’ zones, based on their number of VAR exceptions, with associated implications and potential penalties using multiplication factors.",
          "The Basel framework records daily exceptions of a 99% VAR over the last year. A model is expected to have 2.5 exceptions over 250 days. Up to 4 exceptions is acceptable ('green light'). 5 or more exceptions activate the 'yellow' zone, and the 'red' zone incurs an automatic penalty.",
        "The Basel Committee's verification process consists of recording daily exceptions of the 99% VAR over the last year, expecting about 2.5 instances of exceptions. Up to 4 exceptions is 'green light,' 5 or more activate the 'yellow' zone, and the 'red' zone incurs a penalty.",
        "The Basel framework aims to balance type 1 errors (rejecting a correct model) and type 2 errors (accepting an incorrect model). The test is designed with a low type 1 error rate, but it also has high type 2 error rates for banks that intentionally cheat on VAR reports.",
        "The framework defines three zones: 'green' with less than 4 exceptions, 'yellow' for 5 or more exceptions, where the risk model is questionable, and 'red' for 10 or more exceptions, resulting in a penalty. The framework uses a 99% coverage, generating few exceptions, which limits test power and allows high type 2 error rates.",
           "Banks under the 'yellow' zone face supervisory review, with penalties depending on the reason for the exception (model integrity, accuracy, intraday trading, or bad luck).",
        "Within the 'yellow' zone, the penalty is determined by the supervisor depending on the reason for the exception: model integrity, model accuracy, intraday trading, or bad luck. If the number of exceptions is between 5 and 9, the bank incurs a progressive penalty where the multiplicative factor k increases from 3 to 4. A 'red' zone incurs an automatic penalty where k=4, otherwise k=3.",
         "The Basel framework identifies reasons for exceptions: basic integrity of model, model accuracy, intraday trading, and bad luck. It provides flexibility in how rules are applied.",
        "Basel's penalty guidelines are suitably vague, allowing flexibility while focusing on separating bad luck from poor performance, and balancing type 1 and 2 errors. The probability of a type 1 error is 10.8 percent, and a high type 2 error (12.8%).",
         "The framework can be improved by lowering the VAR confidence levels, since low levels generate more exceptions and thus more information. If the VAR confidence level is reduced from 99 to 95 percent, then the type 1 error rate is close to the Basel framework value, but the type 2 error is now lower.",
        "Reducing the VAR confidence level from 99 to 95% significantly reduces the probability of not identifying a faulty model, while keeping the type 1 error rate similar, by using a cutoff for the number of exceptions.",
         "Regulators also consider type 2 errors: the risk of accepting a faulty model, which represents a conflict as a supervisor should not fail a bank that is willfully cheating its VAR reporting.",
        "Regulators design backtesting rules considering type 1 error rates, the probability of rejecting a correct model. They aim to avoid unduly penalizing banks for bad luck and instead encourage risk management best practices."
      ]
    },
      {
      "topic": "Backtesting VAR",
       "sub_topics": [
          "Backtesting involves the systematic comparison of historical VAR forecasts with associated portfolio returns, serving as a 'reality check' to verify the calibration of VAR models, helping to identify faulty assumptions, incorrect parameters, or inaccurate modeling. Backtesting is central to regulatory decisions.",
          "Backtesting is a statistical hypothesis testing problem, using failure rates to assess model accuracy using Bernoulli trials. The number of exceptions follows a binomial distribution, approximated by a normal distribution for large sample sizes, enabling z-statistics.",
          "Backtesting is a formal statistical framework to verify if real losses align with projected losses. It involves comparing the history of VAR forecasts with associated portfolio returns, serving as a 'reality check' for model calibration. Backtesting is essential for VAR users and risk managers to ensure VAR forecasts are well-calibrated.",
          "Model validation, a critical process, involves backtesting to ensure the adequacy of Value-at-Risk (VAR) models. Backtesting compares predicted losses with actual losses to evaluate VAR model accuracy. Ideally, the number of observations exceeding the VAR should be consistent with the chosen confidence level.",
           "Value-at-Risk (VAR) models are useful for risk prediction, but require validation through backtesting to verify their accuracy by comparing historical VAR forecasts against actual portfolio returns. Backtesting can be extended to use parametric information like realized and forecasted volatility if the VAR is obtained from a multiple of the standard deviation, enabling more powerful tests that incorporate more information.",
           "Backtesting frameworks can be improved using a lower VAR confidence level or by increasing the number of data observations. It is important to acknowledge that trading portfolios do change over time, and the models used evolve, adding to structural instability. However, backtesting is a crucial component of risk management to ensure that risk models do not fail.",
            "The core of backtesting lies in examining whether the number of observations falling outside the VAR aligns with the model's confidence level. Too many exceptions indicate risk underestimation, while too few may suggest excessive capital allocation. The fundamental procedure of backtesting involves counting deviations from the VAR model, comparing the number of exceptions against what's expected given the model’s confidence level.",
          "Backtesting involves comparing historical VAR with subsequent returns. Given a specified confidence level, the figure should be exceeded in some instances, aligning with the probability of the chosen confidence level. The process is essential for ensuring that VAR models are well-calibrated, and should be re-examined for faulty assumptions, incorrect parameters, or inaccurate modeling.",
           "The process of backtesting involves the systematic analysis of historical VAR measures in relation to subsequent returns, with the objective of verifying if the number of exceptions aligns with the predefined confidence level. The backtesting approach involves determining if the observed number of exceptions deviates significantly from the expected number.",
         "The selection of a short horizon, such as daily returns, for backtesting helps mitigate the impact of portfolio changes and contamination, which refers to changes in the portfolio composition due to trading activity during the day. It also provides a higher frequency for validation data.",
           "To address this contamination, backtesting should consider both the actual portfolio return and a hypothetical return that closely matches the VAR forecast, typically obtained from fixed positions applied to actual security returns from close to close. The choice of return type, either hypothetical (frozen portfolio) or actual return (P&L), impacts backtesting. Ideally both should be used to provide comparative information.",
         "Discrepancies between backtesting results may indicate a model issue or problems with intra-day trading practices. Both actual and hypothetical returns are valuable in backtesting, with actual returns reflecting real profits and losses scrutinized by regulators, and hypothetical returns indicating the model's accuracy under controlled conditions. Divergences between backtesting results using both measures may reveal issues with intraday trading.",
        "In backtesting, a key data issue arises from the assumption that portfolios remain frozen over a specific horizon. However, in practice, trading portfolios evolve dynamically, leading to contamination in actual returns. The use of hypothetical returns is crucial in backtesting to match the nature of VAR forecasts, although actual returns are also informative. The use of a Z-statistic is a convenient shortcut to measure the deviation from the expected number of exceptions. If the absolute value of Z is greater than 1.96, the model is rejected.",
          "The test design involves a trade-off between type 1 and type 2 errors. A good verification test needs to balance these two types of errors. The test is considered powerful when it presents a low type 1 error and a low type 2 error.",
          "The design of a backtesting framework requires maximizing the probability of identifying banks that intentionally understate risk, while avoiding penalizing banks that exceed VAR due to bad luck, which highlights the statistical decision-making inherent in backtesting procedures. Type 1 and Type 2 errors are central to backtesting decisions. Minimizing both is a tradeoff that needs to be carefully managed.",
           "The test for the number of exceptions is a hypothesis test for a sequence of successes and failures, and the distribution of exceptions can be approximated by the normal distribution for large samples. The decision to reject the model must be made at some confidence level, which is not related to the probability of the VAR. The decision rule may involve rejecting the null hypothesis based on an excessively large number of exceptions, but also based on an excessively small number of exceptions.",
         "The choice of the confidence level for the decision rule is independent from the quantitative level chosen for the VAR model. Both are critical elements of a backtesting strategy. The frequency of exceptions is a critical parameter, where too many exceptions suggest an underestimation of risk, requiring corrective actions to the model. Exceedances, also called exceptions, indicate potential risk underestimation or model calibration issues, which could lead to insufficient capital allocation or regulatory penalties.",
         "The concept of 'exception' is central to backtesting, representing occurrences where actual losses exceed the predicted VAR. The number of exceptions is used to assess the model's accuracy. The use of the normal approximation for the binomial distribution, especially for large samples, reduces computational complexity by simplifying the calculations of the probability of occurrence of exceptions."
      ]
    }
  ]
}