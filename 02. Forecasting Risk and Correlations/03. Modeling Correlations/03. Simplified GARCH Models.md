### Introdu√ß√£o

Este cap√≠tulo aprofunda-se na modelagem de correla√ß√µes, especialmente com o uso de modelos GARCH multivariados simplificados, como os modelos diagonal VEC (DVEC) e scalar GARCH. Em continuidade √† discuss√£o sobre a import√¢ncia da modelagem de correla√ß√µes para a gest√£o de risco [^14], exploraremos as simplifica√ß√µes impostas a esses modelos, as implica√ß√µes para a precis√£o e o desafio de garantir a positividade definida da matriz de covari√¢ncia [^16, 25].

### Modelos GARCH Multivariados Simplificados

Como mencionado anteriormente, a principal limita√ß√£o ao uso de modelos GARCH multivariados √© a grande quantidade de par√¢metros a serem estimados, que cresce exponencialmente com o n√∫mero de ativos [^16]. Para tornar esses modelos mais trat√°veis, v√°rias simplifica√ß√µes s√£o empregadas. Nesta se√ß√£o, nos concentraremos em dois tipos de modelos GARCH multivariados simplificados: os modelos Diagonal VEC (DVEC) e os modelos Scalar GARCH.

#### Modelos Diagonal VEC (DVEC)

Os modelos Diagonal VEC (DVEC) s√£o uma forma de simplificar a estrutura de depend√™ncia nos modelos GARCH multivariados [^16]. Em vez de permitir que todos os par√¢metros da matriz de covari√¢ncia condicional interajam livremente, os modelos DVEC imp√µem a restri√ß√£o de que as matrizes de par√¢metros sejam diagonais [^16]. Isso significa que cada vari√¢ncia condicional e covari√¢ncia condicional depende apenas de seus pr√≥prios *lags* e dos *lags* das inova√ß√µes correspondentes, eliminando a depend√™ncia cruzada entre diferentes elementos da matriz de covari√¢ncia [^16].

> üí° **Formaliza√ß√£o Matem√°tica:** Considere um modelo VEC (Vector Error Correction) geral para $N$ ativos. A equa√ß√£o para a matriz de covari√¢ncia condicional $H_t$ pode ser escrita como:
>
> $$vec(H_t) = C + A \cdot vec(\epsilon_{t-1} \epsilon_{t-1}') + B \cdot vec(H_{t-1})$$
>
> Onde:
>
> *   $vec(\cdot)$ denota o operador de vetoriza√ß√£o, que transforma uma matriz em um vetor empilhando suas colunas.
> *   $H_t$ √© a matriz de covari√¢ncia condicional $N \times N$ no tempo $t$.
> *   $\epsilon_t$ √© o vetor de res√≠duos $N \times 1$ no tempo $t$.
> *   $C$ √© um vetor de constantes $N(N+1)/2 \times 1$.
> *   $A$ e $B$ s√£o matrizes de par√¢metros $N(N+1)/2 \times N(N+1)/2$.
>
> Em um modelo DVEC, as matrizes $A$ e $B$ s√£o restritas a serem diagonais. Isso significa que o elemento $(i, j)$ de $A$ e $B$ √© zero se $i \neq j$. Portanto, a equa√ß√£o se simplifica para:
>
> $$h_{ij,t} = c_{ij} + a_{ij} \epsilon_{i,t-1} \epsilon_{j,t-1} + b_{ij} h_{ij,t-1}$$
>
> Onde:
>
> *   $h_{ij,t}$ √© a covari√¢ncia condicional entre os ativos $i$ e $j$ no tempo $t$.
> *   $\epsilon_{i,t-1}$ e $\epsilon_{j,t-1}$ s√£o os res√≠duos dos ativos $i$ e $j$ no tempo $t-1$.
> *   $c_{ij}$, $a_{ij}$ e $b_{ij}$ s√£o os par√¢metros correspondentes.
>
> A restri√ß√£o diagonal reduz drasticamente o n√∫mero de par√¢metros a serem estimados, tornando a estimativa do modelo mais fact√≠vel, especialmente para portf√≥lios com muitos ativos [^16].
>
> üí° **Exemplo Num√©rico:** Para um portf√≥lio com 2 ativos, um modelo VEC completo tem 21 par√¢metros, enquanto um modelo DVEC tem apenas 9 par√¢metros [^16]. A simplifica√ß√£o DVEC reduz o n√∫mero de par√¢metros a serem estimados, mas pode n√£o capturar totalmente a complexidade da din√¢mica da covari√¢ncia.

**Proposi√ß√£o 1:** A imposi√ß√£o da estrutura diagonal nas matrizes $A$ e $B$ em um modelo VEC GARCH garante que a matriz de covari√¢ncia condicional resultante seja positiva definida se os par√¢metros diagonais $a_{ij}$ e $b_{ij}$ forem n√£o negativos e as vari√¢ncias iniciais forem positivas.

*Estrat√©gia de Prova:*

1. **Defini√ß√£o de Positividade Definida:** Uma matriz √© positiva definida se todos os seus autovalores forem positivos, ou equivalentemente, se $x^T H_t x > 0$ para todo vetor n√£o nulo $x$.
2. **Condi√ß√µes para DVEC:** Dada a estrutura do modelo DVEC:  $h_{ij,t} = c_{ij} + a_{ij} \epsilon_{i,t-1} \epsilon_{j,t-1} + b_{ij} h_{ij,t-1}$, a positividade definida pode ser garantida se $a_{ij} \geq 0$, $b_{ij} \geq 0$ e as vari√¢ncias condicionais iniciais $h_{ii,0} > 0$ [^16].
3. **Positividade das Vari√¢ncias Condicionais:** Se $h_{ii,t-1} > 0$ e $a_{ii} \geq 0$, ent√£o $h_{ii,t} = c_{ii} + a_{ii} \epsilon_{i,t-1}^2 + b_{ii} h_{ii,t-1} > 0$, pois $c_{ii}$ √© uma constante e $\epsilon_{i,t-1}^2$ √© sempre n√£o negativo.
4. **Propaga√ß√£o da Positividade:** Se as vari√¢ncias condicionais forem positivas, a matriz $H_t$ ser√° positiva definida.

*Prova:*

I. **Hip√≥tese:** Assumimos que $a_{ij} \geq 0$, $b_{ij} \geq 0$ para todo $i,j$ e as vari√¢ncias condicionais iniciais $h_{ii,0} > 0$.

II. **Passo Base (t=1):** Mostraremos que se as condi√ß√µes s√£o satisfeitas no tempo $t=0$, ent√£o $H_1$ √© positiva definida. Dado que as vari√¢ncias iniciais s√£o positivas, $h_{ii,0} > 0$ para todo $i$.

III. **Passo Indutivo:** Assumimos que $H_{t-1}$ √© positiva definida e mostraremos que $H_t$ tamb√©m √© positiva definida.
    *   Como $a_{ij} \geq 0$, $b_{ij} \geq 0$ e $h_{ii,t-1} > 0$, ent√£o $h_{ii,t} = c_{ii} + a_{ii} \epsilon_{i,t-1}^2 + b_{ii} h_{ii,t-1} > 0$.
    *   As covari√¢ncias $h_{ij,t}$ s√£o afetadas apenas por seus pr√≥prios lags e res√≠duos, mantendo a estrutura diagonal, logo, as vari√¢ncias positivas garantem a positividade definida de $H_t$.

IV. **Conclus√£o:** Dado que a positividade das vari√¢ncias condicionais se propaga ao longo do tempo e que $a_{ij} \geq 0$, $b_{ij} \geq 0$, a matriz $H_t$ √© positiva definida para todo $t$. ‚ñ†

> üí° **Observa√ß√£o:** Embora os modelos DVEC simplifiquem a estrutura de depend√™ncia, eles ainda podem capturar uma parte significativa da din√¢mica da volatilidade e da covari√¢ncia, tornando-os √∫teis em muitas aplica√ß√µes pr√°ticas. A imposi√ß√£o de uma estrutura diagonal reduz o n√∫mero de par√¢metros a serem estimados e, consequentemente, o risco de *overfitting* [^16].
>
> üí° **Exemplo Num√©rico:** Suponha que estamos estimando um modelo DVEC para dois ativos. Ap√≥s a estima√ß√£o, obtemos os seguintes par√¢metros:
>
> $a_{11} = 0.05, a_{22} = 0.03, b_{11} = 0.9, b_{22} = 0.85$
>
> Suponha que os res√≠duos ao quadrado no tempo $t-1$ sejam:
>
> $\epsilon_{1,t-1}^2 = 0.0004, \epsilon_{2,t-1}^2 = 0.0009$
>
> E as vari√¢ncias condicionais no tempo $t-1$ sejam:
>
> $h_{11,t-1} = 0.0016, h_{22,t-1} = 0.0025$
>
> Ent√£o, as vari√¢ncias condicionais no tempo $t$ seriam:
>
> $h_{11,t} = c_{11} + 0.05(0.0004) + 0.9(0.0016) = 0.00002 + 0.00144 + c_{11}$
>
> $h_{22,t} = c_{22} + 0.03(0.0009) + 0.85(0.0025) = 0.000027 + 0.002125 + c_{22}$
>
> Se $c_{11} = 0.0001$ e $c_{22} = 0.0002$, ent√£o
>
> $h_{11,t} = 0.0001 + 0.00002 + 0.00144 = 0.00156$ e $h_{22,t} = 0.0002 + 0.000027 + 0.002125 = 0.002352$
>
> A estrutura diagonal garante que a vari√¢ncia de cada ativo dependa apenas de seus pr√≥prios res√≠duos e da sua vari√¢ncia passada, mantendo a simplicidade do modelo.
> ```python
> import numpy as np
>
> # Par√¢metros do modelo DVEC
> a11 = 0.05
> a22 = 0.03
> b11 = 0.9
> b22 = 0.85
> c11 = 0.0001
> c22 = 0.0002
>
> # Res√≠duos ao quadrado no tempo t-1
> epsilon1_sq = 0.0004
> epsilon2_sq = 0.0009
>
> # Vari√¢ncias condicionais no tempo t-1
> h11_t_minus_1 = 0.0016
> h22_t_minus_1 = 0.0025
>
> # C√°lculo das vari√¢ncias condicionais no tempo t
> h11_t = c11 + a11 * epsilon1_sq + b11 * h11_t_minus_1
> h22_t = c22 + a22 * epsilon2_sq + b22 * h22_t_minus_1
>
> print(f"Vari√¢ncia condicional do ativo 1 no tempo t: {h11_t}")
> print(f"Vari√¢ncia condicional do ativo 2 no tempo t: {h22_t}")
> ```
> Este exemplo demonstra como os par√¢metros $a_{ii}$ e $b_{ii}$ afetam a vari√¢ncia condicional de cada ativo, mostrando a autocorrela√ß√£o na volatilidade.

**Corol√°rio 1:** Se os par√¢metros $a_{ij}$ e $b_{ij}$ em um modelo DVEC forem estimados de forma que $a_{ij} + b_{ij} < 1$ para todo $i,j$, ent√£o o modelo ser√° estacion√°rio e as vari√¢ncias e covari√¢ncias condicionais convergir√£o para seus valores incondicionais.

*Estrat√©gia de Prova:*

1.  **Estacionariedade:** Um processo GARCH √© estacion√°rio se a soma dos par√¢metros de persist√™ncia for menor que 1.
2.  **Converg√™ncia:** Se um processo √© estacion√°rio, ent√£o suas vari√¢ncias e covari√¢ncias condicionais convergem para seus valores incondicionais √† medida que o tempo tende ao infinito.
3.  **Aplica√ß√£o ao DVEC:** Aplicar as condi√ß√µes de estacionariedade e converg√™ncia ao modelo DVEC.

*Prova:*

I. **Hip√≥tese:** Assumimos que $a_{ij} + b_{ij} < 1$ para todo $i,j$.

II. **Estacionariedade:** Para cada vari√¢ncia condicional $h_{ii,t}$, a equa√ß√£o √© $h_{ii,t} = c_{ii} + a_{ii} \epsilon_{i,t-1}^2 + b_{ii} h_{ii,t-1}$. A condi√ß√£o $a_{ii} + b_{ii} < 1$ garante que o processo GARCH para cada vari√¢ncia condicional √© estacion√°rio.

III. **Converg√™ncia:** Se cada processo GARCH para as vari√¢ncias condicionais √© estacion√°rio, ent√£o $h_{ii,t}$ converge para um valor incondicional √† medida que $t \rightarrow \infty$. Similarmente, as covari√¢ncias condicionais $h_{ij,t}$ tamb√©m convergem para seus valores incondicionais sob a mesma condi√ß√£o.

IV. **Conclus√£o:** Portanto, se $a_{ij} + b_{ij} < 1$ para todo $i,j$, o modelo DVEC ser√° estacion√°rio e as vari√¢ncias e covari√¢ncias condicionais convergir√£o para seus valores incondicionais. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando os par√¢metros do exemplo anterior ($a_{11} = 0.05, b_{11} = 0.9, a_{22} = 0.03, b_{22} = 0.85$), verificamos a condi√ß√£o de estacionariedade:
>
> $a_{11} + b_{11} = 0.05 + 0.9 = 0.95 < 1$
> $a_{22} + b_{22} = 0.03 + 0.85 = 0.88 < 1$
>
> Ambos os ativos satisfazem a condi√ß√£o de estacionariedade. Isso significa que, ao longo do tempo, as vari√¢ncias condicionais de ambos os ativos se estabilizar√£o em torno de seus n√≠veis incondicionais, tornando o modelo √∫til para previs√µes de longo prazo.

#### Modelos Scalar GARCH

Os modelos Scalar GARCH imp√µem uma restri√ß√£o ainda maior √† estrutura de depend√™ncia do que os modelos DVEC [^16]. Nesses modelos, assume-se que todas as vari√¢ncias condicionais e covari√¢ncias condicionais s√£o impulsionadas por uma din√¢mica comum, representada por um √∫nico processo GARCH [^16]. Isso implica que todas as s√©ries temporais t√™m a mesma persist√™ncia e resposta a choques, embora seus n√≠veis possam ser diferentes [^16].

> üí° **Formaliza√ß√£o Matem√°tica:** Em um modelo Scalar GARCH, a matriz de covari√¢ncia condicional $H_t$ √© modelada como:
>
> $$H_t = \bar{H} \odot h_t$$
>
> Onde:
>
> *   $\bar{H}$ √© uma matriz de covari√¢ncia incondicional est√°tica.
> *   $h_t$ √© um processo GARCH escalar que impulsiona a evolu√ß√£o temporal de todos os elementos de $H_t$.
> *   $\odot$ representa o produto de Hadamard (produto elemento a elemento) [^16].
>
> A equa√ß√£o para o processo GARCH escalar $h_t$ √©:
>
> $$h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}' \epsilon_{t-1} + \beta h_{t-1}$$
>
> Onde:
>
> *   $\alpha_0$, $\alpha_1$ e $\beta$ s√£o par√¢metros escalares que determinam a din√¢mica do processo GARCH.
> *   $\epsilon_{t-1}$ √© um vetor de res√≠duos no tempo $t-1$.
>
> Essa estrutura garante que todas as vari√¢ncias e covari√¢ncias condicionais variem proporcionalmente ao processo $h_t$, mantendo as correla√ß√µes constantes ao longo do tempo [^16].
>
> üí° **Exemplo Num√©rico:** Considere um portf√≥lio com dois ativos. A matriz de covari√¢ncia incondicional √©:
>
> $$\bar{H} = \begin{bmatrix} 1.0 & 0.5 \\ 0.5 & 1.0 \end{bmatrix}$$
>
> O processo GARCH escalar √© definido por:
>
> $h_t = 0.01 + 0.1 \epsilon_{t-1}' \epsilon_{t-1} + 0.8 h_{t-1}$
>
> Se $h_{t-1} = 0.04$ e $\epsilon_{t-1} = [0.1, -0.1]^T$, ent√£o:
>
> $h_t = 0.01 + 0.1 ([0.1, -0.1] \cdot [0.1, -0.1]^T) + 0.8 (0.04)$
>
> $h_t = 0.01 + 0.1 (0.02) + 0.032 = 0.044$
>
> A matriz de covari√¢ncia condicional no tempo $t$ √©:
>
> $$H_t = \begin{bmatrix} 1.0 & 0.5 \\ 0.5 & 1.0 \end{bmatrix} \odot 0.044 = \begin{bmatrix} 0.044 & 0.022 \\ 0.022 & 0.044 \end{bmatrix}$$
>
> A estrutura Scalar GARCH imp√µe uma din√¢mica comum a todas as vari√¢ncias e covari√¢ncias, simplificando a estimativa, mas limitando a flexibilidade do modelo.

**Proposi√ß√£o 2:** Em um modelo Scalar GARCH, se a matriz de covari√¢ncia incondicional $\bar{H}$ for positiva definida e o processo GARCH escalar $h_t$ gerar valores positivos, ent√£o a matriz de covari√¢ncia condicional $H_t$ ser√° sempre positiva definida.

*Estrat√©gia de Prova:*

1. **Defini√ß√£o de Positividade Definida:** Uma matriz √© positiva definida se todos os seus autovalores forem positivos, ou equivalentemente, se $x^T H_t x > 0$ para todo vetor n√£o nulo $x$.
2. **Condi√ß√µes para Scalar GARCH:** Dada a estrutura do modelo Scalar GARCH, $H_t = \bar{H} \odot h_t$, a positividade definida pode ser garantida se $\bar{H}$ for positiva definida e $h_t > 0$ [^16].
3. **Positividade do Processo GARCH Escalar:** Para garantir que $h_t > 0$, √© necess√°rio que $\alpha_0 > 0$ e que a persist√™ncia do processo seja menor que 1 (i.e., $\alpha_1 + \beta < 1$).

*Prova:*

I. **Hip√≥tese:** Assumimos que $\bar{H}$ √© positiva definida, $\alpha_0 > 0$, e $\alpha_1 + \beta < 1$.

II. **Passo Base (t=1):** Mostraremos que se as condi√ß√µes s√£o satisfeitas, ent√£o $H_1$ √© positiva definida.
    *   Como $\alpha_0 > 0$, o processo GARCH escalar gera valores positivos, $h_1 > 0$.
    *   Dado que $\bar{H}$ √© positiva definida e $h_1 > 0$, ent√£o $H_1 = \bar{H} \odot h_1$ tamb√©m √© positiva definida, pois multiplicar cada elemento de uma matriz positiva definida por um escalar positivo preserva a positividade definida.

III. **Passo Indutivo:** Assumimos que $H_{t-1}$ √© positiva definida e mostraremos que $H_t$ tamb√©m √© positiva definida.
    *   Como $\alpha_0 > 0$, $\alpha_1 + \beta < 1$, e $h_{t-1} > 0$, ent√£o $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}' \epsilon_{t-1} + \beta h_{t-1} > 0$.
    *   Dado que $\bar{H}$ √© positiva definida e $h_t > 0$, ent√£o $H_t = \bar{H} \odot h_t$ tamb√©m √© positiva definida.

IV. **Conclus√£o:** Dado que a positividade do processo GARCH escalar se propaga ao longo do tempo e que $\bar{H}$ √© positiva definida, a matriz $H_t$ √© positiva definida para todo $t$. ‚ñ†

> üí° **Observa√ß√£o:** Os modelos Scalar GARCH s√£o os mais simples entre os modelos GARCH multivariados, facilitando a estimativa e a implementa√ß√£o [^16]. No entanto, a imposi√ß√£o de uma din√¢mica comum a todas as s√©ries temporais pode ser uma restri√ß√£o muito forte em muitas aplica√ß√µes, levando a uma representa√ß√£o inadequada da estrutura de depend√™ncia e a previs√µes de risco imprecisas [^25].
>
> üí° **Exemplo Num√©rico:** Suponha que tenhamos estimado um modelo Scalar GARCH para um portf√≥lio com dois ativos. Ap√≥s a estima√ß√£o, obtemos os seguintes par√¢metros para o processo GARCH escalar:
>
> $\alpha_0 = 0.0001, \alpha_1 = 0.1, \beta = 0.8$
>
> A matriz de covari√¢ncia incondicional √©:
>
> $$\bar{H} = \begin{bmatrix} 0.0025 & 0.0010 \\ 0.0010 & 0.0036 \end{bmatrix}$$
>
> Suponha que os res√≠duos no tempo $t-1$ sejam:
>
> $\epsilon_{1,t-1} = 0.02, \epsilon_{2,t-1} = -0.01$
>
> E o valor do processo GARCH escalar no tempo $t-1$ seja:
>
> $h_{t-1} = 0.0016$
>
> Ent√£o, o valor do processo GARCH escalar no tempo $t$ seria:
>
> $h_t = 0.0001 + 0.1 (0.02^2 + (-0.01)^2) + 0.8 (0.0016) = 0.0001 + 0.00005 + 0.00128 = 0.00143$
>
> A matriz de covari√¢ncia condicional no tempo $t$ √©:
>
> $$H_t = \begin{bmatrix} 0.0025 & 0.0010 \\ 0.0010 & 0.0036 \end{bmatrix} \odot 0.00143 = \begin{bmatrix} 0.000003575 & 0.00000143 \\ 0.00000143 & 0.000005148 \end{bmatrix}$$
>
> Observe que a correla√ß√£o entre os ativos permanece constante ao longo do tempo, pois a varia√ß√£o √© impulsionada por um √∫nico processo GARCH escalar.
> ```python
> import numpy as np
>
> # Par√¢metros do modelo Scalar GARCH
> alpha_0 = 0.0001
> alpha_1 = 0.1
> beta = 0.8
>
> # Matriz de covari√¢ncia incondicional
> H_bar = np.array([[0.0025, 0.0010],
>                   [0.0010, 0.0036]])
>
> # Res√≠duos no tempo t-1
> epsilon_t_minus_1 = np.array([0.02, -0.01])
>
> # Valor do processo GARCH escalar no tempo t-1
> h_t_minus_1 = 0.0016
>
> # C√°lculo do processo GARCH escalar no tempo t
> h_t = alpha_0 + alpha_1 * (epsilon_t_minus_1.T @ epsilon_t_minus_1) + beta * h_t_minus_1
>
> # C√°lculo da matriz de covari√¢ncia condicional no tempo t
> H_t = H_bar * h_t  # Produto de Hadamard
>
> print(f"Processo GARCH escalar no tempo t: {h_t}")
> print("Matriz de covari√¢ncia condicional no tempo t:\n", H_t)
> ```
> Este exemplo ilustra como o processo GARCH escalar, impulsionado pelos res√≠duos, afeta a matriz de covari√¢ncia condicional. A correla√ß√£o entre os ativos permanece constante, conforme determinado pela matriz de covari√¢ncia incondicional.

**Teorema 1:** Em um modelo Scalar GARCH, se $\alpha_1 + \beta < 1$, ent√£o o processo GARCH escalar $h_t$ √© estacion√°rio e tem m√©dia incondicional finita.

*Estrat√©gia de Prova:*

1.  **Estacionariedade de GARCH(1,1):** Um processo GARCH(1,1) √© estacion√°rio se e somente se a soma dos par√¢metros GARCH, $\alpha_1$ e $\beta$, for menor que 1.
2.  **M√©dia Incondicional Finita:** Se o processo GARCH √© estacion√°rio, ent√£o sua m√©dia incondicional √© finita e pode ser expressa em termos dos par√¢metros do modelo.
3.  **Aplica√ß√£o ao Scalar GARCH:** Aplicar as condi√ß√µes de estacionariedade e m√©dia finita ao processo GARCH escalar no modelo Scalar GARCH.

*Prova:*

I. **Hip√≥tese:** Assumimos que $\alpha_1 + \beta < 1$.

II. **Estacionariedade:** A condi√ß√£o $\alpha_1 + \beta < 1$ garante que o processo GARCH escalar $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}' \epsilon_{t-1} + \beta h_{t-1}$ √© estacion√°rio.

III. **M√©dia Incondicional:** Para encontrar a m√©dia incondicional, tomamos o valor esperado de ambos os lados da equa√ß√£o:
$E[h_t] = E[\alpha_0 + \alpha_1 \epsilon_{t-1}' \epsilon_{t-1} + \beta h_{t-1}]$
$E[h_t] = \alpha_0 + \alpha_1 E[\epsilon_{t-1}' \epsilon_{t-1}] + \beta E[h_{t-1}]$
Assumindo que o processo √© estacion√°rio, $E[h_t] = E[h_{t-1}] = \bar{h}$, e $E[\epsilon_{t-1}' \epsilon_{t-1}] = E[\sum_{i=1}^N \epsilon_{i,t-1}^2] = \sum_{i=1}^N E[\epsilon_{i,t-1}^2] = \sum_{i=1}^N \sigma_i^2$, onde $\sigma_i^2$ √© a vari√¢ncia incondicional do ativo $i$.
Ent√£o, $\bar{h} = \alpha_0 + \alpha_1 \sum_{i=1}^N \sigma_i^2 + \beta \bar{h}$
$\bar{h} (1 - \beta) = \alpha_0 + \alpha_1 \sum_{i=1}^N \sigma_i^2$
$\bar{h} = \frac{\alpha_0 + \alpha_1 \sum_{i=1}^N \sigma_i^2}{1 - \beta}$

IV. **Conclus√£o:** Dado que $\alpha_1 + \beta < 1$, o processo GARCH escalar √© estacion√°rio e sua m√©dia incondicional $\bar{h} = \frac{\alpha_0 + \alpha_1 \sum_{i=1}^N \sigma_i^2}{1 - \beta}$ √© finita. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando os par√¢metros do exemplo anterior ($\alpha_0 = 0.0001, \alpha_1 = 0.1, \beta = 0.8$), verificamos a condi√ß√£o de estacionariedade:
>
> $\alpha_1 + \beta = 0.1 + 0.8 = 0.9 < 1$
>
> A condi√ß√£o de estacionariedade √© satisfeita. Agora, suponha que as vari√¢ncias incondicionais dos dois ativos sejam $\sigma_1^2 = 0.0025$ e $\sigma_2^2 = 0.0036$. Ent√£o, a m√©dia incondicional do processo GARCH escalar √©:
>
> $\bar{h} = \frac{0.0001 + 0.1 (0.0025 + 0.0036)}{1 - 0.8} = \frac{0.0001 + 0.1 (0.0061)}{0.2} = \frac{0.0001 + 0.00061}{0.2} = \frac{0.00071}{0.2} = 0.00355$
>
> Este valor representa o n√≠vel m√©dio em torno do qual o processo GARCH escalar ir√° flutuar ao longo do tempo.

### Garantindo a Positividade Definida da Matriz de Covari√¢ncia

Um dos principais desafios na modelagem GARCH multivariada √© garantir que a matriz de covari√¢ncia condicional resultante seja sempre positiva definida [^16]. A positividade definida √© necess√°ria para garantir que as vari√¢ncias sejam positivas e que as correla√ß√µes estejam dentro do intervalo v√°lido [-1, 1]. Se a matriz de covari√¢ncia n√£o for positiva definida, pode levar a resultados sem sentido, como volatilidades negativas ou pesos de portf√≥lio √≥timos que n√£o s√£o economicamente razo√°veis [^16].

> üí° **T√©cnicas para Garantir a Positividade Definida:**
>
> *   **Restri√ß√µes nos Par√¢metros:** Impor restri√ß√µes aos par√¢metros do modelo, como garantir que todos os par√¢metros de volatilidade sejam n√£o negativos e que a soma dos par√¢metros de persist√™ncia seja menor que 1 [^16].
> *   **Transforma√ß√µes de Cholesky:** Expressar a matriz de covari√¢ncia condicional em termos de uma decomposi√ß√£o de Cholesky, garantindo que a matriz resultante seja sempre positiva definida por constru√ß√£o [^16].
> *   **Processos de Recorte (Clipping):** Recortar os autovalores da matriz de covari√¢ncia condicional para garantir que sejam todos positivos [^16].

**Lema 1:** A matriz de covari√¢ncia deve ser positiva definida para que o modelo seja considerado v√°lido e para que as opera√ß√µes subsequentes, como a otimiza√ß√£o de portf√≥lio, sejam poss√≠veis.

*Prova:*

I. **Defini√ß√£o de Positividade Definida:** Uma matriz $H$ √© positiva definida se e somente se $x^T H x > 0$ para todo vetor n√£o nulo $x$.

II. **Implica√ß√µes para Vari√¢ncias:** Se $H$ √© uma matriz de covari√¢ncia, ent√£o os elementos diagonais de $H$ representam as vari√¢ncias dos ativos. Se $H$ n√£o √© positiva definida, ent√£o existe um vetor $x$ tal que $x^T H x \leq 0$, o que implica que a vari√¢ncia de uma combina√ß√£o linear dos ativos √© n√£o positiva, o que √© imposs√≠vel.

III. **Implica√ß√µes para Correla√ß√µes:** Se $H$ n√£o √© positiva definida, ent√£o as correla√ß√µes calculadas a partir de $H$ podem estar fora do intervalo $[-1, 1]$, o que tamb√©m √© imposs√≠vel.

IV. **Implica√ß√µes para Otimiza√ß√£o de Portf√≥lio:** A otimiza√ß√£o de portf√≥lio envolve a minimiza√ß√£o do risco do portf√≥lio, que √© uma fun√ß√£o da matriz de covari√¢ncia. Se a matriz de covari√¢ncia n√£o √© positiva definida, ent√£o o problema de otimiza√ß√£o pode n√£o ter uma solu√ß√£o bem definida, ou a solu√ß√£o pode n√£o ser economicamente razo√°vel.

*Exemplo Num√©rico:*
Suponha que temos uma matriz de covari√¢ncia:
$$
H = \begin{bmatrix}
1 & 2 \\
2 & 1
\end{bmatrix}
$$
Para verificar se √© positiva definida, calculamos os autovalores:
$\text{det}(H - \lambda I) = (1-\lambda)^2 - 4 = 0$
$\lambda^2 - 2\lambda - 3 = 0$
$\lambda = \frac{2 \pm \sqrt{4 + 12}}{2} = 1 \pm 2$
$\lambda_1 = 3$, $\lambda_2 = -1$
Como um dos autovalores √© negativo, a matriz n√£o √© positiva definida.  Isso significa que podemos encontrar um vetor $x$ tal que $x^T H x < 0$.  Por exemplo, se $x = [1, -1]^T$, ent√£o $x^T H x = [1, -1] \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = [1, -1] \begin{bmatrix} -1 \\ 1 \end{bmatrix} = -2 < 0$.
A conclus√£o √© que, sem a positividade definida, o modelo se torna inv√°lido [^16]. ‚ñ†

**Transforma√ß√µes de Cholesky:**
Em uma decomposi√ß√£o de Cholesky, a matriz de covari√¢ncia $H$ √© escrita como o produto de uma matriz triangular inferior $L$ e sua transposta $L^T$, de forma que $H = LL^T$. Esta decomposi√ß√£o garante que $H$ √© positiva definida, desde que os elementos diagonais de $L$ sejam positivos.

**Lema 2:** A Decomposi√ß√£o de Cholesky garante a condi√ß√£o de positiva definida para a matriz de covari√¢ncia.

*Prova:*

I. **Defini√ß√£o da Decomposi√ß√£o de Cholesky:** A matriz de covari√¢ncia $H$ √© decomposta em $H = LL^T$, onde $L$ √© uma matriz triangular inferior.

II. **Positividade Definida:** Seja $x$ um vetor n√£o nulo. Ent√£o, $x^T H x = x^T (LL^T) x = (L^T x)^T (L^T x) = ||L^T x||^2 \geq 0$, pois o resultado √© a norma ao quadrado de um vetor, que √© sempre n√£o negativo.

III. **Condi√ß√£o para Positividade Estrita:** Para garantir que $H$ seja estritamente positiva definida (e n√£o apenas semidefinida), √© necess√°rio que $L$ seja invert√≠vel, o que ocorre se e somente se todos os elementos diagonais de $L$ forem n√£o nulos.

IV. **Conclus√£o:** Portanto, a decomposi√ß√£o de Cholesky garante que $H$ √© positiva definida [^16]. ‚ñ†

Transforma√ß√µes de Cholesky podem ser usadaspara simular matrizes de covari√¢ncia, resolver sistemas de equa√ß√µes lineares e realizar outras opera√ß√µes em estat√≠stica e an√°lise num√©rica.

## Decomposi√ß√£o de Valor Singular (SVD)

A Decomosi√ß√£o de Valor Singular (SVD) √© uma t√©cnica de fatora√ß√£o de matrizes que decomp√µe uma matriz em tr√™s outras matrizes: uma matriz ortogonal, uma matriz diagonal e outra matriz ortogonal. A SVD √© uma ferramenta poderosa para reduzir a dimensionalidade de dados, remover ru√≠do e identificar padr√µes em dados.

### Defini√ß√£o Matem√°tica da SVD

Dada uma matriz $A$ de dimens√µes $m \times n$, a SVD decomp√µe $A$ da seguinte forma:

$$
A = U \Sigma V^T
$$

onde:
- $U$ √© uma matriz ortogonal $m \times m$ cujas colunas s√£o os vetores singulares esquerdos de $A$.
- $\Sigma$ √© uma matriz diagonal $m \times n$ com os valores singulares n√£o negativos de $A$ na diagonal principal.
- $V$ √© uma matriz ortogonal $n \times n$ cujas colunas s√£o os vetores singulares direitos de $A$.

### Aplica√ß√µes da SVD

1.  **Redu√ß√£o de Dimensionalidade:** A SVD pode ser usada para reduzir o n√∫mero de dimens√µes em um conjunto de dados, mantendo a maior parte da informa√ß√£o original. Isso √© feito selecionando os maiores valores singulares e seus correspondentes vetores singulares esquerdos e direitos.

2.  **Recomenda√ß√£o de Sistemas:** Em sistemas de recomenda√ß√£o, a SVD pode ser usada para prever as prefer√™ncias de um usu√°rio com base em suas avalia√ß√µes anteriores. A matriz de avalia√ß√µes √© decomposta usando SVD, e os valores singulares e vetores singulares s√£o usados para prever as avalia√ß√µes desconhecidas.

3.  **Processamento de Imagens:** A SVD pode ser usada para comprimir imagens, remover ru√≠do e extrair caracter√≠sticas importantes. A imagem √© representada como uma matriz, que √© ent√£o decomposta usando SVD. Os valores singulares menores podem ser descartados para reduzir o tamanho da imagem ou remover ru√≠do.

### Exemplo em Python com NumPy

```python
import numpy as np

# Exemplo de uma matriz
A = np.array([[1, 2], [3, 4], [5, 6]])

# Decomposi√ß√£o SVD
U, s, V = np.linalg.svd(A)

# Criar a matriz Sigma
Sigma = np.zeros((A.shape[0], A.shape[1]))
Sigma[:A.shape[1], :A.shape[1]] = np.diag(s)

# Imprimir as matrizes resultantes
print("Matriz U:\n", U)
print("Matriz Sigma:\n", Sigma)
print("Matriz V^T:\n", V)

# Reconstru√ß√£o da matriz original
B = U.dot(Sigma.dot(V))
print("Matriz Reconstru√≠da:\n", B)
```

### Teorema da Decomposi√ß√£o de Valor Singular

**Teorema:** Para qualquer matriz $A_{m \times n}$, existem matrizes ortogonais $U_{m \times m}$ e $V_{n \times n}$ tais que $A = U\Sigma V^T$, onde $\Sigma_{m \times n}$ √© uma matriz diagonal com entradas n√£o negativas na diagonal principal.

*Demonstra√ß√£o:*

A demonstra√ß√£o deste teorema envolve conceitos avan√ßados de √°lgebra linear e n√£o ser√° detalhada aqui. No entanto, a exist√™ncia da decomposi√ß√£o √© garantida pelas propriedades espectrais de $A^TA$ e $AA^T$.

### Diagrama da SVD

```mermaid
graph LR
    A((A)) -->|U| U((U))
    A -->|Œ£| Sigma((Œ£))
    A -->|V^T| VT(((V^T)))
    U -->|Multiplica√ß√£o| USV((UŒ£V^T))
    Sigma -->|Multiplica√ß√£o| USV
    VT -->|Multiplica√ß√£o| USV
```

## Norma de Matriz

A norma de uma matriz √© uma medida do "tamanho" da matriz. Existem v√°rias formas de definir a norma de uma matriz, cada uma com suas pr√≥prias propriedades e aplica√ß√µes.

### Defini√ß√µes de Normas de Matriz

1.  **Norma de Frobenius:** A norma de Frobenius de uma matriz $A$ √© definida como a raiz quadrada da soma dos quadrados de todos os seus elementos:

    $$
    ||A||_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}
    $$

2.  **Norma Espectral (ou Norma 2):** A norma espectral de uma matriz $A$ √© definida como o maior valor singular de $A$:

    $$
    ||A||_2 = \sigma_{max}(A)
    $$

    onde $\sigma_{max}(A)$ √© o maior valor singular de $A$.

3.  **Norma Nuclear:** A norma nuclear de uma matriz $A$ √© definida como a soma de seus valores singulares:

    $$
    ||A||_* = \sum_{i=1}^{min(m, n)} \sigma_i(A)
    $$

    onde $\sigma_i(A)$ s√£o os valores singulares de $A$.

### Propriedades das Normas de Matriz

*   N√£o negatividade: $||A|| \geq 0$ para todas as matrizes $A$, e $||A|| = 0$ se e somente se $A = 0$.
*   Homogeneidade: $||\alpha A|| = |\alpha| \cdot ||A||$ para todos os escalares $\alpha$ e matrizes $A$.
*   Desigualdade triangular: $||A + B|| \leq ||A|| + ||B||$ para todas as matrizes $A$ e $B$.

### Exemplo em Python com NumPy

```python
import numpy as np

# Exemplo de uma matriz
A = np.array([[1, 2], [3, 4]])

# Norma de Frobenius
norm_frobenius = np.linalg.norm(A, 'fro')
print("Norma de Frobenius:", norm_frobenius)

# Norma Espectral (Norma 2)
norm_spectral = np.linalg.norm(A, 2)
print("Norma Espectral:", norm_spectral)

# Norma Nuclear (requer c√°lculo dos valores singulares)
U, s, V = np.linalg.svd(A)
norm_nuclear = np.sum(s)
print("Norma Nuclear:", norm_nuclear)
```

### Aplica√ß√µes das Normas de Matriz

1.  **An√°lise de Estabilidade:** As normas de matrizes s√£o usadas para analisar a estabilidade de sistemas lineares. A norma de uma matriz de sistema pode indicar se o sistema √© est√°vel ou inst√°vel.

2.  **Regulariza√ß√£o:** Em problemas de otimiza√ß√£o, as normas de matrizes s√£o usadas para regularizar solu√ß√µes, promovendo solu√ß√µes com certas propriedades desej√°veis (por exemplo, esparsidade).

3.  **Condicionamento de Matrizes:** A norma de uma matriz e sua inversa podem ser usadas para determinar o condicionamento da matriz, indicando a sensibilidade da solu√ß√£o de um sistema linear a pequenas mudan√ßas nos dados de entrada.

### Teorema sobre Normas de Matriz

**Teorema:** Para qualquer norma de matriz induzida, $||AB|| \leq ||A|| \cdot ||B||$.

*Demonstra√ß√£o:*

Este teorema √© uma consequ√™ncia direta da defini√ß√£o de normas de matriz induzidas e da desigualdade triangular.

## Autovalores e Autovetores

Autovalores e autovetores s√£o conceitos fundamentais na √°lgebra linear, com aplica√ß√µes em diversas √°reas como f√≠sica, engenharia e ci√™ncia da computa√ß√£o.

### Defini√ß√µes

Dado uma matriz quadrada $A$ de tamanho $n \times n$, um autovetor de $A$ √© um vetor n√£o nulo $v$ tal que, quando $A$ √© multiplicado por $v$, o resultado √© um m√∫ltiplo escalar de $v$. Esse escalar √© chamado de autovalor de $A$ associado a $v$. Matematicamente, isso √© expresso como:

$$
Av = \lambda v
$$

onde:
- $A$ √© uma matriz quadrada $n \times n$.
- $v$ √© um autovetor de $A$.
- $\lambda$ √© um autovalor de $A$ associado a $v$.

### C√°lculo de Autovalores e Autovetores

Para encontrar os autovalores de uma matriz $A$, resolvemos a equa√ß√£o caracter√≠stica:

$$
\det(A - \lambda I) = 0
$$

onde:
- $\det$ denota o determinante.
- $A$ √© a matriz dada.
- $\lambda$ √© o autovalor.
- $I$ √© a matriz identidade de tamanho $n \times n$.

As solu√ß√µes para $\lambda$ dessa equa√ß√£o s√£o os autovalores de $A$. Para cada autovalor $\lambda$, o autovetor correspondente $v$ √© encontrado resolvendo o sistema de equa√ß√µes lineares:

$$
(A - \lambda I)v = 0
$$

### Exemplo em Python com NumPy

```python
import numpy as np

# Exemplo de uma matriz quadrada
A = np.array([[4, 2], [1, 3]])

# C√°lculo dos autovalores e autovetores
eigenvalues, eigenvectors = np.linalg.eig(A)

# Imprimir os autovalores e autovetores
print("Autovalores:", eigenvalues)
print("Autovetores:\n", eigenvectors)
```

### Propriedades dos Autovalores e Autovetores

1.  **Autovetores Linearly Independentes:** Se uma matriz $A$ tem $n$ autovalores distintos, ent√£o os correspondentes autovetores s√£o linearmente independentes.

2.  **Decomposi√ß√£o Espectral:** Se uma matriz $A$ √© sim√©trica, ent√£o seus autovetores s√£o ortogonais e podem ser usados para diagonalizar $A$. Isso significa que existe uma matriz ortogonal $Q$ e uma matriz diagonal $\Lambda$ tal que $A = Q\Lambda Q^T$, onde os elementos diagonais de $\Lambda$ s√£o os autovalores de $A$.

3.  **Tra√ßo e Determinante:** A soma dos autovalores de uma matriz √© igual ao seu tra√ßo (a soma dos elementos na diagonal principal), e o produto dos autovalores √© igual ao seu determinante.

### Aplica√ß√µes dos Autovalores e Autovetores

1.  **An√°lise de Componentes Principais (PCA):** Os autovetores da matriz de covari√¢ncia dos dados s√£o usados como as componentes principais, que representam as dire√ß√µes de m√°xima vari√¢ncia nos dados.

2.  **Estabilidade de Sistemas Din√¢micos:** Os autovalores de uma matriz que descreve um sistema din√¢mico podem indicar a estabilidade do sistema. Se todos os autovalores t√™m parte real negativa, o sistema √© est√°vel.

3.  **Mec√¢nica Qu√¢ntica:** Em mec√¢nica qu√¢ntica, os autovalores de um operador representam os poss√≠veis resultados de uma medi√ß√£o, e os autovetores representam os estados correspondentes.

### Teorema Espectral

**Teorema:** Uma matriz $A$ de tamanho $n \times n$ √© diagonaliz√°vel se e somente se ela tem $n$ autovetores linearmente independentes.

*Demonstra√ß√£o:*

A demonstra√ß√£o deste teorema envolve a constru√ß√£o de uma matriz de transforma√ß√£o $P$ cujas colunas s√£o os autovetores linearmente independentes de $A$. Ent√£o, $P^{-1}AP$ √© uma matriz diagonal com os autovalores de $A$ na diagonal principal.

### Diagrama de Autovalores e Autovetores

```mermaid
graph LR
    A((Matriz A)) -->|Multiplica√ß√£o| Resultado
    v((Autovetor v)) -->|Multiplica√ß√£o| Resultado
    Resultado((Œªv))
    A -->|Œª| Autovalor((Autovalor Œª))
    Autovalor --> Resultado
```

## Diagonaliza√ß√£o de Matrizes

A diagonaliza√ß√£o de uma matriz √© um processo que transforma uma matriz quadrada em uma matriz diagonal atrav√©s de uma transforma√ß√£o de similaridade. Este processo √© √∫til para simplificar c√°lculos e entender propriedades da matriz original.

### Defini√ß√£o

Uma matriz quadrada $A$ de tamanho $n \times n$ √© dita diagonaliz√°vel se existe uma matriz invert√≠vel $P$ e uma matriz diagonal $D$ tal que:

$$
A = PDP^{-1}
$$

onde:
- $A$ √© a matriz original.
- $P$ √© a matriz de autovetores de $A$.
- $D$ √© a matriz diagonal cujos elementos diagonais s√£o os autovalores de $A$.

### Processo de Diagonaliza√ß√£o

1.  **Encontrar os Autovalores:** Calcule os autovalores $\lambda_i$ da matriz $A$ resolvendo a equa√ß√£o caracter√≠stica $\det(A - \lambda I) = 0$.

2.  **Encontrar os Autovetores:** Para cada autovalor $\lambda_i$, encontre o autovetor correspondente $v_i$ resolvendo o sistema de equa√ß√µes $(A - \lambda_i I)v_i = 0$.

3.  **Construir a Matriz P:** Forme a matriz $P$ usando os autovetores como colunas.

4.  **Construir a Matriz D:** Forme a matriz diagonal $D$ com os autovalores na diagonal principal.

5.  **Verificar:** Confirme se $A = PDP^{-1}$.

### Exemplo em Python com NumPy

```python
import numpy as np

# Exemplo de uma matriz quadrada
A = np.array([[4, 2], [3, 5]])

# C√°lculo dos autovalores e autovetores
eigenvalues, eigenvectors = np.linalg.eig(A)

# Constru√ß√£o da matriz P (matriz de autovetores)
P = eigenvectors

# Constru√ß√£o da matriz D (matriz diagonal de autovalores)
D = np.diag(eigenvalues)

# Inversa da matriz P
P_inv = np.linalg.inv(P)

# Diagonaliza√ß√£o: A = PDP^{-1}
A_reconstructed = P @ D @ P_inv

# Imprimir as matrizes
print("Matriz A:\n", A)
print("Matriz P:\n", P)
print("Matriz D:\n", D)
print("Matriz P^-1:\n", P_inv)
print("Matriz Reconstru√≠da:\n", A_reconstructed)
```

### Condi√ß√µes para Diagonaliza√ß√£o

Uma matriz $A$ √© diagonaliz√°vel se e somente se:

1.  $A$ tem $n$ autovetores linearmente independentes.

2.  Para cada autovalor $\lambda$ de $A$, a multiplicidade geom√©trica de $\lambda$ (a dimens√£o do autoespa√ßo associado a $\lambda$) √© igual √† sua multiplicidade alg√©brica (a multiplicidade de $\lambda$ como raiz da equa√ß√£o caracter√≠stica).

### Aplica√ß√µes da Diagonaliza√ß√£o

1.  **C√°lculo de Pot√™ncias de Matrizes:** Se $A = PDP^{-1}$, ent√£o $A^k = PD^kP^{-1}$. Isso simplifica o c√°lculo de pot√™ncias de matrizes, pois $D^k$ √© simplesmente a matriz diagonal com os elementos diagonais elevados √† pot√™ncia $k$.

2.  **Resolu√ß√£o de Sistemas de Equa√ß√µes Diferenciais Lineares:** A diagonaliza√ß√£o √© usada para desacoplar sistemas de equa√ß√µes diferenciais lineares, tornando a solu√ß√£o mais f√°cil de encontrar.

3.  **An√°lise de Estabilidade:** A diagonaliza√ß√£o pode ser usada para analisar a estabilidade de sistemas lineares, determinando se os autovalores da matriz do sistema t√™m parte real negativa.

### Teorema da Diagonaliza√ß√£o

**Teorema:** Uma matriz $A$ de tamanho $n \times n$ √© diagonaliz√°vel se e somente se existe uma base de $\mathbb{R}^n$ consistindo de autovetores de $A$.

*Demonstra√ß√£o:*

A demonstra√ß√£o deste teorema segue diretamente da defini√ß√£o de diagonaliza√ß√£o e das propriedades dos autovetores e autovalores.

### Diagrama de Diagonaliza√ß√£o

```mermaid
graph LR
    A((Matriz A)) -->|PDP^{-1}| Diagonaliza√ß√£o
    P((Matriz P))
    D((Matriz Diagonal D))
    P_inv((Matriz P^{-1}))
    Diagonaliza√ß√£o --> P
    Diagonaliza√ß√£o --> D
    Diagonaliza√ß√£o --> P_inv
```

## Formas Quadr√°ticas

As formas quadr√°ticas s√£o fun√ß√µes que mapeiam vetores em escalares, definidas por uma express√£o quadr√°tica envolvendo uma matriz sim√©trica. Elas s√£o importantes em diversas √°reas, como otimiza√ß√£o, estat√≠stica e f√≠sica.

### Defini√ß√£o

Uma forma quadr√°tica √© uma fun√ß√£o $Q: \mathbb{R}^n \rightarrow \mathbb{R}$ definida como:

$$
Q(x) = x^T A x
$$

onde:
- $x$ √© um vetor em $\mathbb{R}^n$.
- $A$ √© uma matriz sim√©trica $n \times n$.

### Matriz Associada a uma Forma Quadr√°tica

Dada uma forma quadr√°tica $Q(x)$, a matriz $A$ tal que $Q(x) = x^T A x$ √© chamada de matriz associada √† forma quadr√°tica. √â importante que $A$ seja sim√©trica, pois isso garante que a forma quadr√°tica seja unicamente determinada por $A$.

### Exemplo

Seja $x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$ e $A = \begin{bmatrix} a & b \\ b & c \end{bmatrix}$. Ent√£o a forma quadr√°tica associada a $A$ √©:

$$
Q(x) = x^T A x = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} a & b \\ b & c \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = ax_1^2 + 2bx_1x_2 + cx_2^2
$$

### Classifica√ß√£o das Formas Quadr√°ticas

As formas quadr√°ticas s√£o classificadas com base nos autovalores da matriz associada $A$:

1.  **Definida Positiva:** $Q(x) > 0$ para todo $x \neq 0$. Todos os autovalores de $A$ s√£o positivos.

2.  **Definida Negativa:** $Q(x) < 0$ para todo $x \neq 0$. Todos os autovalores de $A$ s√£o negativos.

3.  **Semidefinida Positiva:** $Q(x) \geq 0$ para todo $x$. Todos os autovalores de $A$ s√£o n√£o negativos.

4.  **Semidefinida Negativa:** $Q(x) \leq 0$ para todo $x$. Todos os autovalores de $A$ s√£o n√£o positivos.

5.  **Indefinida:** $Q(x)$ pode ser positiva ou negativa para diferentes valores de $x$. A tem autovalores positivos e negativos.

### Exemplo em Python com NumPy

```python
import numpy as np

# Matriz sim√©trica
A = np.array([[2, 1], [1, 3]])

# Fun√ß√£o para calcular a forma quadr√°tica
def quadratic_form(x, A):
    return x.T @ A @ x

# Vetor x
x = np.array([1, 2])

# Calcular a forma quadr√°tica
result = quadratic_form(x, A)
print("Forma Quadr√°tica:", result)

# Calcular os autovalores de A
eigenvalues = np.linalg.eigvalsh(A)
print("Autovalores de A:", eigenvalues)

# Classifica√ß√£o da forma quadr√°tica
if np.all(eigenvalues > 0):
    print("A forma quadr√°tica √© definida positiva.")
elif np.all(eigenvalues < 0):
    print("A forma quadr√°tica √© definida negativa.")
elif np.all(eigenvalues >= 0):
    print("A forma quadr√°tica √© semidefinida positiva.")
elif np.all(eigenvalues <= 0):
    print("A forma quadr√°tica √© semidefinida negativa.")
else:
    print("A forma quadr√°tica √© indefinida.")
```

### Teorema Espectral para Formas Quadr√°ticas

**Teorema:** Seja $A$ uma matriz sim√©trica $n \times n$. Ent√£o existe uma matriz ortogonal $P$ tal que $A = PDP^T$, onde $D$ √© uma matriz diagonal com os autovalores de $A$ na diagonal principal. A forma quadr√°tica $Q(x) = x^T A x$ pode ser transformada em uma forma quadr√°tica diagonal $Q(y) = y^T D y$ por meio da mudan√ßa de vari√°veis $x = Py$.

*Demonstra√ß√£o:*

A demonstra√ß√£o deste teorema utiliza o teorema espectral e a ortogonalidade dos autovetores de uma matriz sim√©trica.

### Aplica√ß√µes das Formas Quadr√°ticas

1.  **Otimiza√ß√£o:** As formas quadr√°ticas s√£o usadas para modelar fun√ß√µes objetivo em problemas de otimiza√ß√£o, como programa√ß√£o quadr√°tica.

2.  **Estat√≠stica:** As formas quadr√°ticas aparecem na an√°lise de vari√¢ncia e na estima√ß√£o de par√¢metros em modelos estat√≠sticos.

3.  **Geometria:** As formas quadr√°ticas s√£o usadas para descrever superf√≠cies quadr√°ticas, como elips√≥ides, hiperbol√≥ides e parabol√≥ides.

4.  **F√≠sica:** As formas quadr√°ticas s√£o usadas para descrever a energia potencial em sistemas f√≠sicos, como osciladores harm√¥nicos.

### Diagrama de Forma Quadr√°tica

```mermaid
graph LR
    x((Vetor x)) -->|x^T| xT((x^T))
    A((Matriz A))
    xT -->|A| Ax((x^T A))
    Ax -->|x| Q((Q(x) = x^T A x))
```
<!-- END -->