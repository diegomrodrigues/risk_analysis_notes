### Forecasting Risk and Correlations: Modeling Volatility Clustering and Structural Breaks

### Introdução
Building upon the previous discussion on modeling time-varying risk using moving averages and GARCH models [^1, 4, 5], this section will delve into the empirical phenomenon of **volatility clustering** in financial markets and the impact of **structural breaks** on volatility dynamics. We'll examine how periods of high volatility tend to cluster together, suggesting predictable patterns that can be exploited for risk management and trading strategies [^1]. Additionally, we'll explore how structural market events, such as exchange rate changes and monetary policy shifts, influence the average daily volatility and necessitate the use of models that can adapt to these changes [^1, 3]. Understanding these concepts is crucial for refining our risk forecasting methodologies and making more informed decisions in volatile market conditions.

### Conceitos Fundamentais

#### 9.2.5 Volatility Clustering
Volatility clustering is a well-documented phenomenon in financial markets [^1]. It refers to the tendency for periods of high volatility to be followed by periods of high volatility, and periods of low volatility to be followed by periods of low volatility [^1]. This means that volatility is not randomly distributed over time, but rather exhibits a degree of predictability [^1].

**Definition:**
Volatility clustering can be formally defined as the positive autocorrelation of volatility over time. In other words, the volatility at time *t* is positively correlated with the volatility at time *t*-1, *t*-2, and so on.

> 💡 **Exemplo Numérico:** To see how the **Autocorrelation Function (ACF)** can be used to identify volatility clustering, let's compute and visualize the ACF of squared returns for a simulated series exhibiting clustering.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Function to simulate GARCH process
> def simulate_garch(n, alpha0, alpha1, beta):
>     np.random.seed(42)
>     returns = np.zeros(n)
>     sigma2 = np.zeros(n)
>     sigma2[0] = alpha0 / (1 - alpha1 - beta)  # Initial variance
>
>     for t in range(1, n):
>         returns[t] = np.random.normal(0, np.sqrt(sigma2[t-1]))
>         sigma2[t] = alpha0 + alpha1 * returns[t-1]**2 + beta * sigma2[t-1]
>     return returns
>
> # Simulate GARCH(1,1) process with volatility clustering
> n = 500
> alpha0 = 0.00001
> alpha1 = 0.1
> beta = 0.8
> returns = simulate_garch(n, alpha0, alpha1, beta)
>
> # Calculate squared returns as proxy for volatility
> squared_returns = returns**2
>
> # Plot ACF of squared returns
> plt.figure(figsize=(10, 5))
> plot_acf(squared_returns, lags=40, ax=plt.gca())
> plt.title('Autocorrelation Function of Squared Returns')
> plt.xlabel('Lags')
> plt.ylabel('Autocorrelation')
> plt.show()
> ```
>
> In the plot above, observe that the autocorrelation coefficients are significantly greater than zero for several lags, indicating the presence of volatility clustering in the simulated time series. If volatility was randomly distributed, we would expect the ACF coefficients to be close to zero, after lag zero.

To further illustrate the concept of volatility clustering, we can examine some of its statistical properties.

**Proposição 1:** *For a time series exhibiting volatility clustering, the sample variance of returns calculated over a long period will typically be greater than the average of the conditional variances estimated over the same period.*

*Proof (Sketch):* Volatility clustering implies that periods of high volatility are followed by periods of high volatility. Therefore, the unconditional variance, which averages over all periods, will be inflated by these periods of high volatility compared to the average of the conditional variances, which adapt to the current volatility level.

*Proof:*
Let $r_t$ be the return at time $t$, and $h_t$ be the conditional variance at time $t$. The unconditional variance $\sigma^2$ is estimated as:

$$\sigma^2 = \frac{1}{T} \sum_{t=1}^{T} r_t^2$$

where $T$ is the total number of observations.  We assume that $E[r_t] = 0$ for simplicity.

The average of the conditional variances is:

$$\bar{h} = \frac{1}{T} \sum_{t=1}^{T} h_t$$

Volatility clustering means that the conditional variance $h_t$ exhibits positive autocorrelation. Consequently, periods of high $h_t$ tend to cluster together. This implies that the unconditional variance $\sigma^2$ will capture these periods of high volatility, resulting in a higher overall estimate compared to the average of the conditional variances $\bar{h}$, which adapt to the changing volatility levels. Thus, $\sigma^2 > \bar{h}$.

> 💡 **Exemplo Numérico:** Suppose we have the following daily returns (in percentage) for a stock over 10 days: `[0.1, 0.2, -0.15, 0.05, -0.02, 1.5, -1.2, 0.8, -0.9, 1.1]`. Let's also assume we fit a GARCH(1,1) model to these returns and obtain the following conditional variances: `[0.01, 0.015, 0.012, 0.009, 0.008, 0.025, 0.02, 0.018, 0.016, 0.022]`.
>
> 1.  Calculate the sample variance of returns:
>
> $$\sigma^2 = \frac{1}{10} \sum_{t=1}^{10} r_t^2 = \frac{1}{10} (0.1^2 + 0.2^2 + \ldots + 1.1^2) = \frac{6.1125}{10} = 0.61125$$
>
> 2.  Calculate the average of the conditional variances:
>
> $$\bar{h} = \frac{1}{10} \sum_{t=1}^{10} h_t = \frac{1}{10} (0.01 + 0.015 + \ldots + 0.022) = \frac{0.155}{10} = 0.0155$$
>
> In this case, $\sigma^2 = 0.61125 > \bar{h} = 0.0155$, which supports Proposição 1. The high volatility days (e.g., returns of 1.5, -1.2, 0.8, -0.9, 1.1) significantly inflate the sample variance compared to the average of the conditional variances, which are more adaptive to recent volatility levels.

#### 9.2.6 Structural Breaks
Structural breaks are significant, often unanticipated, shifts in the underlying parameters or dynamics of a time series [^1]. In financial markets, these breaks can be caused by a variety of factors, including:

*   Changes in monetary policy [^1]
*   Changes in exchange rate regimes [^1]
*   Financial crises
*   Regulatory changes
*   Geopolitical events

**Impact on Volatility:**
Structural breaks can have a profound impact on the level and persistence of volatility [^1]. For example, the shift from a fixed exchange rate regime to a floating exchange rate regime can lead to a sustained increase in volatility [^1]. Similarly, a sudden change in monetary policy can trigger a period of increased market uncertainty and volatility [^1].

> 💡 **Exemplo Numérico:** Consider the \$/BP exchange rate analysis mentioned earlier [^2, 3]. The pound's exit from the European Monetary System (EMS) in September 1992 was a structural break that led to a significant increase in the volatility of the \$/BP exchange rate [^2]. Models that did not account for this structural break would have underestimated the risk associated with the \$/BP exchange rate following the event [^2].
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simulate returns with a structural break
> np.random.seed(42)
> n = 200
> returns = np.random.normal(0, 0.01, n) # Initial low volatility period
> returns[100:] = np.random.normal(0, 0.03, n-100) # High volatility after break
>
> # Calculate rolling volatility (standard deviation)
> window_size = 20
> rolling_volatility = np.zeros(n)
> for t in range(window_size, n):
>     rolling_volatility[t] = np.std(returns[t-window_size:t])
>
> # Plot the returns and rolling volatility
> fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)
>
> axs[0].plot(returns)
> axs[0].axvline(x=100, color='r', linestyle='--', label='Structural Break')
> axs[0].set_title('Simulated Returns with Structural Break')
> axs[0].set_ylabel('Returns')
> axs[0].legend()
>
> axs[1].plot(rolling_volatility)
> axs[1].axvline(x=100, color='r', linestyle='--', label='Structural Break')
> axs[1].set_title('Rolling Volatility (Window Size = 20)')
> axs[1].set_ylabel('Volatility')
> axs[1].legend()
>
> plt.xlabel('Time')
> plt.tight_layout()
> plt.show()
> ```
>
> Visual inspection of the plots would show a clear increase in volatility around the structural break. A rolling window measure of volatility (such as the exponentially weighted moving average discussed in section 9.2.4 of the text) would clearly highlight this change. This is because the shock was introduced after the EMA, which would increase the estimate. Before the break the volatility is smoother.
>
> **Statistical Tests for Structural Breaks:**
>
> *   **Chow Test:** Tests whether there is a significant difference in the regression coefficients before and after a specific point in time. The Chow test statistic follows an F-distribution.
> *   **Quandt-Andrews Test:** Tests for a structural break at an unknown point in time. It calculates the Chow test statistic for every possible break point and uses the maximum of these statistics.

**Lemma 1:** *The Chow test statistic is sensitive to the choice of the break point.*

*Proof (Sketch):* The Chow test compares the sum of squared residuals from a single regression model spanning the entire period with the sum of squared residuals from two separate regression models, one for each sub-period defined by the break point. If the chosen break point does not correspond to a true structural break, the difference in the sum of squared residuals will be small, resulting in a low Chow test statistic and a failure to reject the null hypothesis of no structural break.

Following the discussion on statistical tests, it is worthwhile noting the assumptions underlying these tests.

**Observation:** The Chow test assumes homoscedasticity within each sub-period, meaning that the variance of the error term is constant within each period before and after the break. If this assumption is violated, the Chow test may produce misleading results. The Quandt-Andrews test is more robust to this issue as it searches for the optimal break point.

> 💡 **Exemplo Numérico:** Let's illustrate the Chow Test with a simplified linear regression model. Suppose we have a time series of stock prices ($y_t$) and we want to determine if a structural break occurred at time $t=50$ due to a change in market regulation. We model the stock prices as a linear function of time ($t$): $y_t = \beta_0 + \beta_1 t + \epsilon_t$, where $\epsilon_t$ is the error term.
>
> 1.  **Estimate the model over the entire period (1 to 100):** We obtain the estimated coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$ and calculate the residual sum of squares (RSS), denoted as $RSS_{full}$.
>
> 2.  **Estimate the model over two sub-periods:**
>     *   Period 1: $t = 1$ to $49$. We obtain the estimated coefficients $\hat{\beta}_{0,1}$ and $\hat{\beta}_{1,1}$ and calculate the residual sum of squares $RSS_1$.
>     *   Period 2: $t = 50$ to $100$. We obtain the estimated coefficients $\hat{\beta}_{0,2}$ and $\hat{\beta}_{1,2}$ and calculate the residual sum of squares $RSS_2$.
>
> 3.  **Calculate the Chow test statistic:**
>
> $$F = \frac{(RSS_{full} - (RSS_1 + RSS_2))/k}{(RSS_1 + RSS_2)/(n - 2k)}$$
>
>     where:
>     *   $n$ is the total number of observations (100).
>     *   $k$ is the number of parameters in each regression model (2: $\beta_0$ and $\beta_1$).
>
> Suppose we obtain the following values:
>
> *   $RSS_{full} = 150$
> *   $RSS_1 = 50$
> *   $RSS_2 = 60$
>
> Then, the Chow test statistic is:
>
> $$F = \frac{(150 - (50 + 60))/2}{(50 + 60)/(100 - 2*2)} = \frac{20/2}{110/96} = \frac{10}{1.1458} \approx 8.73$$
>
> 4.  **Compare the F-statistic with the critical value:** We compare the calculated F-statistic (8.73) with the critical value from an F-distribution with $(k, n-2k)$ degrees of freedom, i.e., $(2, 96)$. If we choose a significance level of $\alpha = 0.05$, the critical value is approximately 3.09. Since $8.73 > 3.09$, we reject the null hypothesis of no structural break at $t=50$. This suggests that there is a statistically significant difference in the regression coefficients before and after time $t=50$, indicating a structural break.
>
> This numerical example illustrates how the Chow test can be used to detect structural breaks in a time series by comparing the fit of a regression model over different sub-periods.

#### 9.2.7 Modeling Volatility Clustering and Structural Breaks

Several approaches can be used to model volatility clustering and structural breaks:

1.  **GARCH Models:** As discussed in the previous section, GARCH models are naturally suited to capture volatility clustering [^5]. The autoregressive component in the conditional variance equation allows the model to "remember" past volatility levels and adjust the current volatility forecast accordingly [^5].

2.  **GARCH with Exogenous Variables:** GARCH models can be extended to include exogenous variables that capture the impact of structural breaks. For example, a dummy variable can be included in the conditional variance equation to represent the occurrence of a specific structural break.

3.  **Regime-Switching Models:** These models assume that the market operates in different "regimes," each characterized by a different level of volatility and correlation. The model then switches between these regimes based on some underlying process.

4.  **Stochastic Volatility Models:** These models treat volatility as a latent variable that follows its own stochastic process. This allows for more flexibility in capturing the dynamics of volatility.

5.  **Break Point Detection Algorithms:** Use change point detection statistical algorithms to detect structural breaks in real time. These algorithms continuously monitor the data stream for change points that violate the data generation process.

**Theorem 4:** *GARCH models, due to their autoregressive nature, are capable of capturing volatility clustering, but may not fully account for the instantaneous impact of structural breaks.*

*Proof (Sketch):* The GARCH(1,1) model equation is $h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}$. The $\beta h_{t-1}$ component allows volatility to persist over time, thus capturing clustering. However, if a structural break causes a sudden, large change in volatility that is not reflected in the previous period's return ($r_{t-1}$) or variance ($h_{t-1}$), the model may not immediately capture the full extent of the change. The parameter $\alpha_1$ controls the instantaneous impact, but if the shock is unusually large, it can take several periods for the GARCH model to fully adjust.

*Proof:*

I.   The GARCH(1,1) model is given by:
$$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}$$
II.  Volatility clustering occurs because the conditional variance $h_t$ depends on past returns $r_{t-1}^2$ and past conditional variance $h_{t-1}$. The term $\beta h_{t-1}$ captures the persistence of volatility.
III.  However, structural breaks can cause abrupt and significant changes in the volatility level. The GARCH(1,1) model can only respond to these changes through the term $\alpha_1 r_{t-1}^2$, which depends on the squared return in the previous period.
IV. If a structural break causes a sudden increase in volatility that is not reflected in $r_{t-1}^2$, the GARCH model will not immediately capture the full extent of the change in volatility. It will take several periods for the model to fully adjust to the new volatility level.
V.  Therefore, while GARCH models are effective at capturing volatility clustering due to their autoregressive nature, they may not fully account for the instantaneous impact of structural breaks. A more flexible model, such as a regime-switching model or a GARCH model with exogenous variables, may be needed to fully capture the impact of structural breaks on volatility. ■

To extend on Theorem 4, we can formulate a corollary that considers the use of exogenous variables within a GARCH framework to improve the model's response to structural breaks.

**Corolário 4.1:** *A GARCH model augmented with an exogenous dummy variable representing a structural break will exhibit an improved response to the break compared to a standard GARCH model, provided the dummy variable is appropriately specified and statistically significant.*

*Proof (Sketch):* By including a dummy variable that takes the value of 1 after the structural break and 0 before, the conditional variance equation can directly incorporate the impact of the break. This allows the model to adjust more rapidly to the new volatility regime.

*Proof:*
I.  The GARCH(1,1) model with an exogenous dummy variable is given by:

$$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1} + \gamma D_t$$

where $D_t$ is a dummy variable that equals 1 after the structural break and 0 before, and $\gamma$ is the coefficient capturing the impact of the break on the conditional variance.

II. If a structural break occurs at time $t^*$, then $D_t = 0$ for $t < t^*$ and $D_t = 1$ for $t \geq t^*$.

III. The inclusion of the term $\gamma D_t$ allows the conditional variance $h_t$ to immediately adjust to the new volatility level after the break. The magnitude and sign of $\gamma$ determine the size and direction of this adjustment.

IV. Assuming that the dummy variable is appropriately specified (i.e., it correctly identifies the timing of the break) and that the coefficient $\gamma$ is statistically significant (i.e., the break has a significant impact on volatility), the GARCH model with the dummy variable will exhibit an improved response to the break compared to a standard GARCH model that does not account for the break. The variance of the shock will be better predicted. ■

> 💡 **Exemplo Numérico:** To illustrate the benefit of adding an exogenous variable, let’s assume we have returns data and suspect a structural break occurred at t=100 due to a major policy announcement. We can compare a standard GARCH(1,1) model with a GARCH(1,1) model including a dummy variable $D_t$ that equals 1 if $t \geq 100$ and 0 otherwise.
>
> Suppose, after fitting both models to the data (using, for instance, maximum likelihood estimation), we obtain the following parameter estimates (hypothetical):
>
> **Model 1: Standard GARCH(1,1)**
>
> $$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}$$
>
> *   $\hat{\alpha}_0 = 0.00001$
> *   $\hat{\alpha}_1 = 0.1$
> *   $\hat{\beta} = 0.8$
>
> **Model 2: GARCH(1,1) with Dummy Variable**
>
> $$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1} + \gamma D_t$$
>
> *   $\hat{\alpha}_0 = 0.000005$
> *   $\hat{\alpha}_1 = 0.08$
> *   $\hat{\beta} = 0.75$
> *   $\hat{\gamma} = 0.0005$
>
> Let's compare how these models predict the conditional variance one step ahead at $t=100$, assuming $r_{99} = 0.02$ and $h_{99} = 0.0004$:
>
> **Model 1:**
>
> $$h_{100} = 0.00001 + 0.1 \times (0.02)^2 + 0.8 \times 0.0004 = 0.00001 + 0.00004 + 0.00032 = 0.00037$$
>
> **Model 2:**
>
> $$h_{100} = 0.000005 + 0.08 \times (0.02)^2 + 0.75 \times 0.0004 + 0.0005 \times 1 = 0.000005 + 0.000032 + 0.0003 + 0.0005 = 0.000837$$
>
> Notice that Model 2 (with the dummy variable) predicts a much higher conditional variance at $t=100$ (0.000837) compared to Model 1 (0.00037). This is because the dummy variable captures the immediate impact of the structural break by adding $\hat{\gamma}$ to the variance. By including $D_t$, this leads to better risk management practices because it leads to a larger capital charge (or other risk-reduction measures). Also, this assumes $D_t$ is appropriately specified: that there is indeed a structural break (e.g., hypothesis testing confirms it).
>
> This demonstrates that the GARCH model with the dummy variable reacts more strongly to the structural break, providing a more accurate estimate of the increased volatility. The inclusion of an exogenous variable helps to improve the model’s ability to capture and respond to structural breaks.

> 💡 **Exemplo Numérico:** To illustrate how a GARCH model responds to a structural break, let's simulate a scenario where the volatility suddenly increases at time $t=100$, and then compare the GARCH model's response to that break with a moving average model.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define parameters
> n = 200  # Number of time steps
> break_point = 100  # Time of the structural break
>
> # GARCH parameters
> alpha0 = 0.00001
> alpha1 = 0.1
> beta = 0.8
>
> # High break parameters
> break_multiplier = 5 # Increase post-break volatility five-fold
>
> # Generate returns with structural break
> np.random.seed(42)
> returns = np.random.normal(0, 0.01, n)
>
> # Higher volatility after break
> returns[break_point:] = np.random.normal(0, 0.01*break_multiplier, n - break_point)
>
> # Initialize
> h = np.zeros(n)
> h[0] = np.var(returns[:break_point]) # Use variance before break
>
> # Implement GARCH model
> for i in range(1, n):
>     h[i] = alpha0 + alpha1 * returns[i-1]**2 + beta * h[i-1]
>
> # Implement Moving Average Volatility
> window_size = 20
> ma_volatility = np.zeros(n)
>
> for t in range(window_size, n):
>     ma_volatility[t] = np.sqrt(np.mean(returns[t-window_size:t]**2))
>
> # Plot results
> plt.figure(figsize=(12, 6))
> plt.plot(h, label='GARCH Volatility')
> plt.plot(ma_volatility, label=f'Moving Average Volatility (Window Size = {window_size})')
> plt.axvline(x=break_point, color='r', linestyle='--', label='Structural Break')
> plt.legend()
> plt.title('GARCH and Moving Average Response to Structural Break')
> plt.xlabel('Time')
> plt.ylabel('Volatility')
> plt.show()
> ```
>
>  After time 100 the actual returns are generated from a distribution with standard deviation .05, or a variance of .0025. The alpha0 parameter forces the GARCH model to gradually approach that new level. It takes time for the beta parameter to force the shock back toward alpha0 and to fully appreciate that new level of volatility.  The MA response takes even longer, because of the moving window's effects. For that time the data series is more volatile.
![This image illustrates the GARCH and MA model responses to a structural break, where the post-break volatility is higher than the pre-break volatility](./../images/garch_ma_structural_break.png)

#### 9.2.8 Implications for Risk Management

The presence of volatility clustering and structural breaks has significant implications for risk management:

*   **VaR and Expected Shortfall:** Models that fail to capture volatility clustering and structural breaks will underestimate the true risk, leading to inadequate capital reserves and increased exposure to losses [^1]. More sophisticated models, such as GARCH, are better at capturing these dynamics and providing more accurate risk estimates [^5].
*   **Portfolio Diversification:** The benefits of portfolio diversification can be diminished during periods of high volatility and market stress [^1]. Correlations between assets tend to increase during these periods, reducing the effectiveness of diversification strategies [^1]. Therefore, it is important to use models that can capture these time-varying correlations and adjust portfolio allocations accordingly.
*   **Stress Testing:** Stress testing is a crucial component of risk management [^1]. It involves simulating extreme but plausible scenarios to assess the potential impact on a portfolio. Scenarios should include both volatility clustering and structural breaks to ensure that the portfolio is robust to a wide range of adverse market conditions.

Furthermore, the accurate modeling of volatility clustering and structural breaks is critical for option pricing, as demonstrated by the following proposition:

**Proposição 2:** *Option prices derived from models that do not account for volatility clustering and structural breaks will systematically misprice options, particularly those with longer maturities or strike prices far from the current asset price.*

*Proof (Sketch):* Option prices are highly sensitive to volatility estimates. Volatility clustering implies that short-term volatility forecasts can be more accurately modeled using models like GARCH, leading to more precise short-term option pricing. Structural breaks, if ignored, can lead to significant underestimation of long-term volatility, thus mispricing long-term options. Specifically, models failing to account for these breaks will underestimate the volatility used to calculated the fair price for the option. This effect is stronger as the option expiration window increases.

*Proof:*
I. Option prices, such as those calculated using the Black-Scholes model, are highly dependent on the volatility parameter ($\sigma$).

II. Volatility clustering results in short-term volatility being predictable, as periods of high volatility tend to be followed by more high volatility periods. This predictability is not captured by simple models that assume constant volatility. GARCH models and other time-series models effectively capturing volatility clustering will result in more accurate short-term volatility estimation and thus more accurate option prices.

III. Structural breaks are unanticipated shifts in the underlying volatility. If a structural break occurs and is not accounted for, models will underestimate the future volatility and thus underprice the option. Specifically, consider the case of an option expiring after a structural break that is not accounted for. Ignoring the break would mean that the increased volatility caused by the break is not factored into the option price.

IV. The mispricing effect due to ignoring structural breaks is more pronounced for options with longer maturities, because these options are more exposed to the risk of structural breaks occurring during their lifetime. Similarly, options with strike prices far from the current asset price (out-of-the-money options) are also more sensitive to volatility, because they depend more on the likelihood of extreme price movements.

V. Therefore, option prices derived from models that do not account for volatility clustering and structural breaks will systematically misprice options, particularly those with longer maturities or strike prices far from the current asset price. ■

> 💡 **Exemplo Numérico:** To quantify how option prices are affected by structural breaks, consider a European call option on a stock with the following characteristics:
>
> *   Current stock price ($S_0$): \$ 100
> *   Strike price ($K$): \$ 110
> *   Time to maturity ($T$): 1 year
> *   Risk-free interest rate ($r$): 5% per year
>
> We will use the Black-Scholes model to calculate the option price under two scenarios: (1) constant volatility and (2) a structural break leading to increased volatility.
>
> **Scenario 1: Constant Volatility**
>
> Assume a constant volatility ($\sigma$) of 20% per year. Using the Black-Scholes formula:
>
> $$C = S_0 N(d_1) - K e^{-rT} N(d_2)$$
>
> $$d_1 = \frac{\ln(\frac{S_0}{K}) + (r + \frac{\sigma^2}{2})T}{\sigma \sqrt{T}}$$
>
> $$d_2 = d_1 - \sigma \sqrt{T}$$
>
> First, compute $d_1$ and $d_2$:
>
> $$d_1 = \frac{\ln(\frac{100}{110}) + (0.05 + \frac{0.2^2}{2}) \times 1}{0.2 \sqrt{1}} = \frac{-0.0953 + 0.07}{0.2} = -0.1265$$
>
> $$d_2 = -0.1265 - 0.2 \sqrt{1} = -0.3265$$
>
> Next, calculate $N(d_1)$ and $N(d_2)$ using the standard normal cumulative distribution function:
>
> *   $N(d_1) = N(-0.1265) \approx 0.45$
> *   $N(d_2) = N(-0.3265) \approx 0.37$
>
> Now, compute the call option price:
>
> $$C = 100 \times 0.45 - 110 \times e^{-0.05 \times 1} \times 0.37 = 45 - 110 \times 0.9512 \times 0.37 = 45 - 38.84 \approx 6.16$$
>
> **Scenario 2: Structural Break Leading to Increased Volatility**
>
> Assume that a structural break occurs immediately, leading to an increase in volatility from 20% to 30% per year.
>
> Repeat the calculation with $\sigma = 0.3$:
>
> $$d_1 = \frac{\ln(\frac{100}{110}) + (0.05 + \frac{0.3^2}{2}) \times 1}{0.3 \sqrt{1}} = \frac{-0.0953 + 0.095}{0.3} = -0.001$$
>
> $$d_2 = -0.001 - 0.3 \sqrt{1} = -0.301$$
>
> *   $N(d_1) = N(-0.001) \approx 0.50$
> *   $N(d_2) = N(-0.301) \approx 0.38$
>
> Now, compute the call option price:
>
> $$C = 100 \times 0.50 - 110 \times e^{-0.05 \times 1} \times 0.38 = 50 - 110 \times 0.9512 \times 0.38 = 50 - 39.54 \approx 10.46$$
>
> **Comparison**
>
> *   Option price with constant volatility (20%): \$ 6.16
> *   Option price with increased volatility (30% due to structural break): \$ 10.46
>
> In this example, the option price increases significantly from \$ 6.16 to \$ 10.46 due to the structural break leading to increased volatility. A model that fails to account for the structural break and assumes constant volatility would underestimate the true option value by approximately 40%. This is a substantial difference that could lead to mispricing and potential losses for market participants. This simplified example illustrates how the option price increases due to increased volatility, especially because the call option is out-of-the-money and one year from expiring.

### Conclusão
In this section, we explored the concepts of volatility clustering and structural breaks and their impact on risk management [^1]. We saw that volatility clustering is a common feature of financial markets and that structural breaks can lead to sudden and significant changes in volatility levels [^1]. We also discussed several approaches for modeling these phenomena, including GARCH models, regime-switching models, and stochastic volatility models [^5]. By incorporating these concepts into our risk management framework, we can develop more robust and accurate risk estimates and make more informed decisions in volatile market conditions [^1].

### Referências
[^1]: Página 219: *“The purpose of this chapter is to present techniques to forecast vari-ation in risk and correlations. Section 9.1presents a simple motivation, and Section 9.2 describes the data. Section 9.3 presents single-series volatility models, and Section 9.4 discusses multivariate models. Section 9.5 discusses diagnostic checking of volatility models, and Section 9.6 concludes.”*

## Capítulo 10: Otimização de Portfólio com Restrições Cardinais e de Grupo

Este capítulo aborda a otimização de portfólio sob restrições cardinais e de grupo, um problema complexo com aplicações práticas significativas. As restrições cardinais limitam o número de ativos que podem ser incluídos no portfólio, enquanto as restrições de grupo impõem limites à alocação de ativos dentro de grupos predefinidos.

### 10.1 Introdução às Restrições Cardinais e de Grupo

As restrições cardinais e de grupo são frequentemente encontradas em cenários de gestão de portfólio do mundo real. As restrições cardinais surgem de custos de transação, requisitos regulatórios ou preferências de gestão. As restrições de grupo refletem estratégias de investimento específicas, como alocar uma porcentagem mínima a um determinado setor ou região geográfica.

### 10.2 Formulação do Problema

O problema de otimização de portfólio com restrições cardinais e de grupo pode ser formulado como um problema de programação inteira mista (MILP). Seja $x_i$ a proporção do portfólio alocada ao ativo $i$, $N$ o número total de ativos, e $K$ o limite cardinal. Seja $G_j$ o conjunto de ativos no grupo $j$, e $L_j$ e $U_j$ os limites inferior e superior para a alocação total ao grupo $j$. O problema pode ser formulado como:

$$
\begin{aligned}
\text{Maximizar} \quad & \mu^T x - \gamma x^T \Sigma x \\
\text{sujeito a} \quad & \sum_{i=1}^{N} x_i = 1 \\
& \sum_{i=1}^{N} y_i \leq K \\
& x_i \leq y_i \quad \forall i \\
& x_i \geq 0 \quad \forall i \\
& y_i \in \{0, 1\} \quad \forall i \\
& L_j \leq \sum_{i \in G_j} x_i \leq U_j \quad \forall j
\end{aligned}
$$

onde:
- $\mu$ é o vetor de retornos esperados.
- $\Sigma$ é a matriz de covariância.
- $\gamma$ é o parâmetro de aversão ao risco.
- $y_i$ é uma variável binária que indica se o ativo $i$ está incluído no portfólio.

### 10.3 Métodos de Solução

Resolver o problema de otimização de portfólio com restrições cardinais e de grupo é computacionalmente desafiador devido à presença de variáveis inteiras. Vários métodos podem ser usados, incluindo:

1.  **Solvers de MILP:** Solvers comerciais como Gurobi e CPLEX podem resolver instâncias de MILP de forma eficiente.

2.  **Heurísticas:** Heurísticas como algoritmos genéticos e simulated annealing podem fornecer soluções aproximadas em um tempo razoável.

3.  **Relaxação:** Técnicas de relaxação, como relaxação Lagrangiana, podem ser usadas para obter limites inferiores na solução ótima.

### 10.4 Exemplo Numérico

Considere um portfólio com 10 ativos. Os retornos esperados e a matriz de covariância são fornecidos abaixo. O limite cardinal é definido como 5 e existem duas restrições de grupo: a alocação total para os ativos 1 a 5 deve estar entre 0,4 e 0,6, e a alocação total para os ativos 6 a 10 deve estar entre 0,3 e 0,5.

*   Retornos Esperados: $\mu = [0.10, 0.12, 0.15, 0.08, 0.09, 0.11, 0.13, 0.14, 0.16, 0.10]$
*   Matriz de Covariância: (uma matriz 10x10)

Resolver este problema usando um solver de MILP fornece a alocação de portfólio ótima.

### 10.5 Conclusão

A otimização de portfólio com restrições cardinais e de grupo é um problema complexo com aplicações práticas significativas. Os solvers de MILP e as heurísticas podem ser usados para resolver este problema.

<!-- END -->