## Forecasting Risk and Correlations: Modeling Time-Varying Risk with Moving Averages and GARCH Models

### Introdu√ß√£o
Expanding on the concept presented earlier about *fat tails* and time-varying distributions [^3], this section delves into specific techniques for modeling time-varying risk [^1]. The aim is to go beyond simply identifying non-normality and to provide quantitative tools for predicting and managing risk more effectively. We will cover two main classes of models: moving averages (MA) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models, detailing their advantages, disadvantages, and practical applications. The motivation for using time-varying models lies in the fact that the volatility of financial markets is not constant [^1], and the ability to predict changes in volatility is crucial for risk management, particularly in the calculation of Value at Risk (VaR) [^1].

### Conceitos Fundamentais

#### 9.2.1 Moving Averages
A simple approach to modeling time-varying volatility is the use of **moving averages** [^4]. This method calculates volatility as the average of squared returns over a fixed time window [^4].

**Definition:**
Let $r_t$ be the return at time *t*, and *M* be the size of the time window. The moving average based volatility estimate is given by:

$$
\sigma_t^2 = (1/M) \sum_{i=1}^{M} r_{t-i+1}^2
$$

This formula calculates volatility as the average of the squares of the most recent *M* returns [^4]. Here, we focus on raw returns instead of returns around the mean, since for most financial series, ignoring expected returns over very short time intervals makes little difference for volatility estimates [^4].

**Example:**
Consider a window of 20 business days (*M* = 20) or 60 business days (approximately one quarter, *M* = 60) [^4]. Each day, the forecast is updated by adding the information from the preceding day and removing the information from (*M* + 1) days ago [^4]. All past returns are given the same weight (1/*M*) [^4]. Figure 9-3 illustrates 20- and 60-day moving averages for the \$/BP rate [^4].
![Moving Average volatility forecasts](./../images/moving_average_volatility_forecasts.jpg)
**Disadvantages:**
Despite its simplicity, the moving average model has several drawbacks [^4]:

1.  **Ignores the dynamic ordering of observations:** Recent information is given the same weight as older observations in the window, which may not be appropriate if volatility is changing rapidly [^4].
2.  **Ghosting Feature:** Removing a large return from the window substantially affects the volatility estimate [^4]. For instance, the 3 percent drop on September 17, 1992, would affect the MA(20), which would revert to a lower value after 20 days, and the MA(60), which would revert to a lower value after 60 days [^4]. This subsequent drop is an artifact of the window size and is called the *"ghosting feature"* [^4], as the MA measure changes for no apparent reason [^4].

**Lemma 2:** *A moving average of volatility is a non-parametric estimate that smooths volatility fluctuations over time but introduces a delay in response to abrupt changes in volatility.*

*Proof (Sketch):* The moving average is a convolution of the squared returns with a rectangular weight function of width M. This convolution acts as a mean filter, smoothing high-frequency fluctuations. However, the response to a volatility shock is delayed because the shock only affects the moving average as it enters the calculation window and continues to affect it until it leaves the window.

*Proof:*

I. The volatility estimate by moving average is given by:
$$\sigma_t^2 = \frac{1}{M} \sum_{i=1}^M r_{t-i+1}^2$$
II. This equation represents a convolution of the squared returns ($r_t^2$) with a rectangular weight function $w_i$:
$$w_i = \frac{1}{M}, \quad i = 1, 2, \ldots, M$$
$$w_i = 0, \quad \text{otherwise}$$
III. The convolution smooths high-frequency fluctuations in the squared returns, acting as a mean filter.
IV. Consider a volatility shock at time $t^*$, such that $r_{t^*}^2$ is significantly larger than previous squared returns. Before time $t^*$, the shock has no effect on the moving average.
V.  At time $t^*$, the shock enters the moving average calculation window and begins to affect $\sigma_t^2$. The effect increases as the shock remains in the window.
VI. The maximum effect of the shock is achieved when the shock is fully within the window (i.e., for $t > t^* + M$).
VII. After time $t^* + M$, the shock leaves the moving average calculation window and its effect gradually diminishes until it disappears completely.
VIII. Therefore, the moving average introduces a delay in the response to abrupt changes in volatility, as the shock takes time to enter and leave the calculation window. ‚ñ†

**Analysis:**

Figure 9-3 shows that the MA(60) is more stable than the MA(20), which is understandable because longer periods decrease the weight of any specific day [^5]. However, this raises the question: which window is better? This approach leaves unanswered the choice of the moving window [^5]. Longer periods increase the accuracy of the estimate but may miss the underlying variation in volatility [^5].

> üí° **Exemplo Num√©rico:** Suppose we have the following daily returns for a stock over 25 days: `[0.01, -0.02, 0.005, 0.015, -0.01, 0.008, -0.005, 0.02, -0.012, 0.01, 0.03, -0.025, 0.018, -0.015, 0.022, 0.005, -0.003, 0.012, -0.008, 0.01, 0.02, -0.01, 0.005, 0.015, -0.02]`. Let's calculate the volatility using a 5-day moving average for the last day.
>
> 1.  Square the last 5 returns: `[0.0004, 0.000225, 0.000025, 0.0001, 0.0004]`
> 2.  Sum the squared returns: `0.0004 + 0.000225 + 0.000025 + 0.0001 + 0.0004 = 0.00115`
> 3.  Divide by the window size (5): `0.00115 / 5 = 0.00023`
> 4.  Take the square root to get the volatility: `sqrt(0.00023) ‚âà 0.0152` or 1.52%. This gives the estimated daily volatility for day 25 using a 5-day moving average.
>
> ```python
> import numpy as np
>
> returns = np.array([0.01, -0.02, 0.005, 0.015, -0.01, 0.008, -0.005, 0.02, -0.012, 0.01, 0.03, -0.025, 0.018, -0.015, 0.022, 0.005, -0.003, 0.012, -0.008, 0.01, 0.02, -0.01, 0.005, 0.015, -0.02])
> window_size = 5
>
> squared_returns = returns[-window_size:]**2
> volatility = np.sqrt(np.mean(squared_returns))
>
> print(f"Volatility (5-day MA): {volatility:.4f}")
> ```
>
> The simplicity of this calculation hides the trade-off between responsiveness and stability. A shorter window would react more quickly to new information but also be more susceptible to noise, while a longer window would smooth out the noise but react more slowly.

To address the selection of the optimal window size *M* in moving average models, consider the following extension:

**Teorema 3:** *The optimal window size M for a moving average volatility estimator balances bias and variance.*

*Proof (Sketch):*
The bias arises because the moving average assumes constant volatility within the window, which is rarely true in practice. A smaller window reduces this bias but increases the variance of the estimate due to the smaller sample size. Conversely, a larger window reduces variance but increases bias. The optimal *M* minimizes the mean squared error (MSE), which is the sum of the squared bias and the variance.

*Proof:*

I.  Let $\sigma_t^2$ be the true volatility at time *t*, and $\hat{\sigma}_t^2(M)$ be the moving average estimate with window size *M*.
II. The Mean Squared Error (MSE) is given by:
    $$MSE(M) = E[(\hat{\sigma}_t^2(M) - \sigma_t^2)^2] = Bias^2(M) + Variance(M)$$
    where
    $$Bias(M) = E[\hat{\sigma}_t^2(M)] - \sigma_t^2$$
    $$Variance(M) = E[(\hat{\sigma}_t^2(M) - E[\hat{\sigma}_t^2(M)])^2]$$
III. The bias term arises because the moving average assumes that volatility is constant within the window of size *M*. If the true volatility is changing, this assumption introduces a bias. As *M* increases, the assumption of constant volatility becomes less valid, and the bias typically increases.
IV. The variance term reflects the statistical uncertainty in the estimate due to the finite sample size *M*. As *M* increases, the variance decreases because the estimate is based on more data points.
V.  The optimal window size $M^*$ is the one that minimizes the MSE:
    $$M^* = \arg\min_M MSE(M)$$
VI. Finding the analytical solution for $M^*$ is generally not possible without making specific assumptions about the dynamics of the true volatility process $\sigma_t^2$. However, one can numerically estimate the MSE for different values of *M* using historical data and choose the *M* that yields the smallest MSE.
VII. Therefore, the optimal window size *M* balances the trade-off between bias and variance. A smaller *M* reduces bias but increases variance, while a larger *M* reduces variance but increases bias. The optimal *M* minimizes the MSE. ‚ñ†

> üí° **Exemplo Num√©rico:** To illustrate the bias-variance trade-off, let's simulate a scenario where the true volatility changes over time and then compare the MSE for different window sizes (M) in a moving average model.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simulate true volatility (example: increasing trend)
> np.random.seed(42)
> time = np.arange(200)
> true_volatility = 0.01 + 0.0001 * time + 0.005 * np.sin(0.1 * time) # Example: increasing trend with some oscillation
>
> # Simulate returns based on true volatility
> returns = np.random.normal(0, true_volatility, size=len(time))
>
> # Function to calculate moving average volatility
> def moving_average_volatility(returns, M):
>     volatility = np.zeros_like(returns)
>     for t in range(M, len(returns)):
>         volatility[t] = np.sqrt(np.mean(returns[t-M:t]**2))
>     return volatility
>
> # Calculate MSE for different window sizes
> window_sizes = [5, 20, 50]
> mse_values = []
>
> for M in window_sizes:
>     ma_volatility = moving_average_volatility(returns, M)
>     mse = np.mean((ma_volatility[M:] - true_volatility[M:])**2)
>     mse_values.append(mse)
>
> # Plotting the results
> plt.figure(figsize=(12, 6))
> plt.plot(time, true_volatility, label='True Volatility', linewidth=2)
> for i, M in enumerate(window_sizes):
>     ma_volatility = moving_average_volatility(returns, M)
>     plt.plot(time[M:], ma_volatility[M:], label=f'MA Volatility (M={M}), MSE={mse_values[i]:.6f}', alpha=0.7)
>
> plt.xlabel('Time')
> plt.ylabel('Volatility')
> plt.title('Moving Average Volatility Estimation with Different Window Sizes')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print("MSE Values for Different Window Sizes:")
> for i, M in enumerate(window_sizes):
>     print(f"M = {M}: MSE = {mse_values[i]:.6f}")
> ```
>
> In this simulation, we generate a 'true volatility' series with an increasing trend and oscillations. We then simulate returns based on this volatility. We calculate moving average volatility estimates for window sizes of 5, 20, and 50 days. The MSE is calculated by comparing the MA volatility estimates to the true volatility. From the graph and the printed MSE values, you can observe how different window sizes lead to different trade-offs between bias and variance. Smaller window sizes (e.g., M=5) are more responsive to changes but also more noisy, potentially leading to higher variance. Larger window sizes (e.g., M=50) smooth out the noise but may lag behind the true volatility, increasing bias. The optimal window size is the one that minimizes the MSE, and this value depends on the specific characteristics of the volatility process. This example illustrates the need to carefully consider the window size when using moving averages for volatility estimation, as different choices can have a significant impact on the accuracy of the estimates.

This theorem highlights the inherent trade-off in choosing the window size for moving averages. In practice, one might use cross-validation techniques to empirically determine the optimal window size for a given dataset.

#### 9.2.2 GARCH Estimation
To overcome the limitations of moving averages, volatility modeling has evolved to models that give more weight to recent information [^5]. The first such model was the **Generalized Autoregressive Conditional Heteroskedastic (GARCH)**, proposed by Engle (1982) and Bollerslev (1986) [^5]. The term *"heteroskedastic"* refers to the fact that variances are changing [^5].

**Definition:**
The GARCH model assumes that the variance of returns follows a predictable process [^5]. The conditional variance depends on the latest innovation, but also on the previous conditional variance [^5].

Let $h_t$ be the conditional variance using information up to time *t* - 1 and $r_{t-1}$ be the previous day's return [^5]. The simplest GARCH(1,1) model is given by [^5]:

$$
h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}
$$

where:
*   $\alpha_0$ is a constant.
*   $\alpha_1$ measures the sensitivity of volatility to new information (past returns).
*   $\beta$ measures the persistence of volatility.

**Recognition:**
The importance of measuring time variation in risk was recognized when Professor Robert Engle was awarded the 2003 Nobel Prize in Economics [^6]. The Royal Swedish Academy of Sciences stated that Professor Engle's "ARCH models have become indispensable tools not only for researchers but also for analysts on financial markets, who use them in asset pricing and in evaluating portfolio risk" [^6].

**Properties and Estimates:**

To find the average unconditional variance, we define $E(r_{t-1}^2) = h_t = h_{t-1} = h$. Solving for *h*, we find [^6]:

$$
h = \frac{\alpha_0}{1 - \alpha_1 - \beta}
$$

*Proof:*

I. Starting from the GARCH(1,1) equation:
$$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}$$
II. Taking the expectation of both sides:
$$E[h_t] = E[\alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}]$$
III. Assuming stationarity, $E[h_t] = h$ and $E[r_{t-1}^2] = h$:
$$h = \alpha_0 + \alpha_1 h + \beta h$$
IV. Rearranging the terms to solve for *h*:
$$h - \alpha_1 h - \beta h = \alpha_0$$
$$h(1 - \alpha_1 - \beta) = \alpha_0$$
V. Finally, dividing by $(1 - \alpha_1 - \beta)$:
$$h = \frac{\alpha_0}{1 - \alpha_1 - \beta}$$ ‚ñ†

For the model to be stationary, the sum of the parameters $\alpha_1 + \beta$ must be less than unity [^6]. This sum is also called persistence, for reasons that will become clearer later [^6].

> üí° **Numerical Example:** Consider a GARCH(1,1) model with parameters $\alpha_0 = 0.00001$, $\alpha_1 = 0.05$, and $\beta = 0.9$. The persistence is $\alpha_1 + \beta = 0.95$, which indicates high persistence in volatility. The unconditional variance is $h = 0.00001 / (1 - 0.95) = 0.0002$. If the current volatility is higher than the unconditional mean, it is expected to gradually decrease over time.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # GARCH(1,1) parameters
> alpha0 = 0.00001
> alpha1 = 0.05
> beta = 0.9
>
> # Calculation of unconditional variance
> unconditional_variance = alpha0 / (1 - alpha1 - beta)
>
> # Initialization of conditional variance
> conditional_variance = np.zeros(100)
> conditional_variance[0] = unconditional_variance * 2  # Initialization with a value above the mean
>
> # Simulation of conditional variance over time
> for i in range(1, 100):
>     conditional_variance[i] = alpha0 + alpha1 * 0 + beta * conditional_variance[i-1]
>
> # Plotting the conditional variance
> plt.figure(figsize=(10, 6))
> plt.plot(conditional_variance)
> plt.axhline(y=unconditional_variance, color='r', linestyle='--', label='Unconditional Variance')
> plt.title('Evolution of Conditional Variance in the GARCH(1,1) Model')
> plt.xlabel('Time')
> plt.ylabel('Conditional Variance')
> plt.legend()
> plt.show()
>
> print(f"Unconditional variance: {unconditional_variance:.6f}")
> ```
>
> This code shows how conditional volatility converges to unconditional volatility over time in a GARCH(1,1) model.

To further illustrate the concept of persistence, consider the following lemma:

**Lema 3:** *The persistence parameter (Œ±‚ÇÅ + Œ≤) in a GARCH(1,1) model determines the rate at which volatility shocks decay.*

*Proof (Sketch):*
The lemma states that if the sum (Œ±‚ÇÅ + Œ≤) is close to 1, shocks to volatility will persist for a longer time. If it is closer to 0, the shocks will decay more quickly. This can be shown by recursively substituting the equation for $h_t$ to express it as a function of past shocks.

*Proof:*

I. The GARCH(1,1) model is given by:
$$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}$$
II. Substituting $h_{t-1}$ with its own definition, we get:
$$h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta (\alpha_0 + \alpha_1 r_{t-2}^2 + \beta h_{t-2})$$
$$h_t = \alpha_0(1 + \beta) + \alpha_1 r_{t-1}^2 + \alpha_1 \beta r_{t-2}^2 + \beta^2 h_{t-2}$$
III. Repeating the substitution *n* times, we get:
$$h_t = \alpha_0 \sum_{i=0}^{n-1} \beta^i + \alpha_1 \sum_{i=0}^{n-1} \beta^i r_{t-1-i}^2 + \beta^n h_{t-n}$$
IV. As $n \to \infty$, if $\beta < 1$, the term $\beta^n h_{t-n}$ goes to zero, and the equation becomes:
$$h_t = \frac{\alpha_0}{1 - \beta} + \alpha_1 \sum_{i=0}^{\infty} \beta^i r_{t-1-i}^2$$
V. The impact of a shock $r_{t-1-i}^2$ on the current volatility $h_t$ is weighted by $\beta^i$. If $\beta$ is close to 1, the weights decay slowly, meaning that past shocks have a long-lasting impact on current volatility. If $\beta$ is close to 0, the weights decay quickly, meaning that past shocks have a short-lived impact on current volatility.

VI.  Since $\alpha_1$ also contributes to the persistence, the combined effect of $\alpha_1 + \beta$ determines the overall rate of decay. If $\alpha_1 + \beta$ is close to 1, the volatility shocks persist for a longer time, indicating high persistence. If $\alpha_1 + \beta$ is closer to 0, the shocks decay more quickly, indicating low persistence. ‚ñ†

> üí° **Exemplo Num√©rico:** To see how the persistence parameter affects the decay of volatility shocks, let's simulate two GARCH(1,1) models, one with high persistence (Œ±‚ÇÅ + Œ≤ close to 1) and one with low persistence (Œ±‚ÇÅ + Œ≤ closer to 0), and observe how a single shock affects the conditional variance over time.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # GARCH(1,1) parameters for high persistence
> alpha0_high = 0.00001
> alpha1_high = 0.05
> beta_high = 0.94
>
> # GARCH(1,1) parameters for low persistence
> alpha0_low = 0.00001
> alpha1_low = 0.1
> beta_low = 0.6
>
> # Number of time steps
> time_steps = 100
>
> # Initialize conditional variance arrays
> conditional_variance_high = np.zeros(time_steps)
> conditional_variance_low = np.zeros(time_steps)
>
> # Set initial variance level
> initial_variance = 0.0001
> conditional_variance_high[0] = initial_variance
> conditional_variance_low[0] = initial_variance
>
> # Simulate GARCH models with a volatility shock
> shock_time = 10
> shock_size = 0.05  # Shock of 5% return
>
> for t in range(1, time_steps):
>     # High persistence model
>     if t == shock_time:
>         conditional_variance_high[t] = alpha0_high + alpha1_high * shock_size**2 + beta_high * conditional_variance_high[t-1]
>     else:
>         conditional_variance_high[t] = alpha0_high + alpha1_high * 0 + beta_high * conditional_variance_high[t-1]
>
>     # Low persistence model
>     if t == shock_time:
>         conditional_variance_low[t] = alpha0_low + alpha1_low * shock_size**2 + beta_low * conditional_variance_low[t-1]
>     else:
>         conditional_variance_low[t] = alpha0_low + alpha1_low * 0 + beta_low * conditional_variance_low[t-1]
>
> # Plotting the conditional variances
> plt.figure(figsize=(12, 6))
> plt.plot(conditional_variance_high, label=f'High Persistence (Œ±‚ÇÅ={alpha1_high}, Œ≤={beta_high})')
> plt.plot(conditional_variance_low, label=f'Low Persistence (Œ±‚ÇÅ={alpha1_low}, Œ≤={beta_low})')
> plt.axvline(x=shock_time, color='r', linestyle='--', label='Volatility Shock')
#plt.yscale('log')  # Use log scale to visualize the decay more clearly
> plt.title('Effect of Persistence on Volatility Shock Decay')
> plt.xlabel('Time Step')
> plt.ylabel('Conditional Variance')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"High persistence: {alpha1_high + beta_high}")
> print(f"Low persistence: {alpha1_low + beta_low}")
> ```
>
> This code simulates two GARCH(1,1) models, one with high persistence and one with low persistence. A single volatility shock is introduced at time step 10. The plot shows how the conditional variance evolves over time for both models. Observe that in the high persistence model, the volatility shock decays much slower compared to the low persistence model. This illustrates how the persistence parameter (Œ±‚ÇÅ + Œ≤) determines the rate at which volatility shocks decay. This shows that models with higher persistence weights past shocks for a longer time.

This lemma clarifies the role of the persistence parameter in determining the memory of the GARCH(1,1) model. High persistence implies that volatility forecasts will be heavily influenced by past shocks, while low persistence implies that the forecasts will be more responsive to recent information.

**Parameter Estimation:**

The disadvantage of GARCH models is their nonlinearity [^6]. The parameters must be estimated by maximizing the likelihood function, which involves numerical optimization [^6]. Typically, researchers assume that the scaled residuals $e_t = r_t / \sqrt{h_t}$ have a normal distribution and are independent [^6]. If we have *T* observations, their joint density is the product of the densities for each time period *t* [^7]. The optimization maximizes the logarithm of the likelihood function [^7]:

$$
\max F(\alpha_0, \alpha_1, \beta | r) = \sum_{t=1}^{T} \ln f(r_t | h_t) = \sum_{t=1}^{T} \ln \left( \frac{1}{\sqrt{2\pi h_t}} \exp \left( -\frac{r_t^2}{2h_t} \right) \right)
$$

In practice, this result is even more general. Bollerslev and Wooldridge (1992) have shown that even when the true distribution is not normal, the parameters so estimated are consistent [^7]. The method is then called quasi-maximum likelihood [^7]. Thus, one could estimate the conditional distribution in two steps, first estimating the GARCH parameters using Equation (9.4) and then estimating the distribution parameters for the scaled residual [^7]:

$$
\epsilon_t = \frac{r_t}{\sqrt{h_t}}
$$

The conditional distribution of this scaled residual could be taken as a student *t* distribution or some other parametric distribution or even be sampled from the historical data [^7]. The latter approach is called filtered historical simulation [^7].

**Table 9-1:**

Table 9-1 presents the estimation results for a series of financial series over the period 1990 to 1999 [^7]. There are large differences in the level of volatility across series, but for all these series, the time variation in risk is highly significant [^7]. The persistence parameter is also quite high, on the order of 0.97-0.99, although this depends on the sample period and is not measured perfectly [^7].

| Parameter           | \$/BP  | DM/\$  | Yen/\$ | DM/BP | US Stocks | US Bonds | Crude Oil |
| ------------------- | ----- | ----- | ----- | ----- | ------------- | -------------- | ------------- |
| Average Volatility (% pa) | 11.33 | 10.54 | 11.78 | 7.98  | 14.10        | 4.07           | 37.55       |
| Œ±‚ÇÄ                  | 0.00299 | 0.00576 | 0.01040 | 0.00834 | 0.00492       | 0.00138        | 0.04153       |
| Œ±‚ÇÅ                  | 0.0379  | 0.0390  | 0.0528  | 0.1019  | 0.0485        | 0.0257         | 0.08348       |
| Œ≤                   | 0.9529  | 0.9476  | 0.9284  | 0.8699  | 0.9459        | 0.9532         | 0.9131        |
| Persistence (Œ±‚ÇÅ + Œ≤) | 0.9908  | 0.9866  | 0.9812  | 0.9718  | 0.9944        | 0.9789         | 0.9966        |

Figure 9-4 displays the GARCH forecast of volatility for the \$/BP rate [^8]. It shows increased volatility in the fall of 1992 [^8]. Afterward, volatility decreases progressively over time, not in the abrupt fashion observed in Figure 9-3 [^8].
![GARCH volatility forecast](./../images/garch_volatility_forecast.jpg)

**Practical Applications:**

Figure 9-5 illustrates the practical use of this information, showing daily returns along with conditional 95 percent confidence bands, which involve plus or minus two standard deviations when the conditional residuals are normal [^8]. This model appears to capture variation in risk adequately [^8]. Most of the returns fall within the 95 percent band [^8]. The few outside the bands correspond to the remaining 5 percent of occurrences [^8].
![Returns and GARCH confidence bands](./../images/returns_garch_confidence_bands.jpg)

**Model Extensions:**

In practice, this basic GARCH model can be extended to other specifications [^8]. Because the innovation enters as a quadratic term, a day of exceptionally large value will have a very large effect on the conditional variance [^8]. This effect could be reduced by using the absolute value of the innovation instead [^8]. Also, the basic GARCH model is symmetric [^8]. For some series, such as stocks, large negative returns have a bigger effect on risk than do positive returns, possibly reflecting a leverage effect [^9]. GARCH models can be adapted to this empirical observation by using two terms, one for positive shocks and the other for negative shocks, each with its separate Œ± coefficient [^9].

> üí° **Exemplo Num√©rico:** Let's consider a scenario where we want to forecast Value at Risk (VaR) using a GARCH(1,1) model. Suppose we have estimated the following parameters: $\alpha_0 = 0.000005$, $\alpha_1 = 0.06$, and $\beta = 0.92$. The current conditional variance $h_t$ is 0.0001. We want to calculate the 1-day 99% VaR.
>
> 1.  **Forecast the next day's conditional variance ($h_{t+1}$)**:
>     $h_{t+1} = \alpha_0 + \alpha_1 r_t^2 + \beta h_t$.
>     Let's assume $r_t = 0.01$ (1% return).
>     $h_{t+1} = 0.000005 + 0.06 \times (0.01)^2 + 0.92 \times 0.0001 = 0.000005 + 0.000006 + 0.000092 = 0.000103$.
> 2.  **Calculate the forecasted volatility ($\sigma_{t+1}$)**:
>     $\sigma_{t+1} = \sqrt{h_{t+1}} = \sqrt{0.000103} \approx 0.01015$, or 1.015%.
> 3.  **Determine the Z-score for the 99% confidence level**:
>     For a 99% confidence level (one-tailed), the Z-score is approximately 2.33 (you can find this using a standard normal distribution table or a statistical function).
> 4.  **Calculate the VaR**:
>     $VaR = -Z \times \sigma_{t+1}$.
>     $VaR = -2.33 \times 0.01015 \approx -0.02375$, or -2.375%.
>
> This means we are 99% confident that the losses tomorrow will not exceed 2.375% of the portfolio value, based on the GARCH(1,1) forecast.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # GARCH(1,1) parameters
> alpha0 = 0.000005
> alpha1 = 0.06
> beta = 0.92
>
> # Current conditional variance and return
> ht = 0.0001
> rt = 0.01
>
> # Forecast next day's conditional variance
> ht1 = alpha0 + alpha1 * rt**2 + beta * ht
>
> # Forecast next day's volatility
> sigma_t1 = np.sqrt(ht1)
>
> # Confidence level
> confidence_level = 0.99
>>> # Calculate the critical value (z-score) for the given confidence level
> critical_value = norm.ppf(confidence_level)
>
> # Calculate the Value at Risk (VaR)
> var = -sigma_t1 * critical_value
>
> print(f"Next day's conditional variance forecast: {ht1}")
> print(f"Next day's volatility forecast: {sigma_t1}")
> print(f"Value at Risk (VaR) at {confidence_level*100}% confidence level: {var}")

### Expected Shortfall (ES)
Expected Shortfall (ES), also known as Conditional Value at Risk (CVaR), quantifies the expected loss given that the loss has already exceeded the VaR level. It provides a more comprehensive measure of tail risk compared to VaR.

#### Calculation of Expected Shortfall

The formula for ES is:

$$
ES = E[X | X \le VaR]
$$

Where:
- $ES$ is the Expected Shortfall.
- $X$ is the random variable representing the loss.
- $VaR$ is the Value at Risk.

For a normal distribution, the ES can be calculated as:

$$
ES = - \sigma \frac{\phi(VaR/\sigma)}{1 - \alpha}
$$

Where:
- $\sigma$ is the standard deviation (volatility).
- $\phi$ is the probability density function of the standard normal distribution.
- $\alpha$ is the confidence level.

#### Python Implementation for Expected Shortfall

Here's a Python implementation for calculating ES:

```python
 import numpy as np
 from scipy.stats import norm
 

 def calculate_expected_shortfall(sigma, var, confidence_level):
  """
  Calculate Expected Shortfall (ES) for a given volatility, VaR, and confidence level.
  
  Parameters:
  sigma (float): Volatility (standard deviation).
  var (float): Value at Risk.
  confidence_level (float): Confidence level (e.g., 0.99 for 99%).
  
  Returns:
  float: Expected Shortfall.
  """
  alpha = 1 - confidence_level
  es = -sigma * norm.pdf(norm.ppf(1 - alpha)) / alpha
  return es
 

 # Example usage:
 # Assuming sigma_t1 and var are already calculated
 es = calculate_expected_shortfall(sigma_t1, var, confidence_level)
 

 print(f"Expected Shortfall (ES) at {confidence_level*100}% confidence level: {es}")
 ```

This function `calculate_expected_shortfall` computes the ES given the volatility (`sigma`), VaR, and the confidence level. The ES provides an estimate of the expected loss if the VaR is exceeded, offering a more conservative risk measure.

### Backtesting VaR and ES

Backtesting is the process of comparing the predicted VaR and ES values with the actual returns to assess the accuracy and reliability of the risk model.

#### Basic Backtesting Methodology

1.  **Collect Historical Data**: Gather a sufficient amount of historical return data.
2.  **Calculate VaR and ES**: Compute VaR and ES for each period using the chosen model.
3.  **Compare with Actual Returns**: Compare the predicted VaR and ES with the actual returns for each period.
4.  **Count Violations**: Count the number of times the actual loss exceeds the VaR (VaR violations).
5.  **Evaluate Performance**: Use statistical tests to determine if the number of violations is in line with the expected number based on the confidence level.

#### Python Implementation for Backtesting

Here's a basic Python implementation for backtesting VaR:

```python
 import numpy as np
 

 def backtest_var(returns, var, confidence_level):
  """
  Backtest Value at Risk (VaR) by comparing it with actual returns.
  
  Parameters:
  returns (np.array): Array of actual returns.
  var (np.array): Array of VaR values.
  confidence_level (float): Confidence level (e.g., 0.99 for 99%).
  
  Returns:
  float: Violation rate.
  """
  violations = np.sum(returns < -var)
  total_periods = len(returns)
  violation_rate = violations / total_periods
  
  expected_violation_rate = 1 - confidence_level
  
  print(f"Number of VaR violations: {violations}")
  print(f"Total periods: {total_periods}")
  print(f"Violation rate: {violation_rate:.4f}")
  print(f"Expected violation rate: {expected_violation_rate:.4f}")
  
  return violation_rate
 

 # Example usage:
 # Assuming 'returns' and 'var' are already calculated
 # Example: Assuming returns is an array of daily returns and var is an array of daily VaR predictions
 

 # Generate some dummy returns and VaR for demonstration purposes
 np.random.seed(42)
 returns = np.random.normal(0, 0.01, 250) # Simulate 250 daily returns
 var = np.full(250, 0.02) # Assume VaR is constantly 2%
 

 violation_rate = backtest_var(returns, var, confidence_level)
 ```

This `backtest_var` function calculates the violation rate by comparing the actual returns with the predicted VaR values. The violation rate should be close to the expected violation rate (1 - confidence level) for a well-calibrated VaR model.

### Refinements and Advanced Techniques

#### Handling Fat Tails

Financial returns often exhibit fat tails, meaning extreme events occur more frequently than predicted by a normal distribution. To address this, one can use:

*   **Student's t-distribution**: Captures heavier tails compared to the normal distribution.
*   **Extreme Value Theory (EVT)**: Models the tail behavior directly using extreme value distributions like the Generalized Pareto Distribution (GPD).

#### Incorporating Time-Varying Parameters

Volatility and correlations are not constant over time. Advanced models incorporate time-varying parameters to capture these dynamics:

*   **GARCH Variants**: Extensions of the GARCH model, such as EGARCH and GJR-GARCH, can capture asymmetry in volatility responses to positive and negative shocks.
*   **Dynamic Conditional Correlation (DCC) Models**: Allow correlations between assets to vary over time.

#### Stress Testing

Stress testing involves evaluating the performance of a portfolio under extreme but plausible scenarios. This helps to identify vulnerabilities that may not be apparent under normal market conditions.

*   **Scenario Analysis**: Define specific stress scenarios (e.g., market crash, interest rate shock) and evaluate the portfolio's performance under these scenarios.
*   **Historical Stress Tests**: Use historical events (e.g., the 2008 financial crisis) to simulate extreme market conditions.

#### Model Validation and Governance

*   **Regular Model Review**: Periodically review and validate the risk model to ensure it remains accurate and reliable.
*   **Independent Validation**: Engage independent experts to validate the model and its assumptions.
*   **Documentation**: Maintain comprehensive documentation of the model, its assumptions, and its limitations.

### Visualizations and Summary

Diagrams and visualizations are crucial for understanding and communicating risk measures.

#### VaR and ES Visualization

A simple way to visualize VaR and ES is by plotting the distribution of returns and highlighting the VaR and ES levels.

```python
 import numpy as np
 import matplotlib.pyplot as plt
 from scipy.stats import norm
 

 # Example data (replace with your actual returns)
 np.random.seed(42)
 returns = np.random.normal(0, 0.01, 1000)
 

 # Parameters
 confidence_level = 0.99
 

 # Calculate VaR and ES (using normal distribution as an example)
 sigma = np.std(returns)
 var = norm.ppf(1 - (1 - confidence_level), loc=np.mean(returns), scale=sigma)
 es = -sigma * norm.pdf(norm.ppf(1 - (1 - confidence_level))) / (1 - confidence_level)
 

 # Plotting
 plt.figure(figsize=(10, 6))
 

 # Histogram of returns
 plt.hist(returns, bins=50, density=True, alpha=0.6, color='blue', label='Returns Distribution')
 

 # Plot VaR
 plt.axvline(x=var, color='red', linestyle='--', label=f'VaR at {confidence_level*100}% Confidence')
 

 # Plot ES
 plt.axvline(x=-es, color='green', linestyle='--', label=f'ES at {confidence_level*100}% Confidence')
 

 plt.xlabel('Returns')
 plt.ylabel('Density')
 plt.title('VaR and ES Visualization')
 plt.legend()
 plt.grid(True)
 plt.show()
 ```

This script generates a histogram of returns and overlays the VaR and ES levels, providing a clear visual representation of the risk measures.

#### Risk Model Diagram

A diagram illustrating the components of a risk model can help in understanding its structure and dependencies.

```mermaid
 graph LR
  A[Data Input] --> B(Data Preprocessing);
  B --> C{Model Selection};
  C -- GARCH --> D[Volatility Estimation];
  C -- Historical Simulation --> E[Return Simulation];
  D --> F(VaR Calculation);
  E --> F;
  F --> G(ES Calculation);
  G --> H{Backtesting};
  H -- Pass --> I[Model Validation];
  H -- Fail --> C;
  I --> J[Risk Reporting];
 ```

This Mermaid diagram outlines the key steps in a typical risk modeling process, from data input to risk reporting.

By combining these techniques and visualizations, one can develop a robust and comprehensive risk management framework that effectively quantifies and manages financial risk.
<!-- END -->