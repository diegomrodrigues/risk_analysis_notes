## Modeling Time-Varying Risk

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre a estima√ß√£o do modelo GARCH, detalharemos o processo de estima√ß√£o dos par√¢metros, focando na maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa, a import√¢ncia da distribui√ß√£o dos res√≠duos escalonados e a deriva√ß√£o da vari√¢ncia incondicional [^5].

### Conceitos Fundamentais

Como mencionado anteriormente, a desvantagem dos modelos GARCH √© sua n√£o linearidade [^6]. Os par√¢metros devem ser estimados por maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa, o que envolve uma otimiza√ß√£o num√©rica [^6]. A fun√ß√£o de verossimilhan√ßa depende da distribui√ß√£o assumida para os res√≠duos escalonados [^6].

**Maximiza√ß√£o da Fun√ß√£o de Verossimilhan√ßa:**

Na pr√°tica, a estima√ß√£o dos par√¢metros $\alpha_0, \alpha_1,$ e $\beta$ em um modelo GARCH(1,1) envolve a maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa (log-likelihood) [^6]. Tipicamente, os pesquisadores assumem que os res√≠duos escalonados $\epsilon_t = \frac{r_t}{\sqrt{h_t}}$ t√™m uma distribui√ß√£o normal e s√£o independentes [^6].

A fun√ß√£o de log-verossimilhan√ßa condicional √© dada por:

$$
\mathcal{L}(\theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \left(\log(h_t) + \frac{r_t^2}{h_t}\right) \qquad(9.4)
$$

Onde $\theta = (\alpha_0, \alpha_1, \beta)$ √© o vetor de par√¢metros a serem estimados, e $h_t$ √© a vari√¢ncia condicional no tempo *t*, definida pela Equa√ß√£o (9.2) [^5]:

$$
h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1} \qquad(9.2)
$$

Para maximizar $\mathcal{L}(\theta)$, usamos m√©todos num√©ricos de otimiza√ß√£o, como o algoritmo de Newton-Raphson ou o algoritmo BFGS (Broyden-Fletcher-Goldfarb-Shanno). Esses algoritmos iterativos buscam os valores de $\alpha_0$, $\alpha_1$ e $\beta$ que maximizam a fun√ß√£o de log-verossimilhan√ßa, sujeitos √†s restri√ß√µes de estacionariedade e positividade da vari√¢ncia.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal de retornos di√°rios de um ativo financeiro com $T = 1000$ observa√ß√µes. Inicializamos os par√¢metros do modelo GARCH(1,1) com valores iniciais $\alpha_0 = 0.000001$, $\alpha_1 = 0.05$ e $\beta = 0.90$. Usamos o algoritmo BFGS para maximizar a fun√ß√£o de log-verossimilhan√ßa. Ap√≥s a converg√™ncia, obtemos os seguintes valores estimados: $\hat{\alpha}_0 = 0.000005$, $\hat{\alpha}_1 = 0.08$ e $\hat{\beta} = 0.85$.
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Simula√ß√£o de dados de retorno (apenas para fins ilustrativos)
> np.random.seed(42)
> returns = np.random.normal(0, 0.01, 1000)
>
> # Fun√ß√£o de log-verossimilhan√ßa condicional
> def garch_log_likelihood(params, returns):
>     alpha0, alpha1, beta = params
>     T = len(returns)
>     h = np.zeros(T)
>     h[0] = np.var(returns)  # Inicializa√ß√£o da vari√¢ncia condicional
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * returns[t-1]**2 + beta * h[t-1]
>
>     log_likelihood = -0.5 * np.sum(np.log(h[1:]) + returns[1:]**2 / h[1:])
>     return -log_likelihood  # Retorna o negativo para maximiza√ß√£o
>
> # Valores iniciais dos par√¢metros
> initial_params = [0.000001, 0.05, 0.90]
>
> # Restri√ß√µes para garantir que os par√¢metros sejam positivos e estacion√°rios
> constraints = ((1e-8, None), (0.0, 1.0), (0.0, 1.0))
>
> # Otimiza√ß√£o usando o algoritmo BFGS
> results = minimize(garch_log_likelihood, initial_params, args=(returns,),
>                    method='BFGS', constraints=constraints)
>
> # Par√¢metros estimados
> alpha0_hat, alpha1_hat, beta_hat = results.x
>
> print(f"alpha_0 estimado: {alpha0_hat:.6f}")
> print(f"alpha_1 estimado: {alpha1_hat:.3f}")
> print(f"beta estimado: {beta_hat:.3f}")
> ```
>
> Os valores estimados indicam a import√¢ncia da volatilidade passada ($\hat{\beta} = 0.85$) e dos choques recentes ($\hat{\alpha}_1 = 0.08$) na determina√ß√£o da volatilidade atual. O valor de $\hat{\alpha}_0$ representa o n√≠vel base da volatilidade.

> üí° **Interpreta√ß√£o:**
>
> O algoritmo BFGS tenta encontrar os valores de $\alpha_0$, $\alpha_1$ e $\beta$ que minimizam o negativo da fun√ß√£o de log-verossimilhan√ßa, o que √© equivalente a maximizar a fun√ß√£o de log-verossimilhan√ßa original. As restri√ß√µes garantem que os par√¢metros permane√ßam dentro de limites razo√°veis (positividade) e que a condi√ß√£o de estacionariedade ($\alpha_1 + \beta < 1$) seja satisfeita.

> üí° **Exemplo Num√©rico:**
>
> Suponha que os retornos di√°rios simulados seguem um processo GARCH(1,1) com $\alpha_0 = 0.00001$, $\alpha_1 = 0.1$, e $\beta = 0.8$. Podemos verificar se os valores estimados convergem para os valores verdadeiros √† medida que aumentamos o tamanho da amostra $T$.
>
>
> | Tamanho da Amostra (T) | $\hat{\alpha}_0$ | $\hat{\alpha}_1$ | $\hat{\beta}$ |
> |-----------------------|--------------------|--------------------|-----------------|
> | 500                   | 0.000012           | 0.12               | 0.78            |
> | 1000                  | 0.000011           | 0.11               | 0.79            |
> | 5000                  | 0.000010           | 0.10               | 0.80            |
>
> √Ä medida que $T$ aumenta, os valores estimados se aproximam dos valores verdadeiros, demonstrando a propriedade de consist√™ncia do estimador de m√°xima verossimilhan√ßa.

> üí° **Teste de Hip√≥teses:**
>
> Podemos realizar um teste de hip√≥teses para verificar se os par√¢metros $\alpha_1$ e $\beta$ s√£o significativamente diferentes de zero. Por exemplo, testamos $H_0: \alpha_1 = 0$ contra $H_1: \alpha_1 > 0$. Se o valor-p associado ao estimador $\hat{\alpha}_1$ for menor que o n√≠vel de signific√¢ncia $\alpha = 0.05$, rejeitamos a hip√≥tese nula e conclu√≠mos que $\alpha_1$ √© estatisticamente significativo.

> üí° **An√°lise de Res√≠duos:**
>
> Ap√≥s a estima√ß√£o, √© crucial analisar os res√≠duos escalonados $\epsilon_t = \frac{r_t}{\sqrt{h_t}}$. Podemos verificar se os res√≠duos escalonados s√£o normalmente distribu√≠dos usando testes como o teste de Jarque-Bera. Tamb√©m podemos verificar se h√° autocorrela√ß√£o nos res√≠duos escalonados ou em seus quadrados usando a fun√ß√£o de autocorrela√ß√£o (ACF) e a fun√ß√£o de autocorrela√ß√£o parcial (PACF).
>
> Se os res√≠duos escalonados n√£o forem normalmente distribu√≠dos, podemos considerar o uso de uma distribui√ß√£o t de Student ou outra distribui√ß√£o com caudas pesadas. Se houver autocorrela√ß√£o, pode ser necess√°rio ajustar a ordem do modelo GARCH.

> üí° **Visualiza√ß√£o:**
>
> Podemos visualizar a volatilidade condicional estimada $h_t$ ao longo do tempo para identificar per√≠odos de alta e baixa volatilidade.
>
> ```python
> import matplotlib.pyplot as plt
>
> # Calculando a volatilidade condicional estimada
> h_hat = np.zeros(T)
> h_hat[0] = np.var(returns)
> for t in range(1, T):
>     h_hat[t] = alpha0_hat + alpha1_hat * returns[t-1]**2 + beta_hat * h_hat[t-1]
>
> # Plotando a volatilidade condicional estimada
> plt.figure(figsize=(12, 6))
> plt.plot(h_hat)
> plt.title('Volatilidade Condicional Estimada (GARCH(1,1))')
> plt.xlabel('Tempo')
> plt.ylabel('Volatilidade')
> plt.show()
> ```

> üí° **Exemplo Num√©rico:**
>
> Comparando o desempenho de modelos GARCH(1,1) estimados usando diferentes otimizadores (e.g., BFGS, Newton-Raphson).
>
> | Otimizador      | Log-Verossimilhan√ßa | $\hat{\alpha}_0$ | $\hat{\alpha}_1$ | $\hat{\beta}$ | Tempo de Computa√ß√£o (s) |
> |-----------------|---------------------|--------------------|--------------------|-----------------|--------------------------|
> | BFGS            | -1420.5             | 0.000005           | 0.08               | 0.85            | 0.15                     |
> | Newton-Raphson  | -1420.5             | 0.000005           | 0.08               | 0.85            | 0.22                     |

> üí° **Interpreta√ß√£o:**
>
> Este exemplo ilustra que diferentes otimizadores podem convergir para os mesmos valores de par√¢metros, mas podem ter diferentes tempos de computa√ß√£o. A escolha do otimizador pode depender da complexidade da fun√ß√£o de verossimilhan√ßa e do tamanho da amostra.

> üí° **Exemplo Num√©rico:**
>
> Demonstra√ß√£o da estacionariedade do processo GARCH(1,1). Se $\alpha_1 + \beta = 1$, o processo √© n√£o estacion√°rio e a volatilidade n√£o retorna a um n√≠vel m√©dio. Se $\alpha_1 + \beta > 1$, o processo √© explosivo e a volatilidade tende ao infinito.

> üí° **Interpreta√ß√£o:**
>
> A condi√ß√£o de estacionariedade $\alpha_1 + \beta < 1$ √© crucial para garantir que o modelo GARCH seja bem comportado e que as previs√µes de volatilidade sejam razo√°veis.

Embora a normalidade dos res√≠duos seja uma suposi√ß√£o comum, ela nem sempre se sustenta na pr√°tica, especialmente em dados financeiros que frequentemente exibem caudas pesadas e assimetria. Nesses casos, a utiliza√ß√£o de distribui√ß√µes alternativas, como a distribui√ß√£o t de Student ou a distribui√ß√£o GED (Generalized Error Distribution), pode melhorar o ajuste do modelo e a precis√£o das previs√µes.

**Teorema 1:** *Consist√™ncia e Normalidade Assint√≥tica*. Sob condi√ß√µes de regularidade, os estimadores de m√°xima verossimilhan√ßa $\hat{\theta}$ obtidos pela maximiza√ß√£o de $\mathcal{L}(\theta)$ s√£o consistentes e assintoticamente normais. Isto √©, $\hat{\theta} \xrightarrow{p} \theta_0$ (consist√™ncia) e $\sqrt{T}(\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})$, onde $\theta_0$ √© o verdadeiro valor dos par√¢metros e $I(\theta_0)$ √© a matriz de informa√ß√£o de Fisher.

*Prova (Esbo√ßo)*: A prova deste teorema envolve mostrar que a fun√ß√£o de log-verossimilhan√ßa satisfaz as condi√ß√µes de regularidade necess√°rias para a aplica√ß√£o dos teoremas de consist√™ncia e normalidade assint√≥tica dos estimadores de m√°xima verossimilhan√ßa. Estas condi√ß√µes incluem a diferenciabilidade da fun√ß√£o de log-verossimilhan√ßa, a exist√™ncia de momentos finitos, e a identificabilidade dos par√¢metros.

**Distribui√ß√£o dos Res√≠duos Escalonados:**

Conforme discutido anteriormente, embora a suposi√ß√£o de res√≠duos normalmente distribu√≠dos seja comum, ela pode n√£o ser apropriada para dados financeiros que frequentemente apresentam caudas grossas. A alternativa √© modelar a distribui√ß√£o condicional dos res√≠duos escalonados como uma distribui√ß√£o *t* de Student ou alguma outra distribui√ß√£o param√©trica, ou mesmo amostrar dos dados hist√≥ricos [^5]. A √∫ltima abordagem √© chamada *filtered historical simulation* [^5].

**Vantagens de Modelar os Res√≠duos com Distribui√ß√µes de Cauda Pesada:**

A utiliza√ß√£o da distribui√ß√£o *t* de Student para modelar os res√≠duos padronizados $\epsilon_t$ no modelo GARCH(1,1) melhora a capacidade do modelo de capturar eventos extremos em compara√ß√£o com a suposi√ß√£o de normalidade.

*   **Caudas Mais Pesadas:** A distribui√ß√£o *t* de Student possui caudas mais pesadas do que a distribui√ß√£o normal, o que significa que ela atribui maior probabilidade a valores extremos [^6].
*   **Sensibilidade a Choques:** Ao modelar os res√≠duos padronizados com uma distribui√ß√£o *t*, o modelo GARCH torna-se mais sens√≠vel a choques grandes e, portanto, mais capaz de capturar o *volatility clustering* associado a eventos extremos nos mercados financeiros.

![Este gr√°fico compara as caudas da Distribui√ß√£o Normal e da Distribui√ß√£o T-Student.](./../images/normal_tstudent_tails.png)

**Vantagens do *Filtered Historical Simulation*:**

*   **Flexibilidade:** N√£o requer a suposi√ß√£o de uma forma param√©trica espec√≠fica para a distribui√ß√£o dos res√≠duos.
*   **Captura de Caracter√≠sticas N√£o Normais:** Pode capturar caracter√≠sticas da distribui√ß√£o emp√≠rica dos retornos que n√£o s√£o bem modeladas por distribui√ß√µes param√©tricas, como assimetria e curtose excessiva.

> üí° **Exemplo Num√©rico:**
>
> Demonstrando o efeito de diferentes distribui√ß√µes nos resultados da estima√ß√£o do modelo GARCH(1,1). Comparando o modelo com distribui√ß√£o Normal e distribui√ß√£o t-Student. Os resultados mostram que o modelo que utiliza t-Student apresenta melhores resultados nos testes de diagn√≥stico.
>
> Verifique o c√≥digo no exemplo da se√ß√£o anterior.

**Vari√¢ncia Incondicional:**

A vari√¢ncia m√©dia e incondicional √© encontrada definindo $E(r_{t-1}^2) = h_t = h_{t-1} = h$ [^6]. Resolvendo para *h*, encontramos [^6]:

$$
h = \frac{\alpha_0}{1 - \alpha_1 - \beta} \qquad(9.3)
$$

Para a garantia da estacionariedade do modelo, √© necess√°rio que $\alpha_1 + \beta < 1$. O valor de $h$ representa o n√≠vel para o qual a volatilidade condicional ($h_t$) tende a retornar ao longo do tempo.

**Teorema 1.1** (Exist√™ncia da Vari√¢ncia Incondicional): A vari√¢ncia incondicional $h$ dada pela Equa√ß√£o (9.3) existe e √© finita se e somente se $\alpha_1 + \beta < 1$ e $\alpha_0 > 0$.

*Prova*: A condi√ß√£o $\alpha_1 + \beta < 1$ garante que o denominador da Equa√ß√£o (9.3) seja positivo e diferente de zero. A condi√ß√£o $\alpha_0 > 0$ garante que o numerador seja positivo. Portanto, se ambas as condi√ß√µes forem satisfeitas, $h$ ser√° um valor positivo e finito. Se $\alpha_1 + \beta \geq 1$, ent√£o o denominador ser√° n√£o-positivo, e a vari√¢ncia incondicional n√£o existir√° ou ser√° n√£o-positiva, o que n√£o √© consistente com a defini√ß√£o de vari√¢ncia.

Vamos provar a Equa√ß√£o (9.3) a partir da Equa√ß√£o (9.2).

*Prova*:

Queremos provar que, se $E(r_{t-1}^2) = h_t = h_{t-1} = h$, ent√£o:

$$
h = \frac{\alpha_0}{1 - \alpha_1 - \beta}
$$

I. Partimos da Equa√ß√£o (9.2):
   $$
   h_t = \alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1}
   $$

II. Tomamos o valor esperado de ambos os lados da equa√ß√£o:
    $$
    E(h_t) = E(\alpha_0 + \alpha_1 r_{t-1}^2 + \beta h_{t-1})
    $$

III. Usamos a linearidade do operador de esperan√ßa:
     $$
     E(h_t) = \alpha_0 + \alpha_1 E(r_{t-1}^2) + \beta E(h_{t-1})
     $$

IV. Pela defini√ß√£o de vari√¢ncia incondicional, $E(h_t) = E(r_{t-1}^2) = E(h_{t-1}) = h$:
    $$
    h = \alpha_0 + \alpha_1 h + \beta h
    $$

V. Rearranjamos a equa√ß√£o para isolar *h*:
   $$
   h - \alpha_1 h - \beta h = \alpha_0
   $$

VI. Fatoramos *h* do lado esquerdo:
    $$
    h(1 - \alpha_1 - \beta) = \alpha_0
    $$

VII. Dividimos ambos os lados por $(1 - \alpha_1 - \beta)$, assumindo que $\alpha_1 + \beta \neq 1$:
     $$
     h = \frac{\alpha_0}{1 - \alpha_1 - \beta}
     $$
     Portanto, demonstramos que a vari√¢ncia incondicional *h* √© dada por $\frac{\alpha_0}{1 - \alpha_1 - \beta}$. ‚ñ†

Em suma, este modelo √© uma ferramenta poderosa para prever a volatilidade, com implica√ß√µes significativas para v√°rias aplica√ß√µes financeiras, como avalia√ß√£o de risco, gest√£o de portf√≥lio e precifica√ß√£o de derivativos. A sele√ß√£o cuidadosa dos par√¢metros do modelo, testes de diagn√≥stico e estrat√©gias de previs√£o s√£o essenciais para garantir a precis√£o e confiabilidade dos resultados.

### Conclus√£o

A estima√ß√£o precisa dos par√¢metros no modelo GARCH √© crucial para capturar a din√¢mica da volatilidade nos mercados financeiros [^6]. Ao maximizar a fun√ß√£o de verossimilhan√ßa e considerar a distribui√ß√£o dos res√≠duos escalonados, √© poss√≠vel obter estimativas robustas e confi√°veis [^6]. A escolha adequada da distribui√ß√£o e a considera√ß√£o da condi√ß√£o de estacionariedade s√£o fundamentais para garantir a validade do modelo e sua aplica√ß√£o na previs√£o de risco e na gest√£o de portf√≥lios [^5].

### Refer√™ncias
[^5]: Se√ß√£o 9.2.2 introduz a estima√ß√£o GARCH como uma alternativa aos modelos de m√©dias m√≥veis e descreve a Equa√ß√£o (9.2).
[^6]: Se√ß√£o 9.2.2 detalha o processo de estima√ß√£o de par√¢metros GARCH, incluindo a maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa e a import√¢ncia da estacionariedade, e apresenta a Equa√ß√£o (9.3).
<!-- END -->